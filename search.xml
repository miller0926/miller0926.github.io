<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>hexo + butterfly blog安装</title>
      <link href="2021/01/28/hexo+butterfly/"/>
      <url>2021/01/28/hexo+butterfly/</url>
      
        <content type="html"><![CDATA[<h2 id="hexo-Butterfly-blog安装"><a href="#hexo-Butterfly-blog安装" class="headerlink" title="hexo + Butterfly blog安装"></a>hexo + Butterfly blog安装</h2><h3 id="1-安装hexo-butterfly"><a href="#1-安装hexo-butterfly" class="headerlink" title="1. 安装hexo butterfly"></a>1. 安装hexo butterfly</h3><h4 id="1-安装hexo"><a href="#1-安装hexo" class="headerlink" title="1. 安装hexo"></a>1. 安装hexo</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br><span class="line">hexo init</span><br></pre></td></tr></table></figure><h4 id="2-安装butterfly"><a href="#2-安装butterfly" class="headerlink" title="2. 安装butterfly"></a>2. 安装butterfly</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/Butterfly</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>将hexo根目录下<code>_config.yml</code>改为<code>theme: Butterfly</code>。</p><h5 id="平滑升级"><a href="#平滑升级" class="headerlink" title="平滑升级"></a>平滑升级</h5><blockquote><p>推荐把主题默认的配置文件<code>_config.yml</code> 复製到 Hexo 根目录下的 `_config.butterfly.yml’</p><p>Hexo會自動合併主題中的<code>_config.yml</code>和 <code>_config.butterfly.yml</code>裡的配置，如果存在同名配置，會使用<code>_config.butterfly.yml</code>的配置，其優先度較高。</p></blockquote><p>安装安装 pug 以及 stylus 的渲染器插件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-renderer-pug hexo-renderer-stylus --save</span><br></pre></td></tr></table></figure><h5 id="proxy"><a href="#proxy" class="headerlink" title="proxy"></a>proxy</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo npm config set proxy&#x3D;socks:&#x2F;&#x2F;127.0.0.1:1080</span><br><span class="line">sudo npm config set proxy&#x3D;http:&#x2F;&#x2F;127.0.0.1:1081</span><br><span class="line">sudo npm config set registry&#x3D;http:&#x2F;&#x2F;registry.npmjs.org</span><br></pre></td></tr></table></figure><h5 id="local-search"><a href="#local-search" class="headerlink" title="local_search"></a>local_search</h5><p>在hexo根目录下<code>_config.yml</code>加上</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">search:</span></span><br><span class="line">  <span class="attr">path:</span> <span class="string">search.xml</span></span><br><span class="line">  <span class="attr">field:</span> <span class="string">post</span></span><br><span class="line">  </span><br></pre></td></tr></table></figure><p>然后，安装插件<code>npm install hexo-generator-search --save</code>.</p><h5 id="安装mermaid插件"><a href="#安装mermaid插件" class="headerlink" title="安装mermaid插件"></a>安装mermaid插件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-filter-mermaid-diagrams</span><br></pre></td></tr></table></figure><p>其它插件安装跟hexo主题一样</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-permalink-pinyin --save <span class="comment">#中文链接转拼音链接</span></span><br><span class="line">npm install hexo-deployer-git --save <span class="comment">#git</span></span><br><span class="line">npm install hexo-generator-seo-friendly-sitemap --save <span class="comment"># sitemap</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="3-cover-图片和-theme"><a href="#3-cover-图片和-theme" class="headerlink" title="3. cover 图片和 theme"></a>3. cover 图片和 theme</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">cover:</span></span><br><span class="line">  <span class="comment"># display the cover or not (是否顯示文章封面)</span></span><br><span class="line">  <span class="attr">index_enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">aside_enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">archives_enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># the position of cover in home page (封面顯示的位置)</span></span><br><span class="line">  <span class="comment"># left/right/both</span></span><br><span class="line">  <span class="attr">position:</span> <span class="string">both</span></span><br><span class="line">  <span class="comment"># When cover is not set, the default cover is displayed (當沒有設置cover時，默認的封面顯示)</span></span><br><span class="line">  <span class="attr">default_cover:</span></span><br><span class="line">    <span class="comment"># - https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/img/ach.jpg</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/img/imgs/0.jpg</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/img/imgs/1.jpg</span></span><br></pre></td></tr></table></figure><p>这个图片放置在，<code>blog\themes\Butterfly\source\img\imgs</code>路径下。</p><ul><li><code>theme</code></li></ul><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">theme_color:</span> </span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">main:</span> <span class="string">&quot;#7B90D2&quot;</span></span><br><span class="line">  <span class="attr">paginator:</span> <span class="string">&quot;#91B493&quot;</span></span><br><span class="line">  <span class="attr">button_hover:</span> <span class="string">&quot;#7B90D2&quot;</span></span><br><span class="line">  <span class="attr">text_selection:</span> <span class="string">&quot;#00c4b6&quot;</span></span><br><span class="line">  <span class="attr">link_color:</span> <span class="string">&quot;#99a9bf&quot;</span></span><br><span class="line">  <span class="attr">meta_color:</span> <span class="string">&quot;#7b90d2&quot;</span></span><br><span class="line">  <span class="attr">hr_color:</span> <span class="string">&quot;#A4D8FA&quot;</span></span><br><span class="line">  <span class="attr">code_foreground:</span> <span class="string">&quot;#F47466&quot;</span></span><br><span class="line">  <span class="attr">code_background:</span> <span class="string">&quot;rgba(27, 31, 35, .05)&quot;</span></span><br><span class="line">  <span class="attr">toc_color:</span> <span class="string">&quot;#7b90d2&quot;</span></span><br><span class="line">  <span class="attr">blockquote_padding_color:</span> <span class="string">&quot;#49b1f5&quot;</span></span><br><span class="line">  <span class="attr">blockquote_background_color:</span> <span class="string">&quot;#49b1f5&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="文章置顶"><a href="#文章置顶" class="headerlink" title="文章置顶"></a>文章置顶</h4><p>将<code>node_modules\hexo-generator-index\lib</code>下 <code>generator.js</code>替换如下</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&#x27;use strict&#x27;</span>;</span><br><span class="line"><span class="keyword">var</span> pagination = <span class="built_in">require</span>(<span class="string">&#x27;hexo-pagination&#x27;</span>);</span><br><span class="line"><span class="built_in">module</span>.exports = <span class="function"><span class="keyword">function</span>(<span class="params">locals</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">var</span> config = <span class="built_in">this</span>.config;</span><br><span class="line">    <span class="keyword">var</span> posts = locals.posts;</span><br><span class="line">    posts.data = posts.data.sort(<span class="function"><span class="keyword">function</span>(<span class="params">a, b</span>) </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(a.top &amp;&amp; b.top) &#123; <span class="comment">// 两篇文章top都有定义</span></span><br><span class="line">            <span class="keyword">if</span>(a.top == b.top) <span class="keyword">return</span> b.date - a.date; <span class="comment">// 若top值一样则按照文章日期降序排</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">return</span> b.top - a.top; <span class="comment">// 否则按照top值降序排</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(a.top &amp;&amp; !b.top) &#123; <span class="comment">// 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233）</span></span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(!a.top &amp;&amp; b.top) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> b.date - a.date; <span class="comment">// 都没定义按照文章日期降序排</span></span><br><span class="line">    &#125;);</span><br><span class="line">    <span class="keyword">var</span> paginationDir = config.pagination_dir || <span class="string">&#x27;page&#x27;</span>;</span><br><span class="line">    <span class="keyword">return</span> pagination(<span class="string">&#x27;&#x27;</span>, posts, &#123;</span><br><span class="line">    perPage: config.index_generator.per_page,</span><br><span class="line">    layout: [<span class="string">&#x27;index&#x27;</span>, <span class="string">&#x27;archive&#x27;</span>],</span><br><span class="line">    format: paginationDir + <span class="string">&#x27;/%d/&#x27;</span>,</span><br><span class="line">    data: &#123;</span><br><span class="line">      __index: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>然后在文章post头加上<code>top：10</code>0数字越大越靠前</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">title:</span> <span class="string">Hexo</span> </span><br><span class="line"><span class="attr">date:</span> <span class="number">2020-04-18 17:32:22</span></span><br><span class="line"><span class="attr">categories:</span> <span class="string">hexo</span></span><br><span class="line"><span class="attr">tags:</span> [<span class="string">hexo1</span>, <span class="string">hexo2</span>, <span class="string">hexo3</span>]</span><br><span class="line"><span class="attr">top:</span> <span class="number">100</span></span><br></pre></td></tr></table></figure><h4 id="新建留言板页"><a href="#新建留言板页" class="headerlink" title="新建留言板页"></a>新建留言板页</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page <span class="string">&quot;contact&quot;</span></span><br></pre></td></tr></table></figure><p>加上 <code>/source/contact/index.md</code></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">contact</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2020-09-30 17:25:30</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">&quot;contact&quot;</span></span><br><span class="line"><span class="attr">layout:</span> <span class="string">&quot;contact&quot;</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure><h4 id="改变页面宽度"><a href="#改变页面宽度" class="headerlink" title="改变页面宽度"></a>改变页面宽度</h4><p><code>themes\Butterfly\source\css\_page\common.styl</code>下</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.layout</span></span><br><span class="line">  <span class="selector-tag">display</span>: <span class="selector-tag">flex</span></span><br><span class="line">  <span class="selector-tag">margin</span>: 0 <span class="selector-tag">auto</span></span><br><span class="line">  <span class="selector-tag">padding</span>: 2<span class="selector-tag">rem</span> 20<span class="selector-tag">px</span></span><br><span class="line">  <span class="selector-tag">max-width</span>: 1600<span class="selector-tag">px</span> #正文页面宽度</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="文章统计图和标签等"><a href="#文章统计图和标签等" class="headerlink" title="文章统计图和标签等"></a>文章统计图和标签等</h3><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-charts -S</span><br></pre></td></tr></table></figure><p> 编辑 主题目录<code>/Butterfly/layout/page.pug</code> 文件，在 <code>.tag-cloud</code> 下面添加一行 <code>#tags-chart</code>，在 <code>.category-content</code> 下面添加一行 <code>#categories-chart</code>.</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">when</span> <span class="string">&#x27;tags&#x27;</span></span><br><span class="line">        <span class="string">include</span> <span class="string">includes/page/tags.pug</span></span><br><span class="line">          <span class="comment">#tags-chart</span></span><br><span class="line">      <span class="string">when</span> <span class="string">&#x27;link&#x27;</span></span><br><span class="line">        <span class="string">include</span> <span class="string">includes/page/flink.pug</span></span><br><span class="line">      <span class="string">when</span> <span class="string">&#x27;categories&#x27;</span></span><br><span class="line">        <span class="string">include</span> <span class="string">includes/page/categories.pug</span></span><br><span class="line">          <span class="comment">#categories-chart</span></span><br></pre></td></tr></table></figure><h3 id="字体大小和font"><a href="#字体大小和font" class="headerlink" title="字体大小和font"></a>字体大小和font</h3><p><code>D:\study\hexo\blog_ing\themes\Butterfly\source\css\_layout\head.style</code></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">.site-page</span></span><br><span class="line">  <span class="attr">position:</span> <span class="string">relative</span></span><br><span class="line">  <span class="attr">padding-bottom:</span> <span class="string">.3rem</span></span><br><span class="line">  <span class="attr">text-shadow:</span> <span class="string">.05rem</span> <span class="string">.05rem</span> <span class="string">.1rem</span> <span class="string">rgba($dark-black,</span> <span class="number">.3</span><span class="string">)</span></span><br><span class="line">  <span class="attr">font-size:</span> <span class="string">.60em</span></span><br><span class="line">  <span class="attr">cursor:</span> <span class="string">pointer</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>修改 <code>_config.butterfly.yml</code></p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">font:</span></span><br><span class="line">  <span class="attr">global-font-size:</span></span><br><span class="line">  <span class="attr">code-font-size:</span></span><br><span class="line">  <span class="attr">font-family:</span> <span class="string">-apple-system,</span> <span class="string">BlinkMacSystemFont,</span> <span class="string">&quot;Segoe UI&quot;</span><span class="string">,</span> <span class="string">&quot;Helvetica Neue&quot;</span><span class="string">,</span> <span class="string">Lato,</span> <span class="string">Roboto,</span> <span class="string">&quot;PingFang SC&quot;</span><span class="string">,</span> <span class="string">&quot;Microsoft JhengHei&quot;</span><span class="string">,</span> <span class="string">&quot;Microsoft YaHei&quot;</span><span class="string">,</span> <span class="string">sans-serif</span></span><br><span class="line">  <span class="attr">code-font-family:</span> <span class="string">consolas,</span> <span class="string">Menlo,</span> <span class="string">&quot;PingFang SC&quot;</span><span class="string">,</span> <span class="string">&quot;Microsoft JhengHei&quot;</span><span class="string">,</span> <span class="string">&quot;Microsoft YaHei&quot;</span><span class="string">,</span> <span class="string">sans-serif</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 前端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Butterfly </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N Lecture 7 Machine Translation, Attention, Subword Models</title>
      <link href="2020/12/14/CS224N%20Lecture%207%20Machine%20Translation,%20Attention,%20Subword%20Models/"/>
      <url>2020/12/14/CS224N%20Lecture%207%20Machine%20Translation,%20Attention,%20Subword%20Models/</url>
      
        <content type="html"><![CDATA[<h3 id="Section-1-Machine-Translation"><a href="#Section-1-Machine-Translation" class="headerlink" title="Section 1: Machine Translation"></a>Section 1: Machine Translation</h3><p>机翻是一个把句子x从一种语言(源语言)翻译到另一种语言(目标语言)的句子y的任务。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214095100.png" alt="image-20210214095051158" style="zoom:16%;" /></p><h4 id="1990s-2010s-Statistical-Machine-Translation"><a href="#1990s-2010s-Statistical-Machine-Translation" class="headerlink" title="1990s-2010s: Statistical Machine Translation"></a>1990s-2010s: Statistical Machine Translation</h4><ul><li><p>核心思想：从数据中学习概率模型</p></li><li><p>假设我们在翻译法语→英语</p></li><li><p>我们希望给定法语句子x，找到最好的英语句子y</p><script type="math/tex; mode=display">\text{argmax }_y P(y \vert x) \tag{1}</script></li><li><p>使用贝叶斯法则将其分解为两个部分分开学习：</p><script type="math/tex; mode=display">= \text{argmax }_y P(x \vert y) P(y)\tag{2}</script></li></ul><p>上图中，x：法语， y：英语， 机器翻译的目标就是寻找y，使得$P(y \vert x)$最大，这就是式1. 接下来，我们利用贝叶斯分解为两个概率的乘积：其中$P(y )$用之前介绍过得语言模型，从英语单语的数据中学习怎么写的通顺。最简单的就用n-gram方法。$P(y \vert x)$，是指由目标语言到源语言的翻译模型，其专注于翻译模型，翻译好局部的单词和短语，这个模型学习来自于平行的数据语料。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214095426.png" alt="image-20210214095424905" style="zoom:18%;" /></p><h4 id="Learning-alignment-for-SMT"><a href="#Learning-alignment-for-SMT" class="headerlink" title="Learning alignment for SMT"></a>Learning alignment for SMT</h4><ul><li>Q: 怎么从平行语料中学习到翻译模型$P(x \vert y)$？</li><li>进一步分解这个任务：引入一个模型的隐变量$a$：$P(x, a \vert y)$</li><li>当a是对齐的，单词程度对应着源句子x到目标句子y。可以认为是两种语言之间单词和单词或短语和短语的一个对齐关系。</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214135931.png" alt="image-20210214135929263" style="zoom:18%;" /></p><h4 id="What-is-alignment"><a href="#What-is-alignment" class="headerlink" title="What is alignment?"></a>What is alignment?</h4><p>对齐是在翻译句子对中指定单词的对应关系。如下图就是英语和法语的alignment。</p><ul><li>不同语言直接的类型差异导致了复杂的对齐关系</li><li>注意：一些单词没有对应单词。</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214140544.png" alt="image-20210214140542816" style="zoom:16%;" /></p><p><strong>对齐是复杂的</strong></p><ul><li>多对一， 一对多， 一对一， 多对多</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214141138.png" alt="image-20210214141137092" style="zoom:18%;" /></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214141302.png" alt="image-20210214141300684" style="zoom:18%;" /></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214141324.png" alt="image-20210214141323292" style="zoom:18%;" /></p><h4 id="Learning-alignment-for-SMT-1"><a href="#Learning-alignment-for-SMT-1" class="headerlink" title="Learning alignment for SMT"></a>Learning alignment for SMT</h4><ul><li>我们学习$P(x, a \vert y)$作为许多组合因素，包括：<ul><li>指定单词对齐的概率(也取决于发送的位置)</li><li>指定单词的概率有指定丰富关系(对应单词的数目)</li><li>etc</li></ul></li><li>对齐a是一个隐变量：它们在数据中不明确具体。<ul><li>需要用特殊的算法(像最大期望算法) ，来学习有隐变量的参数分布。</li></ul></li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214141501.png" alt="image-20210214141459130" style="zoom:20%;" /></p><h4 id="Decoding-for-SMT"><a href="#Decoding-for-SMT" class="headerlink" title="Decoding for SMT"></a>Decoding for SMT</h4><p>如果计算argmax呢?</p><ul><li>穷举所有可能y然后计算这个概率? 代价太大！</li><li>答案是： 采用模型是完全独立的假设，使用动态规划获得全局最优解(像<strong>维特比算法</strong>)，启发式搜索。</li><li>这个过程称为解码</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214142538.png" alt="image-20210214142536962" style="zoom:20%;" /></p><p>在搜索过程中，对概率较低的路径进行剪枝，只保留概率较大的翻译路径。如下图的搜索树，对于概率较低的路径就不往下搜索了。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214143503.png" alt="image-20210214143501378" style="zoom: 20%;" /></p><p><strong>1990s-2010s： 统计机翻</strong></p><ul><li>SMT是一个非常大的研究领域</li><li>最好的系统极其复杂<ul><li>我们还有上百个关键细节没有提到</li><li>系统有许多独立设计的子组件</li><li>许多特征工程，需要设计特征来获取指定的语言现象</li><li>需要编译和维护额为的资源，像相等的短语表</li><li>维护需要大量人力，对每个语言对进行重复艰难的尝试！</li></ul></li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214144206.png" alt="image-20210214144203912" style="zoom:20%;" /></p><h3 id="Section-2-Neural-Machine-Translation"><a href="#Section-2-Neural-Machine-Translation" class="headerlink" title="Section 2: Neural Machine Translation"></a>Section 2: Neural Machine Translation</h3><h4 id="What-is-Neural-Machine-Translation"><a href="#What-is-Neural-Machine-Translation" class="headerlink" title="What is Neural Machine Translation?"></a>What is Neural Machine Translation?</h4><ul><li>神经网络机翻是是用单一端对端神经网络做机翻的方式</li><li>神经网络架构被称为sequence-to-sequence 模型(简称seq2seq)，其包含两个RNNs。</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214145950.png" alt="image-20210214145948971" style="zoom:20%;" /></p><h4 id="Neural-Machine-Translation-NMT"><a href="#Neural-Machine-Translation-NMT" class="headerlink" title="Neural Machine Translation (NMT)"></a>Neural Machine Translation (NMT)</h4><p>seq2seq模型如下图， 它由两个RNN构成。左边红色部分是Encoder RNN，它负责对源语言进行编码；右边的绿色部分称为Decoder RNN，它负责对目标语言进行解码（Decode）。首先，Encoder RNN可以是任意一个RNN，比如朴素RNN、LSTM或者GRU。Encoder RNN负责对源语言进行编码，学习源语言的隐含特征。Encoder RNN的最后一个神经元的隐状态作为Decoder RNN的初始隐状态。Decoder RNN是一个条件语言模型，一方面它是一个语言模型，即用来生成目标语言的；另一方面，它的初始隐状态是基于Encoder RNN的输出，所以称Decoder RNN是条件语言模型。Decoder RNN在预测的时候，需要把上一个神经元的输出作为下一个神经元的输入，不断的预测下一个词，直到预测输出了结束标志符<END>，预测结束。Encoder RNN的输入是源语言的word embeding，Decoder RNN的输入是目标语言的word embeding。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214150300.png" alt="image-20210214150257267" style="zoom: 20%;" /></p><p>Seq2seq是很强大的模型不仅仅用于MT，还有许多NLP任务能变大为seq2seq：</p><ul><li>文本概括</li><li>对话</li><li>解析</li><li><p>代码生成</p></li><li></li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214154407.png" alt="image-20210214154406061" style="zoom:20%;" /></p><ul><li>seq2seq是一个条件语言模型的例子<ul><li>LM因为解码是预测目标句子y的下一个词</li><li>条件是因为它预测也是建立在源句子x条件上</li></ul></li><li>NMT直接计算$P(y \vert x)$，如下图中所示。</li></ul><p>那么问题来了，怎么训练NMT系统呢？</p><ul><li>找一个大的平行语料</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214163622.png" alt="image-20210214163621078" style="zoom:20%;" /></p><h4 id="Training-a-Neural-Machine-Translation-system"><a href="#Training-a-Neural-Machine-Translation-system" class="headerlink" title="Training a Neural Machine Translation system"></a>Training a Neural Machine Translation system</h4><p>seq2seq被优化为一个单一系统。反向传播训练是“端对端”。如下图所示，将Encoder</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214154748.png" alt="image-20210214154746788" style="zoom:20%;" /></p><h4 id="Multi-layer-RNNs"><a href="#Multi-layer-RNNs" class="headerlink" title="Multi-layer RNNs"></a>Multi-layer RNNs</h4><p>多层RNNs：</p><ul><li>RNNs已经是在一个维度上是“深”的(将其展开成许多时刻)</li><li>我们也可以让其在另外的维度上变“深”，就通过应用多个RNNs——变成多层的RNN。</li><li>允许网络计算更复杂的表示，低层的RNNs应该计算低层的特征高层RNNs应该计算高层的特征</li><li>多层RNNs也叫堆叠RNNs</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214163427.png" alt="image-20210214163426512" style="zoom:20%;" /></p><h4 id="Multi-layer-deep-encoder-decoder-machine-translation-net"><a href="#Multi-layer-deep-encoder-decoder-machine-translation-net" class="headerlink" title="Multi-layer deep encoder-decoder machine translation net"></a>Multi-layer deep encoder-decoder machine translation net</h4><p>多层深度编码-解码机翻网络</p><p>来自RNN第i层的隐状态是RNN第i+1层的输入；左边红色部分多层RNN堆叠在一起作为编码器，来构建源句子的意思；右边绿色部分作为解码器来生成翻译。中间就像个瓶颈一样，叫做bottleneck。实际上多层RNNs应用技巧看上篇笔记。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214191855.png" alt="image-20210214191853097" style="zoom:20%;" /></p><h4 id="Greedy-decoding"><a href="#Greedy-decoding" class="headerlink" title="Greedy decoding"></a>Greedy decoding</h4><ul><li>我们看到怎样生成(或者说“解码”)目标句子，在解码的每个时间序列上采用argmax。</li><li>这就是贪婪解码(的在每个step上取最可能的单词)</li><li>这个方法的问题？</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214192823.png" alt="image-20210214192822013" style="zoom:20%;" /></p><p><strong>Problems with greedy decoding</strong>  </p><ul><li>贪婪解码没有办法回溯，那怎么修复这个问题呢?</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214193345.png" alt="image-20210214193344085" style="zoom:20%;" /></p><h4 id="Exhaustive-search-decoding"><a href="#Exhaustive-search-decoding" class="headerlink" title="Exhaustive search decoding"></a>Exhaustive search decoding</h4><p><strong>穷举搜索解码</strong></p><ul><li>理想情况下，我们想要找到一个(长度为T)让y最大化的翻译，如下图所示</li><li>我们尝试计算所有可能的序列y<ul><li>这意味着在解码器的每个step，我们遍历$V^t$个可能的部分翻译，其中$V$是词汇表的size，$O(V^t)$时间复杂度太高了！</li></ul></li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214193552.png" alt="image-20210214193551720" style="zoom:20%;" /></p><h4 id="Beam-search-decoding"><a href="#Beam-search-decoding" class="headerlink" title="Beam search decoding"></a>Beam search decoding</h4><p>束搜索解码</p><ul><li>核心思想：在解码器每一个step上，遍历k个最可能的部分翻译(这个我们叫假设)<ul><li>k是约束大小(实际上值为5-10)</li></ul></li><li>假设$y_1, \cdots, y_t$有一个其对数概率的分数，如下图所示<ul><li>所有分数是负数，分数越高表示更好的结果</li><li>我们在每个step上遍历Top-k个来搜索高分数假设,这里Top-k不仅仅是当前$\hat y$最高的几个概率，而是截止到目前这条路径上的累计概率之和。</li></ul></li><li>束搜索不确保找到最优解</li><li>但比穷举搜索更有效率</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214194330.png" alt="image-20210214194328812" style="zoom:20%;" /></p><p>如下图中，束搜索size=2时，第一个timestep保留的Top-2词为<code>he</code>和<code>I</code>，它们分别作为下一个timestep的输入。</p><p><code>he</code>输入预测输出Top-2是<code>hit</code>和<code>struck</code>，<code>hit</code>这条路径上累加概率是 <code>he</code>加上 <code>hit</code>为-1.7。同样计算其它词对应路径的概率分数。最后在这4条路径上保留$k=2$条路。所以保留 <code>hit</code>和 <code>was</code>。将其作为下一个timestep的输入；那么 <code>struck</code>和 <code>got</code>对应路径应该被剪枝。这就是，对k个假设逐个，找到top k下一个词并计算其分数。 </p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214200251.png" alt="image-20210214200249928" style="zoom:20%;" /></p><p>再下一个timestep同样计算，剪枝后保留<code>a</code>和<code>me</code>。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210215165422.png" alt="image-20210214200232110" style="zoom:20%;" /></p><p>最终搜索树如下图所示，以看到在每个时间步都只保留了k=2个节点往下继续搜索。最后<code>pie</code>对应的路径打分最高，通过回溯法得到概率最高的翻译句子。请注意，束搜索作为一种剪枝策略，并不能保证得到全局最优解，但它能以较大的概率得到全局最优解，同时相比于穷举搜索极大的提高了搜索效率。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214220134.png" alt="image-20210214220133325" style="zoom:20%;" /></p><h4 id="Beam-search-decoding-stopping-criterion"><a href="#Beam-search-decoding-stopping-criterion" class="headerlink" title="Beam search decoding: stopping criterion"></a>Beam search decoding: stopping criterion</h4><p>束搜索解码：停止标准</p><ul><li>在贪婪解码里，通常我们解码知道模型输出一个<code>&lt;END&gt;</code>结束标志符。<ul><li>例子： <code>&lt;START&gt;</code>he hit me with a pie <code>&lt;END&gt;</code></li></ul></li><li>在束约束搜索解码里，不同路径预测输出 <code>&lt;END&gt;</code> 结束标志符的timestep可能不一样，有些路径可能提取结束了，称为完全路径，暂时把这些完全路径放一边，其他路径接着束搜索。</li><li>通常我们继续束搜索直到：<ul><li>我们达到timestep T(其中T是提取设置的中断条件)</li><li>或者我们至少有n个完成的路径(其中n是提取设置的中断条件)</li></ul></li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214220625.png" alt="image-20210214220623849" style="zoom:20%;" /></p><h4 id="Beam-search-decoding-finishing-up"><a href="#Beam-search-decoding-finishing-up" class="headerlink" title="Beam search decoding: finishing up"></a>Beam search decoding: finishing up</h4><p>束搜索解码：结束</p><ul><li>我们有了完全的假设(路径)列表。</li><li>怎么选择有最高分的top one?</li><li>每个假设$y_1, \cdots, y_t$在列表里都有一个分数，如下图。</li><li>问题在于：假设越长，分数越低。在搜索路径上，累加得越多分越低，所有根据长度对打分进行归一化。</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214221531.png" alt="image-20210214221530118" style="zoom:20%;" /></p><blockquote><p>NMT相比于SMT的优点：</p><ol><li>性能更好，表现在：翻译更流程，RNN擅长语言模型；能更好的利用上下文关系；能利用短语相似性</li><li>模型更简单，只需要一个神经网络，端到端训练即可，简单方便</li><li>不需要很多的人工干预和特征工程，对于不同的语言，网络结构保持一样，只需要换一下词向量</li></ol><p>NMT的不足：</p><ol><li>难以解释，表现在：难以调试，难以解释出错原因</li><li>难以控制，比如难以加入一些人工的规则，难以控制NMT不输出什么，由此可能会脑一些笑话甚至导致安全问题</li></ol></blockquote><h4 id="How-do-we-evaluate-Machine-Translation"><a href="#How-do-we-evaluate-Machine-Translation" class="headerlink" title="How do we evaluate Machine Translation?"></a>How do we evaluate Machine Translation?</h4><p>怎么评价机翻?</p><p><strong>BLEU</strong> (Bilingual Evaluation Understudy  )</p><ul><li>BLEU将机翻和人工翻译(一个或多个)进行比较重叠部分，具体公式看Assignment 4，然后基于这个计算一个相似分数<ul><li>n-gram精度</li><li>过短的机翻加上一个惩罚</li></ul></li><li>BLEU 有用但不完美<ul><li>有许多有效的方法来翻译一个句子</li><li>所以一个好的翻译可能得到一个糟糕的分数因为它和人工翻译有较低的n-gram重叠</li></ul></li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214232535.png" alt="image-20210214232534422" style="zoom:20%;" /></p><h4 id="MT-progress-over-time"><a href="#MT-progress-over-time" class="headerlink" title="MT progress over time"></a>MT progress over time</h4><p>数据来源于(<a href="http://www.meta-net.eu/events/meta-forum-2016/slides/09_sennrich.pdf">http://www.meta-net.eu/events/meta-forum-2016/slides/09_sennrich.pdf</a>)</p><blockquote><p>神经机器翻译于2014年从边缘研究活动到2016年成为领先标准方法</p><ul><li>2014：第一篇 seq2seq 的文章发布</li><li>2016：谷歌翻译从 SMT 换成了 NMT</li><li>这是惊人的，由数百名工程师历经多年打造的SMT系统，在短短几个月内就被少数工程师训练过的NMT系统超越</li></ul><p><strong>So is Machine Translation solved?</strong></p><ul><li>不！</li><li>许多困难仍然存在<ul><li>词表外的单词处理</li><li>训练和测试数据之间的 <strong>领域不匹配</strong></li><li>在较长文本上维护上下文</li><li>资源较低的语言对</li></ul></li><li>使用常识仍然很难</li></ul></blockquote><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214233124.png" alt="image-20210214233122492" style="zoom:20%;" /></p><h4 id="NMT-research-continues"><a href="#NMT-research-continues" class="headerlink" title="NMT research continues"></a>NMT research continues</h4><p>NMT是NLP深度学习的旗舰</p><ul><li>NMT研究引领了NLP深度学习的许多创新</li><li>在2021：NMT研究继续蓬勃发展<ul><li>研究这已经找到许多，许多改进在我们刚刚提到的“原生的” seq2seq NMT系统</li><li>但是我们将在接下来介绍新的一种不可或缺的改进——Attention!</li></ul></li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214233514.png" alt="image-20210214233512856" style="zoom:20%;" /></p><h3 id="Section-3-Attention"><a href="#Section-3-Attention" class="headerlink" title="Section 3: Attention"></a>Section 3: Attention</h3><h4 id="Sequence-to-sequence-the-bottleneck-problem"><a href="#Sequence-to-sequence-the-bottleneck-problem" class="headerlink" title="Sequence-to-sequence: the bottleneck problem"></a>Sequence-to-sequence: the bottleneck problem</h4><p>seq2seq: 瓶颈问题</p><p>在下图中，Encoder RNN 的最后一个神经元的隐状态作为Decoder RNN的初始隐状态，也就是说Encoder的最后一个隐状态向量需要承载源句子的所有信息，成为整个模型的“信息”瓶颈。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214234912.png" alt="image-20210214234910353" style="zoom:20%;" /></p><p>在最后一个隐藏状态，需要获取的全部关于源句子的信息。这就是瓶颈问题。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214235213.png" alt="image-20210214235212101" style="zoom:20%;" /></p><h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><ul><li>注意机制提供了解决瓶颈问题的方法</li><li>核心思想：在每一步都解码，用直连解码器来专注于源句子中特殊的部分</li><li>开始我们用图来说明，然后用方程。</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210214235653.png" alt="image-20210214235651799" style="zoom:20%;" /></p><h4 id="Sequence-to-sequence-with-attention"><a href="#Sequence-to-sequence-with-attention" class="headerlink" title="Sequence-to-sequence with attention"></a>Sequence-to-sequence with attention</h4><blockquote><p>具体来说，在时刻t，Decoder第t时刻的隐状态$s_t$和Encoder所有时刻的隐状态$h_1, \cdots, h_n$做点积，得到N个标量Attention score，作为Encoder每个隐状态的权重，然后使用softmax对这些权重进行归一化，得到Attention distribution。这个Attention distribution相当于Decoder在时刻t对Encoder所有隐状态的注意力分布，如下第3张图所示，”he”时刻的注意力主要分布在Encoder的第2和第4个词上，这不正是前面介绍的SMT的对齐alignment操作吗！Attention自动学习到了这种对齐操作，只不过是soft alignment。</p><p>接下来，对Encoder所有隐状态使用Attention distribution进行加权平均，得到Attention output $a_t$。把$a_t$和该时刻的隐状态$s_t$拼起来再进行非线性变换得到输出$\hat y_2$。有时，也可以把上一时刻的Attention output和当前的输入词向量拼起来作为一个新的输入，输入到Decoder RNN中。</p></blockquote><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210215144452.png" alt="image-20210215144443366" style="zoom:20%;" /></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210215144933.png" alt="image-20210215144932320" style="zoom:20%;" /></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210215145002.png" alt="image-20210215145000723" style="zoom:20%;" /></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210215150328.png" alt="image-20210215150326981" style="zoom:20%;" /></p><h4 id="Attention-in-equations"><a href="#Attention-in-equations" class="headerlink" title="Attention: in equations"></a>Attention: in equations</h4><ul><li>编码器的隐状态$h_1, \cdots, h_n \in \mathbb{R}^h$</li><li>在timestep $t$, 解码器隐状态为$s_t \in \mathbb{R}^h$</li><li>得到该时刻$t$的注意力分数</li></ul><script type="math/tex; mode=display">\mathbf{e}^t = [\mathbf{s}_t^T\mathbf{h}_1, \cdots,\mathbf{s}_t^T\mathbf{h}_N   ] \in \mathbb{R}^N</script><ul><li>对$\mathbf{e}^t $取softmax得到注意力分布$\mathbf{\alpha}^t $(这是一个概率分布，和为1)</li></ul><script type="math/tex; mode=display">\mathbf{\alpha}^t = \text{softmax}(\mathbf{e}^t ) \in \mathbb{R}^N</script><ul><li>将$\mathbf{\alpha}^t $  和编码器隐状态$\mathbf{h} $点乘得到注意力输出$\mathbf{a}_t $</li></ul><script type="math/tex; mode=display">\mathbf{a}_t = \sum_{i=1}^N \alpha_i^t\mathbf{h}_i \in \mathbb{R}^N</script><ul><li>最后，将注意力输出$\mathbf{a}_t $和解码器隐状态$s_t$拼起来，并按照非注意力seq2seq模型运行$[\mathbf{a}_t; \mathbf{s}_t  ] \in \mathbb{R}^{2h}$</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210215151757.png" alt="image-20210215151755989" style="zoom:20%;" /></p><h4 id="Attention-is-great"><a href="#Attention-is-great" class="headerlink" title="Attention is great"></a>Attention is great</h4><blockquote><ul><li>注意力显著提高了NMT性能<ul><li>这是非常有用的，让解码器专注于某些部分的源语句</li></ul></li><li>注意力解决瓶颈问题<ul><li>注意力允许解码器直接查看源语句；绕过瓶颈</li></ul></li><li>注意力帮助消失梯度问题<ul><li>提供了跟短接远处状态的方式</li></ul></li><li>注意力提供了一些可解释性<ul><li>通过检查注意力的分布，我们可以看到解码器在关注什么</li><li>我们可以轻松得到(软)对齐</li><li>这很酷，因为我们从来没有明确训练过对齐系统</li><li>网络只是自主学习了对齐</li></ul></li></ul></blockquote><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210215153929.png" alt="image-20210215153928196" style="zoom:20%;" /></p><h4 id="Attention-is-a-general-Deep-Learning-technique"><a href="#Attention-is-a-general-Deep-Learning-technique" class="headerlink" title="Attention is a general Deep Learning technique"></a>Attention is a general Deep Learning technique</h4><ul><li>我们已经看到attention是一种改进机翻的seq2seq模型很好的方法</li><li>然而，你可以在许多架构中用attention</li><li>注意力更普遍的定义是：<ul><li>给定一系列向量的值，或者向量查询队列，attention是一种计算关于查询队列与向量值权重和的技术</li></ul></li><li>我们有时说，查询注意到哪些值</li><li>例如，在seq2seq + attention 模型中，每个解码器的隐状态查询注意到首页解码器隐状态的某一些值。</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210215154327.png" alt="image-20210215154326539" style="zoom:20%;" /></p><p><strong>直觉</strong></p><ul><li>加权和是值中包含的信息的选择性汇总，查询在其中确定要关注哪些值</li><li>注意是一种获取任意一组表示(值)的固定大小表示的方法，依赖于其他一些表示(查询)。</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210215154346.png" alt="image-20210215154345002" style="zoom:20%;" /></p><h4 id="There-are-several-attention-variants"><a href="#There-are-several-attention-variants" class="headerlink" title="There are several attention variants"></a>There are several attention variants</h4><ul><li>一个值的集合$\mathbf{h}_1, \cdots, \mathbf{h}_n \in \mathbb{R}^{d_1}$，一个 查询$\mathbf{s} \in \mathbb{R}^{d_2}$</li><li>Attention总是包括<ul><li>计算注意力分数</li><li>对分数取softmax得到注意力分布</li><li>用注意力分布对隐状态值求权重和来获得注意力输出$\mathbf{a}$</li></ul></li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210215155634.png" alt="image-20210215155633285" style="zoom:20%;" /></p><p><strong>Attention 变种</strong></p><p>其实就是计算从编码器隐状态$\mathbf{h}_1, \cdots, \mathbf{h}_N \in \mathbb{R}^{d_1}$和解码器隐状态$\mathbf{s} \in \mathbb{R}^{d_2}$计算attention分数$\mathbf{e} \in \mathbb{R}^{N}$的几种方式：</p><ul><li>基本点积注意力：$\mathbf{e}_i = \mathbf{s}^T \mathbf{h}_i \in \mathbb{R}$，注假设$d_1= d2$,这就是我们之前见到的</li><li>乘法注意力：$\mathbf{e}_i = \mathbf{s}^T \mathbf{W} \mathbf{h}_i \in \mathbb{R}$, 其中$\mathbf{W} \in \mathbb{R}^{d_2 \times d_1}$是权重矩阵</li><li>加法注意力：$\mathbf{e}_i = \mathbf{v}^T \text{tanh }(\mathbf{W}_1 \mathbf{h}_i + \mathbf{W}_2 \mathbf{s}) \in \mathbb{R}$. 其中$\mathbf{W}_1 \in \mathbb{R}^{d_3 \times d_1}, \ \mathbf{W}_2 \in \mathbb{R}^{d_3 \times d_2}$是权重矩阵，$\mathbf{v}\in \mathbb{R}^{d_3}$是权重向量，$d_3$是超参数</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210215155710.png" alt="image-20210215155708656" style="zoom:20%;" /></p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210215164338.png" alt="image-20210215164336654" style="zoom:20%;" /></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>[1] <a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture07-nmt.pdf">cs224n-2021-lecture07-nmt</a></p><p>[2] <a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf">cs224n-2019-notes06-NMT_seq2seq_attention</a></p><p>[3] <a href="https://ruder.io/deep-learning-nlp-best-practices/index.html">Deep Learning for NLP Best Practices</a></p><p>[4] <a href="https://bitjoy.net/2019/08/02/cs224n%ef%bc%881-31%ef%bc%89translation-seq2seq-attention/">CS224N（1.31）Translation, Seq2Seq, Attention</a></p><p>[5] <a href="https://blog.csdn.net/u011613991/article/details/105616634">Machine Translation, Seq2Seq and Attention</a></p><p>[6] <a href="https://looperxx.github.io/CS224n-2019-08-Machine%20Translation%2C%20Sequence-to-sequence%20and%20Attention/">Science is interesting.08 Machine Translation, Sequence-to-sequence and Attenti</a></p><p>[7] <a href="https://blog.csdn.net/RHJlife/article/details/107321601">MachineTranslation seq2seq Attention</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS224n </tag>
            
            <tag> 机翻 </tag>
            
            <tag> SMT </tag>
            
            <tag> NMT </tag>
            
            <tag> Beam search </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1. Hexo + Matery blog 搭建</title>
      <link href="2020/12/14/hexo+matrey%20blog%E6%90%AD%E5%BB%BA/"/>
      <url>2020/12/14/hexo+matrey%20blog%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Hexo-Matery-blog-搭建"><a href="#1-Hexo-Matery-blog-搭建" class="headerlink" title="1. Hexo + Matery blog 搭建"></a>1. Hexo + Matery blog 搭建</h2><h3 id="1-卸载以前安装-hexo"><a href="#1-卸载以前安装-hexo" class="headerlink" title="1. 卸载以前安装 hexo"></a>1. 卸载以前安装 hexo</h3><h4 id="1-卸载-hexo-及-重新安装"><a href="#1-卸载-hexo-及-重新安装" class="headerlink" title="1. 卸载 hexo 及 重新安装"></a>1. 卸载 hexo 及 重新安装</h4><p><strong>记得备份以前文档</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sudo npm uninstall -g hexo-cli</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">apt-get 卸载</span></span><br><span class="line">sudo apt-get remove --purge npm</span><br><span class="line">sudo apt-get remove --purge nodejs</span><br><span class="line">sudo apt-get remove --purge nodejs-legacy</span><br><span class="line">sudo apt-get autoremove</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">手动删除 npm 相关目录</span></span><br><span class="line">sudo rm -r /usr/local/bin/npm</span><br><span class="line">sudo rm -r /usr/local/lib/node-moudels</span><br><span class="line">sudo find / -name npm</span><br><span class="line"></span><br><span class="line">sudo rm -rf /usr/local/lib/*</span><br><span class="line"></span><br><span class="line">sudo rm -r /tmp/npm* </span><br></pre></td></tr></table></figure><h4 id="2-重新安装nodejs-npm-hexo"><a href="#2-重新安装nodejs-npm-hexo" class="headerlink" title="2. 重新安装nodejs/ npm/ hexo"></a>2. 重新安装nodejs/ npm/ hexo</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">重新安装nodejs</span></span><br><span class="line">sudo curl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash -</span><br><span class="line">sudo apt-get update &amp;&amp; sudo apt-get install yarn</span><br><span class="line">sudo apt-get install nodejs</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">查看版本</span></span><br><span class="line">node -v</span><br><span class="line">npm -v</span><br><span class="line"><span class="meta">#</span><span class="bash"> 升级nodejs和npm</span></span><br><span class="line">sudo npm install n -g</span><br><span class="line">sudo n stable</span><br><span class="line">sudo node -v</span><br><span class="line"></span><br><span class="line">sudo npm i -g npm to update</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装 hexo</span></span><br><span class="line">sudo npm install -g hexo-cli</span><br><span class="line">hexo init</span><br></pre></td></tr></table></figure><h3 id="2-安装和配置Matery"><a href="#2-安装和配置Matery" class="headerlink" title="2. 安装和配置Matery"></a>2. 安装和配置Matery</h3><h4 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h4><p>在<code>hexo</code>的<code>themes</code>目录下：</p><p><a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md">Matery文档</a></p><p><a href="https://blinkfox.github.io/2018/09/28/qian-duan/hexo-bo-ke-zhu-ti-zhi-hexo-theme-matery-de-jie-shao/">blog实例</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/blinkfox/hexo-theme-matery.git</span><br></pre></td></tr></table></figure><p>修改 Hexo 根目录下的 <code>_config.yml</code> 的 <code>theme</code> 的值为：<code>theme: hexo-theme-matery</code></p><h4 id="2-插件"><a href="#2-插件" class="headerlink" title="2. 插件"></a>2. 插件</h4><ol><li><code>sudo npm uninstall hexo-prism-plugin</code>， 如果安装过prism插件，卸载掉！然后修改 Hexo 根目录下 <code>_config.yml</code></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">highlight:</span><br><span class="line">  enable: false</span><br><span class="line">  line_number: true</span><br><span class="line">  auto_detect: false</span><br><span class="line">  tab_replace: &#x27;&#x27;</span><br><span class="line">  wrap: true</span><br><span class="line">  hljs: false</span><br><span class="line">prismjs:</span><br><span class="line">  enable: true</span><br><span class="line">  preprocess: true</span><br><span class="line">  line_number: true</span><br><span class="line">  tab_replace: &#x27;&#x27;</span><br></pre></td></tr></table></figure><ol><li><code>generator-search</code> 安装搜索插件，并在Hexo 根目录下 <code>_config.yml</code>加上3-5行</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install hexo-generator-search --save            #搜索</span><br><span class="line"></span><br><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br></pre></td></tr></table></figure><ol><li><code>hexo-permalink-pinyin</code> 安装链接中文转拼音插件，并在Hexo 根目录下 <code>_config.yml</code>加上3-5行</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo npm i hexo-permalink-pinyin --save</span><br><span class="line"></span><br><span class="line">permalink_pinyin:</span><br><span class="line">  enable: true</span><br><span class="line">  separator: &#x27;-&#x27; # default: &#x27;-&#x27;</span><br></pre></td></tr></table></figure><ol><li><code>hexo-wordcount</code> 安装字数统计插件<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm i --save hexo-wordcount</span><br></pre></td></tr></table></figure>并在matery主题目录下 <code>_config.yml</code>加上<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">postInfo:</span><br><span class="line">  date: true</span><br><span class="line">  update: false</span><br><span class="line">  wordCount: false # 设置文章字数统计为 true.</span><br><span class="line">  totalCount: false # 设置站点文章总字数统计为 true.</span><br><span class="line">  min2read: false # 阅读时长.</span><br><span class="line">  readCount: false # 阅读次数.</span><br></pre></td></tr></table></figure></li><li><code>hexo-filter-github-emojis</code> 安装emoji插件<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install hexo-filter-github-emojis --save</span><br></pre></td></tr></table></figure>在Hexo 根目录下 <code>_config.yml</code>加上<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">githubEmojis:</span><br><span class="line">  enable: true</span><br><span class="line">  className: github-emoji</span><br><span class="line">  inject: true</span><br><span class="line">  styles:</span><br><span class="line">  customEmojis:</span><br></pre></td></tr></table></figure></li><li><code>hexo-generator-feed</code> 安装订阅插件<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install hexo-generator-feed --save #RSS</span><br></pre></td></tr></table></figure>在 Hexo 根目录下的 _config.yml 文件中，新增以下的配置项：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">feed:</span><br><span class="line">  type: atom</span><br><span class="line">  path: atom.xml</span><br><span class="line">  limit: 20</span><br><span class="line">  hub:</span><br><span class="line">  content:</span><br><span class="line">  content_limit: 140</span><br><span class="line">  content_limit_delim: &#x27; &#x27;</span><br><span class="line">  order_by: -date</span><br></pre></td></tr></table></figure></li><li>其它插件安装<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install hexo-deployer-git --save                   #git部署</span><br><span class="line">sudo npm install hexo-generator-seo-friendly-sitemap --save #sitemap</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>公式插件插件安装</p><p>卸载<code>hexo-renderer-marked</code> 不然大量行内公式不渲染，安装<code>install hexo-renderer-kramed</code>.</p></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo npm uninstall hexo-renderer-marked --save</span><br><span class="line">sudo npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure><p><a href="https://qingstudios.com/2020/03/01/Hexo%E4%B8%AD%E6%8F%92%E5%85%A5%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/">参考数学公式解决方法</a></p><p>找到<code>hexo博客目录/node_modules/hexo-renderer-kramed/lib/renderer.js</code>，把下面代码修改。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">formatText</span>(<span class="params">text</span>) </span>&#123;</span><br><span class="line">    <span class="comment">// Fit kramed&#x27;s rule: $$ + \1 + $$</span></span><br><span class="line">    <span class="keyword">return</span> text.replace(<span class="regexp">/`\$(.*?)\$`/g</span>, <span class="string">&#x27;$$$$$1$$$$&#x27;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Change inline math rule</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">formatText</span>(<span class="params">text</span>) </span>&#123;</span><br><span class="line">  <span class="comment">// Fit kramed&#x27;s rule: $$ + \1 + $$</span></span><br><span class="line"><span class="comment">//  return text.replace(/`\$(.*?)\$`/g, &#x27;$$$$$1$$$$&#x27;);</span></span><br><span class="line">  <span class="keyword">return</span> text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>安装<code>mathjax</code> ,如果 已安装<code>hexo-math</code> 用<code>sudo npm uninstall hexo-math --save</code>卸载.</p><p>这样不能完全解决公式问题，还需要将不能渲染行内公式<script type="math/tex"> </script>转变为 $$$ $$$。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install hexo-renderer-mathjax --save #mathjax</span><br></pre></td></tr></table></figure><h4 id="3-proxy"><a href="#3-proxy" class="headerlink" title="3. proxy"></a>3. proxy</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo npm config set proxy=socks://127.0.0.1:1080</span><br><span class="line">sudo npm config set proxy=http://127.0.0.1:8885</span><br><span class="line">sudo npm config set registry=http://registry.npmjs.org</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="4-post-头和"><a href="#4-post-头和" class="headerlink" title="4. post 头和"></a>4. post 头和</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">typora</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2020-09-07 09:25:00</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">miller</span></span><br><span class="line"><span class="attr">img:</span> <span class="string">/source/images/xxx.jpg</span></span><br><span class="line"><span class="attr">top:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">cover:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">coverImg:</span> <span class="string">/images/1.jpg</span></span><br><span class="line"><span class="attr">password:</span> <span class="string">8d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92</span></span><br><span class="line"><span class="attr">toc:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">mathjax:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">summary:</span> <span class="string">这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</span></span><br><span class="line"><span class="attr">categories:</span> <span class="string">Markdown</span></span><br><span class="line"><span class="attr">tags:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Typora</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Markdown</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#a131de</span></span><br><span class="line"></span><br><span class="line"><span class="string">.bg-color</span> &#123;</span><br><span class="line">    <span class="attr">background-image:</span> <span class="string">linear-gradient(to</span> <span class="string">right</span>, <span class="comment">#4cbf30 0%, #0f9d58 100%);</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="string">sudo</span> <span class="string">npm</span> <span class="string">install</span> <span class="string">hexo-heading-index</span> <span class="string">--save</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 前端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Matery </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N Lecture 6 Vanishing Gradients, Fancy RNNs, Seq2Seq</title>
      <link href="2020/12/12/CS224N%20Lecture%206%20Vanishing%20Gradients,%20Fancy%20RNNs,%20Seq2Seq/"/>
      <url>2020/12/12/CS224N%20Lecture%206%20Vanishing%20Gradients,%20Fancy%20RNNs,%20Seq2Seq/</url>
      
        <content type="html"><![CDATA[<h3 id="The-Simple-RNN-Language-Model"><a href="#The-Simple-RNN-Language-Model" class="headerlink" title="The Simple RNN Language Model"></a>The Simple RNN Language Model</h3><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210211185500.png" alt="image-20210211185344655" style="zoom:20%;" /></p><p>简单的RNN语言模型。</p><h4 id="Training-an-RNN-Language-Model"><a href="#Training-an-RNN-Language-Model" class="headerlink" title="Training an RNN Language Model"></a>Training an RNN Language Model</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210211190810.png" alt="image-20210211190631508" style="zoom:20%;" /></p><blockquote><p>RNN在$t$时刻的输出是预测第$t+1$个词的概率分布$y^(t)$；而对于训练集中给定的文本来说，第$t+1$个词是已知的某个词，所以真实答案$y(t)$其实是一个one-hot向量，在第$x(t+1)$位置为1，其他位置为0。所以如果是交叉熵损失函数的话，表达式如上图中间的等式。RNN整体的损失就是所有时刻的损失均值。</p></blockquote><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210211191029.png" alt="image-20210211191026911" style="zoom:20%;" /></p><p>输入序列<code>the students opened their</code> 预测后面的词。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210211191139.png" alt="image-20210211191137637" style="zoom:20%;" /></p><p>训练RNN语言模型，其实loss是所有时刻t的损失的均值。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210211191636.png" alt="image-20210211191633630" style="zoom:20%;" /></p><blockquote><ul><li><p>然而：计算 <strong>整个语料库</strong> 的损失和梯度太昂贵了</p></li><li><p>在实践中，输入序列实际上是一个 <strong>句子</strong> 或是 <strong>文档</strong>，这就跟随机梯度下降法中，随机抽小批样本一样。</p></li><li>回忆 ：随机梯度下降允许我们计算小块数据的损失和梯度，并进行更新。</li><li>计算一个句子的损失(实际上是一批句子)，计算梯度和更新权重。重复上述操作。</li></ul></blockquote><h4 id="Training-the-parameters-of-RNNs-Backpropagation-for-RNNs"><a href="#Training-the-parameters-of-RNNs-Backpropagation-for-RNNs" class="headerlink" title="Training the parameters of RNNs: Backpropagation for RNNs"></a>Training the parameters of RNNs: Backpropagation for RNNs</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210211192118.png" alt="image-20210211192116116" style="zoom:20%;" /></p><p>损失函数对权重矩阵W等参数的偏导是什么？</p><h3 id="Problems-with-Vanishing-and-Exploding-Gradients"><a href="#Problems-with-Vanishing-and-Exploding-Gradients" class="headerlink" title="Problems with Vanishing and Exploding Gradients"></a>Problems with Vanishing and Exploding Gradients</h3><h4 id="Vanishing-gradient-intuition"><a href="#Vanishing-gradient-intuition" class="headerlink" title="Vanishing gradient intuition"></a>Vanishing gradient intuition</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210212132757.png" alt="image-20210212125204219" style="zoom:20%;" /></p><p>梯度消失的直觉，根据链式法则，求每个节点对隐藏变量$\mathbf{h}_i$的梯度时，如果其接近0时，传播越深，梯度信号就会越来越小，直至消失。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210212140059.png" alt="image-20210212140058181" style="zoom:20%;" /></p><h4 id="Vanishing-gradient-proof-sketch-linear-case"><a href="#Vanishing-gradient-proof-sketch-linear-case" class="headerlink" title="Vanishing gradient proof sketch (linear case)"></a>Vanishing gradient proof sketch (linear case)</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210212140206.png" alt="image-20210212140204624" style="zoom: 20%;" /></p><ul><li><p>对于隐藏层$\mathbf{h}^{(t)}$来说，就是激活函数作用到，上一个隐藏输入和输入分布的权重和加上偏置。</p></li><li><p>如果激活函数是恒等函数，比如$\sigma(x) = x$,<br>其梯度就是上面的整体的导数的对角阵再乘以 <script type="math/tex">\mathbf{W}_{h}</script> ；即单位矩阵乘以 <script type="math/tex">\mathbf{W}_{h}</script></p></li><li><p>考虑第$i$时刻损失 <script type="math/tex">\mathbf{J}^{(i)}(\theta)</script> 对于在之前时刻 <script type="math/tex">j</script> 隐藏层 <script type="math/tex">\mathbf{h}^{(j)}</script> 的梯度，令  <script type="math/tex">l = i-j</script></p><p>如果$\mathbf{W}_{h}$本身非常小，当$l$变大时，这项会产生指数问题。</p></li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210212143127.png" alt="image-20210212143125560" style="zoom:20%;" /></p><ul><li>$\mathbf{W}_{h}^{l}$什么问题?</li><li>假设$\mathbf{W}_{h}$的特征值都小于1:</li><li>可以将其梯度写作$\mathbf{W}_{h}$的一组基的形式，这时，就会导致梯度消失</li><li>那非线性激活函数$\sigma$呢？也一样，除了需要证明对于在维度上一些$\gamma$独立和$\sigma$有$\lambda_i &lt; \gamma$</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210212144558.png" alt="image-20210212144556708" style="zoom:20%;" /></p><p>为什么梯度消失是一个问题？</p><ul><li>梯度信号从远处传来会消失是因为，它比近处的小得多</li><li>因此，模型权重只会更新对应的近处影响，而不是长期的影响</li></ul><h4 id="Effect-of-vanishing-gradient-on-RNN-LM"><a href="#Effect-of-vanishing-gradient-on-RNN-LM" class="headerlink" title="Effect of vanishing gradient on RNN-LM"></a>Effect of vanishing gradient on RNN-LM</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210212144855.png" alt="image-20210212144854345" style="zoom:20%;" /></p><ul><li><strong>语言模型的任务</strong>：预测下一个词</li><li>为了从训练样本学习到这个，语言模型需要建立依赖于在第7步“tickets” 和最后目标词“tickets”之间的的模型。</li><li>但是如果梯度很小，模型就无法学习到这个依赖关系。因此模型无法在测试时预测类似的长距离依赖关系</li></ul><h4 id="Why-is-exploding-gradient-a-problem"><a href="#Why-is-exploding-gradient-a-problem" class="headerlink" title="Why is exploding gradient a problem?"></a>Why is exploding gradient a problem?</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210212150152.png" alt="image-20210212150150737" style="zoom:20%;" /></p><ul><li>如果梯度变得过大，那么SGD更新步伐将变大</li><li><p>这将导致错误的更新：我们取太大的步伐，到达一个怪异的错误的参数配置(产生大的loss)</p><ul><li>你想下你发现一个斜坡来跳跃，但突然你在低谷了</li></ul></li><li><p>在最坏的情况下，这会在你网络中导致NaN(那么你得从之前某个checkpoint重新训练)</p></li></ul><p><strong>Gradient clipping: solution for exploding gradient</strong>  </p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210212150901.png" alt="image-20210212150859173" style="zoom:20%;" /></p><ul><li><strong>梯度裁剪</strong>：就像算法1中，如果梯度的模大于阈值，就在SGD之前缩小它</li><li><strong>直觉</strong>：取同样方向的一步，不过是一小步</li><li>实际上，记得裁剪梯度是非常重要的，但梯度爆炸是一个容易解决的问题。</li></ul><p><strong>How to fix the vanishing gradient problem?</strong>  </p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210212151221.png" alt="image-20210212151219392" style="zoom:20%;" /></p><ul><li><p>主要问题是在，对于RNN很难学到遍历许多时刻后保留信息</p></li><li><p>在RNN变种里，隐藏状态不断被重写</p></li><li><p>RNN有独立记忆怎么样?</p><p>引入LSTM</p></li></ul><h3 id="Long-Short-Term-Memory-RNNs-LSTMs"><a href="#Long-Short-Term-Memory-RNNs-LSTMs" class="headerlink" title="Long Short-Term Memory RNNs (LSTMs)"></a>Long Short-Term Memory RNNs (LSTMs)</h3><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210212151629.png" alt="image-20210212151628214" style="zoom:20%;" /></p><ul><li><p>在1997年一种类型的RNN被<a href="https://www.bioinf.jku.at/publications/older/2604.pdf">Hochreiter and Schmidhuber  </a> 提出，其作为梯度消失问题的解决措施。</p><ul><li>每个人引用这篇论文，但是现代LSTM真正关键部分源自Gers <a href="https://dl.acm.org/doi/10.1162/089976600300015015"> Learning to Forget: Continual Prediction with LSTM</a></li></ul></li><li><p>在时刻$t$, 有一个隐藏状态$\mathbf{h}^{(t)}$和cell状态$\mathbf{c}^{(t)}$</p><ul><li>都是长度为$n$的向量</li><li>cell存储长期信息</li><li>LSTM能从cell中读取、删除，和写入信息 ，cell是一个概念而不是计算机里的RAM</li></ul></li><li>通过控制三个对应的门来删除、写入、读取选择性的信息<ul><li>门也是长为$n$的向量</li><li>在每个时刻，每个门元素能打开(1), 关闭(0)，或者介于之间状态</li><li>门是动态的：基于当前上下文来计算它们的值</li></ul></li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210212160623.png" alt="image-20210212160622165" style="zoom:20%;" /></p><p>我们有一个输入序列$\mathbf{x}^{(t)}$,要计算隐藏状态$\mathbf{h}^{(t)}$序列和cell状态$\mathbf{c}^{(t)}$.在时刻$t$:</p><ul><li>遗忘门：控制上一个cell状态的保持和遗忘，$\mathbf{f}^{(t)}=\sigma(\mathbf{W}_f\mathbf{h}^{(t-1)}+\mathbf{U}_f\mathbf{x}^{(t)} + \mathbf{b}_f)$</li><li>输入门：控制新cell的哪些内容写入到cell， $\mathbf{i}^{(t)}=\sigma(\mathbf{W}_i\mathbf{h}^{(t-1)}+\mathbf{U}_i\mathbf{x}^{(t)} + \mathbf{b}_i)$</li><li>输出门：控制哪些cell的部分输出到隐藏层， $\mathbf{o}^{(t)}=\sigma(\mathbf{W}_o\mathbf{h}^{(t-1)}+\mathbf{U}_o\mathbf{x}^{(t)} + \mathbf{b}_o)$</li><li>新cell内容：写入cell的新内容： $\mathbf{\tilde c}^{(t)}=\text{tanh}(\mathbf{W}_c\mathbf{h}^{(t-1)}+\mathbf{U}_c\mathbf{x}^{(t)} + \mathbf{b}_c)$</li><li>Cell状态： 删除上一个cell“forget”中一些内容，写入“input”一些新cell内容：$\mathbf{c}^{(t)}=\mathbf{f}^{(t)}\circ \mathbf{c}^{(t-1)}+\mathbf{i}^{(t)}\circ \mathbf{\tilde c}^{(t)}$</li><li>隐藏状态：读取从cell中“输出”一些内容：$\mathbf{h}^{(t)}=\mathbf{o}^{(t)}\circ \text{tanh} \mathbf{c}^{(t)}$</li></ul><p>注意：门都是应用的逐元素积或者说Hadamard积。</p><p><img src="C:/Users/yy/AppData/Roaming/Typora/typora-user-images/image-20210213003729927.png" alt="image-20210213003729927" style="zoom: 20%;" /></p><p>你可以把LSTM方程可视像上图一样， <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">colah LSTM</a></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210213003945.png" alt="image-20210213003942932" style="zoom: 20%;" /></p><p>具体来说，来看每个门的作用。</p><h4 id="How-does-LSTM-solve-vanishing-gradients"><a href="#How-does-LSTM-solve-vanishing-gradients" class="headerlink" title="How does LSTM solve vanishing gradients?"></a>How does LSTM solve vanishing gradients?</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210213004957.png" alt="image-20210213004955463" style="zoom: 20%;" /></p><p><strong>LSTM怎么解决梯度消失问题?</strong></p><ul><li>LSTM比RNN更容易保留许多时刻后的信息。<ul><li>比如，如果对于一个cell ，遗忘门设置为1，输入门设置为0，那么这个cell的信息被无限地保留</li><li>相比，普通RNN很难学到循环权重矩阵$\mathbf{W}_h$, 其保留在隐藏层state的信息</li><li>实际上，你能得到大约100个时刻而不是7个时刻</li></ul></li><li>LSTM不确保，没有梯度消失，但是提供了让模型更容易学习长矩依赖的方式</li></ul><h4 id="LSTMs-real-world-success"><a href="#LSTMs-real-world-success" class="headerlink" title="LSTMs: real-world success"></a>LSTMs: real-world success</h4><p><img src="C:/Users/yy/AppData/Roaming/Typora/typora-user-images/image-20210213151650925.png" alt="image-20210213151650925" style="zoom:20%;" /></p><p><strong>LSTM：现实的成功</strong></p><ul><li>在2013——2015，LSTMs开始取得最先进的成果：<ul><li>成功的任务包含，手写识别，语音识别，机翻，解析，和图片标题，以及语言模型</li><li>LSTM成为大部分NLP任务的统治方法</li></ul></li><li>在2021年，别的方法（如Transformers）已经成为许多任务的统治方法<ul><li>例如，在WMT</li><li>2016 WMT，总结报告包含 “RNN” 44次</li><li>2019 WMT ， “RNN” 7 次， “Transformer” 105次</li></ul></li></ul><h4 id="Is-vanishing-exploding-gradient-just-a-RNN-problem"><a href="#Is-vanishing-exploding-gradient-just-a-RNN-problem" class="headerlink" title="Is vanishing/exploding gradient just a RNN problem?"></a>Is vanishing/exploding gradient just a RNN problem?</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210213152608.png" alt="image-20210213152606764" style="zoom:20%;" /></p><p>梯度消失/爆炸仅仅是RNN中的问题吗？</p><ul><li><p>不是！所有的神经网络(包括前馈和卷积网络)都会存在梯度爆炸或是梯度消失的问题，特别是很深的网络。</p><ul><li>由于链式法则/非线性激活函数的选择，梯度当其反向传播是渐渐变小</li><li>因此，低层学习得十分缓慢(很难训练)</li></ul></li><li><p>Solution： 许多新的深层前馈/卷积结构，添加更直接的连接(这允许梯度流向更远)</p><p>例子：</p><ul><li>残差连接，“Resnet”</li><li>跳跃连接</li><li>通过默认保留信息的相同连接</li><li>这些深度网络更容易训练</li></ul></li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210213153748.png" alt="image-20210213153746863" style="zoom:20%;" /></p><p>其它方法：</p><ul><li>稠密连接的， “DenseNet”</li><li>直接连接所有将来的层</li><li>高速公路连接， “HighwayNet”</li><li>类似于残差连接，但是等同连接和变换层被一个动态门控制</li><li>受LSTMs的影响， 应用在深层前馈/卷积网络</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210213154310.png" alt="image-20210213154309665" style="zoom:20%;" /></p><p><strong>总结</strong>：梯度消失和梯度爆炸是普遍问题，RNNs尤其不稳定，是因为重复使用相同权重矩阵。</p><h3 id="Bidirectional-and-Multi-layer-RNNs-motivation"><a href="#Bidirectional-and-Multi-layer-RNNs-motivation" class="headerlink" title="Bidirectional and Multi-layer RNNs: motivation"></a>Bidirectional and Multi-layer RNNs: motivation</h3><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210213154606.png" alt="image-20210213154604534" style="zoom:20%;" /></p><p>在情感分类任务中，</p><ul><li>我们可以把隐藏状态看作是这个句子中单词terribly的上下文表示。这个称之为上下文表示</li><li>这些上下文表示只包含左边上下文信息</li><li>那怎么表示右边上下文信息呢？</li><li>这个例子中，exciting是在右边的上下文，它修饰terribly的意思(从否定变为肯定)</li></ul><h4 id="Bidirectional-RNNs"><a href="#Bidirectional-RNNs" class="headerlink" title="Bidirectional RNNs"></a>Bidirectional RNNs</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210213161014.png" alt="image-20210213161012591" style="zoom:20%;" /></p><p><strong>双向RNNs</strong></p><p>上图中，结合后的隐层状态，绿色就可以表示右边的上下文。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210213161323.png" alt="image-20210213161321755" style="zoom:20%;" /></p><p>双向RNNs，计算符号。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210213161457.png" alt="image-20210213161455919" style="zoom:20%;" /></p><p>简化的双向RNN，双向箭头表示双向，和描述的隐层状态被假定为结合的前向和反向状态。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210213161900.png" alt="image-20210213161858762" style="zoom:20%;" /></p><ul><li>注意：双向RNNs只适用于，当你输入整个序列<ul><li>它不适用于语言建模，因为LM你只有左边上下文。</li></ul></li><li>如果你有整个上下文(比如，任何编码)，双向是强大的(你要默认使用它)</li><li>例如，BERT(Bidirectional Encoder Representations from Transformers )是基于双向的强大预训练上下文表示系统。<ul><li>你将在几周后学习更多包含BERT的transformers</li></ul></li></ul><h4 id="Multi-layer-RNNs"><a href="#Multi-layer-RNNs" class="headerlink" title="Multi-layer RNNs"></a>Multi-layer RNNs</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210213162450.png" alt="image-20210213162449032" style="zoom:20%;" /></p><ul><li>RNNs在一个维度上已经很深了(展开到许多时刻)</li><li>我们通过应用多层RNNs来让其在另一个维度deep</li><li>这允许神经网络计算更复杂的表达<ul><li>低层RNNs应该计算低层特征，而高层RNNs应该计算高层</li></ul></li><li>多层RNNs也叫作堆叠RNNs</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210213162517.png" alt="image-20210213162516113" style="zoom:20%;" /></p><p>RNNs的第i层隐层状态就是第i+1层的输入。</p><h4 id="Multi-layer-RNNs-in-practice"><a href="#Multi-layer-RNNs-in-practice" class="headerlink" title="Multi-layer RNNs in practice"></a>Multi-layer RNNs in practice</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210213171054.png" alt="image-20210213171053469" style="zoom:20%;" /></p><ul><li>高表现的RNNs经常是多层的(但没有卷积或前馈网络那么深)</li><li>例如，在2017年一篇论文，Britz et al发现神经机器翻译，2 to 4 层RNN编码器是最好的， 4层解码器是最好的。<ul><li>通常， 跳跃连接/稠密连接是在训练深度RNNs需要的</li></ul></li><li>基于Transformer的神经网络(如，BERT)通常是深层的，像12层或24层<ul><li>其有很多skipping-like连接</li></ul></li></ul><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210213171119.png" alt="image-20210213171117921" style="zoom:20%;" /></p><h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p>[1] <a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf">cs224n-2019-notes05-LM_RNN</a></p><p>[2] <a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture06-fancy-rnn.pdf">cs224n-2021-lecture06-fancy-rnn</a></p><p>[3] <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/">Recurrent Neural Networks Tutorial, Part 3 </a></p><p>[4] <a href="https://ilewseu.github.io/2017/12/30/RNN%E7%AE%80%E5%8D%95%E6%8E%A8%E5%AF%BC/">循环神经网络RNN 梯度推导(BPTT)</a></p><p>[5] <a href="https://bitjoy.net/2019/07/31/cs224n%ef%bc%881-24%ef%bc%89language-models-and-rnns/">CS224N（1.24）Language Models and RNNs</a></p><p>[6] <a href="https://looperxx.github.io/CS224n-2019-07-Vanishing%20Gradients%20and%20Fancy%20RNNs/">Science is interesting.07 Vanishing Gradients and Fancy RNNs</a></p><p>[7] <a href="https://blog.csdn.net/RHJlife/article/details/107279537">2019年CS224N课程笔记-Lecture 7: Vanishing Gradients and Fancy RNNs</a></p><p>[8] <a href="http://klausvon.cn/2019/10/11/cs224n-2019-Lecture-7-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%9C%89%E8%B6%A3%E7%9A%84RNN/">cs224n(2019)-Lecture 7 梯度消失和有趣的RNN</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS224n </tag>
            
            <tag> LSTM </tag>
            
            <tag> BPTT </tag>
            
            <tag> 梯度消失和爆炸 </tag>
            
            <tag> RNNs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N Lecture 5 Recurrent Neural Networks and Language Models</title>
      <link href="2020/12/10/CS224N%20Lecture%205%20Recurrent%20Neural%20Networks%20and%20Language%20Models/"/>
      <url>2020/12/10/CS224N%20Lecture%205%20Recurrent%20Neural%20Networks%20and%20Language%20Models/</url>
      
        <content type="html"><![CDATA[<h3 id="Languages-modeling-and-RNNs"><a href="#Languages-modeling-and-RNNs" class="headerlink" title="Languages modeling and RNNs"></a>Languages modeling and RNNs</h3><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208110540.png" alt="image-20210208105658497"  style="zoom:20%;" /></p><ul><li><p>语言模型是一个预测下一个词是什么的任务，</p><ul><li><code>the students opened their ________</code>这个就是预测横线上填什么单词，(完型填空……)</li></ul></li><li><p>更正式地：给定一个单词序列$\boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \cdots, \boldsymbol{x}^{(t)}$</p><script type="math/tex; mode=display">P(\boldsymbol{x}^{(t+1)} \vert \boldsymbol{x}^{(t)}, \cdots, \boldsymbol{(x)}^{(1)}  )</script></li></ul><p>其中，$\boldsymbol{x}^{(t+1)} $ 是词汇表中的任何一个词。</p><ul><li>这样做的系统就是<strong>语言模型</strong></li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208135218.png" alt="image-20210208134710090" style="zoom:20%;" /></p><ul><li>你也可以认为语言模型是一个将概率分配给一段文本系统</li><li>例如，如果我们有一些文本，那么该文本概率为，如上图。</li></ul><center class="half">   <img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208135233.png" style="zoom:15%;" /><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208135325.png" style="zoom:15%;" /></center><p>上面应用中就是输入就会预测提示下一个词。</p><h4 id="n-gram-Language-Models"><a href="#n-gram-Language-Models" class="headerlink" title="n-gram Language Models"></a>n-gram Language Models</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208135501.png" alt="image-20210208135459185" style="zoom:20%;" /></p><p><strong>n-gram 语言模型</strong></p><ul><li><p>定义：n-gram是一段连续词</p><ul><li>unigrams: “the”, “students”, “opened”, ”their”  只来依赖生成位置的前单个词</li><li>bigrams: “the students”, “students opened”, “opened their”  两个词</li><li>trigrams: “the students opened”, “students opened their”  3个词</li><li>4-grams: “the students opened their”  4个词</li></ul></li><li><p>想法：收集关于不同n-gram出现频率的统计数据，并用这些数据预测下一个单词。</p></li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208140351.png" alt="image-20210208140335978" style="zoom:20%;" /></p><ul><li>首先，我们做一个马尔科夫假设： 后一个词只取决于前n-1个词<ul><li>问题： 我们怎么得到这些n-gram 和 n-1-gram的概率？</li><li>答案：通过其在大量文本语料库的数目。</li></ul></li></ul><h4 id="n-gram-Language-Models-Example"><a href="#n-gram-Language-Models-Example" class="headerlink" title="n-gram Language Models: Example"></a>n-gram Language Models: Example</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208140858.png" alt="image-20210208140856200" style="zoom:20%;" /></p><p>假设我们在学习一个4-gram LM。只要得到：</p><ol><li><code>students opened their w</code>数目</li><li><code>students opened their</code> 数目</li></ol><p>例如，假设在语料库中：</p><ul><li><code>students opened their</code>出现1000次</li><li><code>students opened their books</code>出现400次 ，推出其概率为0.4</li><li><code>students opened their exams</code>出现100次 ，推出其概率为0.1</li></ul><p>那如果文本中出现监考呢？如果出现监考的话， 应该是exam概率比较大的。</p><h4 id="Sparsity-Problems-with-n-gram-Language-Models"><a href="#Sparsity-Problems-with-n-gram-Language-Models" class="headerlink" title="Sparsity Problems with n-gram Language Models"></a>Sparsity Problems with n-gram Language Models</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208141805.png" alt="image-20210208141803236" style="zoom:20%;" /></p><p>n-gram LM的稀疏问题：</p><ul><li><strong>问题</strong> ：如果<code>students open their w</code>从未出现在数据中，那么概率值为 0</li><li><strong>(Partial)解决方案</strong> ：为每个 $w \in V$ 添加极小数 $\delta$。这叫做平滑。这使得词表中的每个单词都至少有很小的概率。</li><li><strong>问题</strong> ：如果<code>students open their</code>从未出现在数据中，那么我们将无法计算任何单词 w 的概率值</li><li><strong>(Partial)解决方案</strong> ：将条件改为<code>open their</code>。这叫做<strong>backoff</strong>。</li></ul><p><strong>注意：</strong></p><p>增大n会放稀疏问题更严重，通常，我们不能让n大于5.</p><h4 id="Storage-Problems-with-n-gram-Language-Models"><a href="#Storage-Problems-with-n-gram-Language-Models" class="headerlink" title="Storage Problems with n-gram Language Models"></a>Storage Problems with n-gram Language Models</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208142828.png" alt="image-20210208142826550" style="zoom:18%;" /></p><p>增大n就是增大模型大小</p><h4 id="n-gram-Language-Models-in-practice"><a href="#n-gram-Language-Models-in-practice" class="headerlink" title="n-gram Language Models in practice"></a>n-gram Language Models in practice</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208143227.png" alt="image-20210208143156261" style="zoom:18%;" /></p><p>可以按照下面网站来试试3-gram LM，<a href="https://nlpforhackers.io/language-models/">language-models</a></p><h4 id="Generating-text-with-a-n-gram-Language-Model"><a href="#Generating-text-with-a-n-gram-Language-Model" class="headerlink" title="Generating text with a n-gram Language Model"></a>Generating text with a n-gram Language Model</h4><center class="half"> <figure>    <img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208143708.png" height="220"><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208143732.png" height="220">  </figure> </center><p>上图中， 依次预测 <code>today the</code> 和 <code>today the price</code>后面的词</p><center class="half">   <img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208143813.png" height="220"><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208143809.png" height="220"> </center><p>上图中， 依次预测 <code>today the price of</code>后面的词, 生成文本</p><h4 id="How-to-build-a-neural-Language-Model"><a href="#How-to-build-a-neural-Language-Model" class="headerlink" title="How to build a neural Language Model?"></a>How to build a neural Language Model?</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208150129.png" alt="image-20210208150127839" style="zoom:18%;"/></p><ul><li>回忆一下语言模型任务：<ul><li>输入：单词序列</li><li>输出：在前面词出现的情况下，下一个词出现的概率</li></ul></li><li>第三讲中的window-based neural model被应用于命名实体识别问题</li></ul><h4 id="A-fixed-window-neural-Language-Model"><a href="#A-fixed-window-neural-Language-Model" class="headerlink" title="A fixed-window neural Language Model"></a>A fixed-window neural Language Model</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208153930.png" alt="image-20210208153928654" style="zoom:20%;"/></p><p>跟命名实体识别问题中一样的网络结构</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208154059.png" alt="image-20210208154057704" style="zoom:20%;" /></p><p>改进 n-gram LM:</p><ul><li>没有稀疏问题</li><li>不需要存储所有观察到的n-grams</li></ul><p>存在问题</p><ul><li>固定窗口太小</li><li>扩大窗口就需要扩大权重矩阵</li><li>窗口再大也不够用</li><li>x1和x2乘以不同权重w。处理输入<strong>不对称</strong>。</li></ul><p>需要一个能处理任何长度输入的神经网络结构</p><blockquote><p>固定窗口的神经语言模型相比于n-gram，优点是：</p><ol><li>不存在稀疏性问题。因为它不要求语料库中出现n-gram的词组，它仅仅是把每个独立的单词的词向量组合起来。只要有词向量，就有输入，至少整个模型能顺利跑通。</li><li>节省存储空间，不需要存储n-gram的组合，只需要存储每个独立的词的词向量。没有组合爆炸的问题。</li></ol><p>依然存在的问题是：</p><ol><li>固定窗口大小还是太小了，受限于窗口大小，不能感知远距离的关系。</li><li>增大窗口大小，权重矩阵W也要相应的增大，导致网络变得复杂。事实上，窗口大小不可能足够大。</li><li>输入$e^{(1)},…,e^{(4)}$对应W中的不同列，即每个e对应的权重完全是独立的，没有某种共享关系，导致训练效率比较低。</li></ol></blockquote><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208155231.png" alt="image-20210208155229700" style="zoom:20%;" /></p><p>核心想法：重复使用一样的权重W。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208155303.png" alt="image-20210208155300934" style="zoom:20%;" /></p><blockquote><p>RNN结构如上图所示。RNN没有所谓窗口的概念，它的输入可以是任意长度，特别适合对顺序敏感的序列问题进行建模。还是以本文的语言模型<code>the students opened their</code>为例，介绍一下RNN的内部结构。对于时刻$t$（位置）的输入词$x(t)$来说，首先把它转换为词向量$e^{(t)}$，作RNN真正的输入；然后对于隐藏层，它的输入来自两部分，一部分是$t$时刻的输入的变换$W_e e^{t}$，另一部分是上一刻的隐状态的变换$W_h h^{(t-1)}$ ，这两部分组合起来再做一个非线性变换，得到当前层的隐状态 $h^{(t)}$；最后，隐状态再接一个<code>softmax</code>层，得到该时刻的输出概率分布。</p><p>需要注意的是，RNN：</p><ul><li>每一个时刻t都可以有输出，上图仅展示了最后时刻$t=4$时的输出</li><li>$h^{(0)}$是初始隐状态，可以是根据之前的学习经验设置的，也可以是随机值，也可以是全0</li><li>整个网络中，所有时刻的$W_e, W_h, U, b_1, b_2$都是同一个参数，即不同时刻的权重是共享的，不同的是不同时刻的输入$x^{(t)}$和隐状态$ h^{(t)}$</li><li>这里的词向量$e$可以是pre-trained得到的，然后固定不动了；也可以根据实际任务进行fine-tune; 在实际任务中现场学习，最好还是用pre-trained的词向量，如果数据量很大的话，再考虑fine-tune 。</li></ul></blockquote><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208161836.png" alt="image-20210208161834408" style="zoom:20%;" /></p><p>RNN的优点：</p><ul><li>能处理任意长度输入</li><li>计算时序t时，能使用许多之前时序的信息(理论上)</li><li>模型大小不随输入文本长度增加</li><li>同样权重应用于每个时序，无论处理多少输入这里是对称的</li></ul><p>RNN的缺点：</p><ul><li>循环计算是非常慢的</li><li>实际上，很难从前面获取信息</li></ul><blockquote><p>RNN相比于固定窗口的神经网络，其优势是：</p><ol><li>不受输入长度限制，可以处理任意长度的序列</li><li>状态t可以感知很久以前的状态</li><li>模型大小是固定的，因为不同时刻的参数$W_e, W_h, U, b_1, b_2$都是共享的，不受输入长度的影响</li><li>所有参数$W_e, W_h, U, b_1, b_2$是共享的，训练起来更高效</li></ol><p>存在的不足：</p><ol><li>训练起来很慢，因为后续状态需要用到前面的状态，是串行的，难以并行计算</li><li>虽然理论上t时刻可以感知很久以前的状态，但实际上很难，因为梯度消失的问题</li></ol></blockquote><h4 id="Training-an-RNN-Language-Model"><a href="#Training-an-RNN-Language-Model" class="headerlink" title="Training an RNN Language Model"></a>Training an RNN Language Model</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208162303.png" alt="image-20210208162300905" style="zoom:18%;" /></p><ul><li>获取一个大的文本语料，单词$x^{(1)}, \cdots, x^{(T)}$序列</li><li><p>输入进RNN-LM；计算每个step t 上的输出概率分布$\hat y^{(t)}$</p><ul><li>如给定词，预测到当前的每个词的分布</li></ul></li><li>损失函数在step t 是，关于 预测概率$\hat y^{(t)}$和下一个词$\hat y ^{(t)}$之间的交叉熵</li><li>平均这个交叉熵，得到整个训练集的损失。</li></ul><blockquote><p>训练RNN依然是梯度下降。首先我们需要定义损失函数，RNN在$t$时刻的输出是预测第$t+1$个词的概率分布$y^(t)$；而对于训练集中给定的文本来说，第$t+1$个词是已知的某个词，所以真实答案$y(t)$其实是一个one-hot向量，在第$x(t+1)$位置为1，其他位置为0。所以如果是交叉熵损失函数的话，表达式如上图中间的等式。RNN整体的损失就是所有时刻的损失均值。</p></blockquote><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208163459.png"  style="zoom:22%;" ></p><p>如上图中依次获得$J^{(1)}(\theta), J^{(2)}(\theta), J^{(3)}(\theta), J^{(4)}(\theta) $</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208164038.png" alt="image-20210208164036434" style="zoom:20%;" /></p><blockquote><ul><li><p>然而：计算 <strong>整个语料库</strong> 的损失和梯度太昂贵了</p></li><li><p>在实践中，我们通常将x 序列 看做一个 <strong>句子</strong> 或是 <strong>文档</strong></p></li><li>回忆 ：随机梯度下降允许我们计算小块数据的损失和梯度，并进行更新。</li><li>计算一个句子的损失(实际上是一批句子)，计算梯度和更新权重。重复上述操作。</li></ul></blockquote><h4 id="Backpropagation-for-RNNs"><a href="#Backpropagation-for-RNNs" class="headerlink" title="Backpropagation for RNNs"></a>Backpropagation for RNNs</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210210202030.png" alt="image-20210210202011894" style="zoom:20%;" /></p><p>RNN结构，左边是未展开形式，右边是根据时间展开得网络图，注意所有时刻$U, W, V$都是一样的，为了便于里解，输出的由$o$改为$y$.</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208164924.png" alt="image-20210208164922233" style="zoom:20%;" /></p><p>Q: 我们怎么计算损失函数对权重w的梯度?</p><p>A: 反向传播遍历时序$t=t, …, 0$，就是之前每个时刻的梯度之和。</p><h4 id="Generating-text-with-a-RNN-Language-Model"><a href="#Generating-text-with-a-RNN-Language-Model" class="headerlink" title="Generating text with a RNN Language Model"></a>Generating text with a RNN Language Model</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208170225.png" alt="image-20210208170222882" style="zoom:20%;" /></p><p>我们也可像n-gram 模型一样通过重复采样生成文本。采样输出变成下一步的输入。</p><center class="half">   <img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208170257.png" height="200"><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208170329.png" height="200"> </center><p>一些RNN-LM有趣的试验，比如生成奥巴马演讲  <a href="https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0">Obama-RNN</a>和 哈利波特小说。</p><center> </center><h4 id="Evaluating-Language-Models"><a href="#Evaluating-Language-Models" class="headerlink" title="Evaluating Language Models"></a>Evaluating Language Models</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208171031.png" alt="image-20210208171029512" style="zoom:20%;" /></p><ul><li>标准的LM估计指标是perplexity</li><li>等于交叉熵损失的指数，低perplexity更好！</li></ul><p><strong>RNNs have greatly improved perplexity</strong>  </p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208171549.png" alt="image-20210208171514290" style="zoom:18%;" /></p><h4 id="Why-should-we-care-about-Language-Modeling"><a href="#Why-should-we-care-about-Language-Modeling" class="headerlink" title="Why should we care about Language Modeling?"></a>Why should we care about Language Modeling?</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208171624.png" alt="image-20210208171622627" style="zoom:18%;" /></p><blockquote><ul><li>语言模型是一项基准测试任务，它帮助我们衡量我们在理解语言方面的进展</li><li>语言建模是许多NLP任务的子组件，尤其是那些涉及生成文本或估计文本概率的任务<ul><li>预测性打字</li><li>语音识别</li><li>手写识别</li><li>拼写/语法纠正</li><li>作者识别</li><li>机器翻译</li><li>摘要</li><li>对话</li><li>等等</li></ul></li></ul></blockquote><h4 id="Recap"><a href="#Recap" class="headerlink" title="Recap"></a>Recap</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208171919.png" alt="image-20210208171917577" style="zoom:18%;" /></p><blockquote><ul><li>语言模型： <strong>预测下一个单词</strong> 的系统</li><li>递归神经网络：一系列神经网络<ul><li>采用任意长度的顺序输入</li><li>在每一步上应用相同的权重</li><li>可以选择在每一步上生成输出</li></ul></li><li>递归神经网络≠≠语言模型</li><li>我们已经证明，RNNs是构建LM的一个很好的方法。</li><li>但RNNs的用处要大得多!</li></ul></blockquote><p><strong>RNN的应用</strong></p><ol><li>RNN 用来标注</li></ol><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208172146.png" alt="image-20210208172144061" style="zoom:18%;" /></p><ol><li>RNN用来做句子分类(情感分类等)<center class="half"><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208172246.png" height="250"><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208172432.png" height="250"> </center><center> <img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208172331.png" style="zoom:20%;"> </center></li></ol><p>基本：用最终的隐层层来编码，使用所有隐层的最大值和均值来做逐元素</p><ol><li>RNN用来做编码器</li></ol><p>编码器和解码器是注意力机制中的一个内容，后续肯定会学习的～，编码器和解码器其实都是RNN实现的（也可能是RNN的变种）</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208173253.png" alt="image-20210208173251041" style="zoom:20%;" /></p><ol><li>RNN做文本生成</li></ol><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210208173344.png" alt="image-20210208173342714" style="zoom:20%;" /></p><h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p>[1] <a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture05-rnnlm.pdf">cs224n-2021-lecture05-rnnlm</a></p><p>[2] <a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf">cs224n-2019-notes05-LM_RNN</a></p><p>[3] <a href="https://bitjoy.net/2019/07/31/cs224n%ef%bc%881-24%ef%bc%89language-models-and-rnns/">RNN 梯度推导</a></p><p>[4] <a href="https://looperxx.github.io/CS224n-2019-06-The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/">CS224n 笔记</a></p><p>[5] <a href="https://blog.csdn.net/RHJlife/article/details/107169895">BPTT</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS224n </tag>
            
            <tag> RNN </tag>
            
            <tag> n-grams </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224n Lecture 4 Dependency Parsing</title>
      <link href="2020/12/08/CS224N%20Lecture%204%20Dependency%20Parsing/"/>
      <url>2020/12/08/CS224N%20Lecture%204%20Dependency%20Parsing/</url>
      
        <content type="html"><![CDATA[<h3 id="1-Two-views-of-linguistic-structure"><a href="#1-Two-views-of-linguistic-structure" class="headerlink" title="1. Two views of linguistic structure"></a>1. Two views of linguistic structure</h3><p>两个语言结构观点： Constituency = 句子结构 = 无上下文语法 context-free grammars  (CFG)</p><h4 id="context-free-grammars"><a href="#context-free-grammars" class="headerlink" title="context-free grammars"></a>context-free grammars</h4><p>句子结构组织单词成嵌套成分</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205140155.png" alt="image-20210205140122821" width="300" /></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205140737.png" alt="image-20210205140735369" width="300" /></p><p>句子及各个组织单词成嵌套成分</p><ul><li>能用CFG规则来表示语法</li><li>起步单元：单词被赋予一个类别 (part of speech = pos)</li><li>单词可以组成不同类别的短语</li><li>短语可以递归第组合成更大的短语</li></ul><p>其中，</p><blockquote><ul><li><code>Det</code> 指的是 <strong>Determiner</strong>，在语言学中的含义为 <strong>限定词</strong></li><li><code>NP</code> 指的是 <strong>Noun Phrase</strong> ，在语言学中的含义为 <strong>名词短语</strong></li><li><code>VP</code> 指的是 <strong>Verb Phrase</strong> ，在语言学中的含义为 <strong>动词短语</strong></li><li><code>P</code> 指的是 <strong>Preposition</strong> ，在语言学中的含义为 <strong>介词</strong><ul><li><code>PP</code> 指的是 <strong>Prepositional Phrase</strong> ，在语言学中的含义为 <strong>介词短语</strong></li></ul></li><li>NP→Det N</li><li>NP→Det (Adj) N</li><li>NP→Det (Adj) N PP<ul><li>PP→P NP</li></ul></li><li>VP→V P<ul><li>中文中，介词短语会出现在动词之前</li></ul></li></ul></blockquote><h4 id="Dependency-Parsing"><a href="#Dependency-Parsing" class="headerlink" title="Dependency Parsing"></a>Dependency Parsing</h4><p>依存分析是一种在计算语言学中占主导地位的观点。</p><ul><li><p>依存结构表示哪些词依赖(修饰或其参数)哪些其他词。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205142022.png" alt="1560692691439" width="350" /></p></li><li><p>look是整个句子的root， 而look又依赖于crate。</p></li><li>in， the， large都是crate依赖</li><li>in the kitchen都是crate的修饰</li><li>in， the都是kitchen的依赖，同理分析by the door</li></ul><h4 id="为什么我们要句子结构"><a href="#为什么我们要句子结构" class="headerlink" title="为什么我们要句子结构?"></a>为什么我们要句子结构?</h4><ul><li>我们需要理解句子结构，为了能正确解释语言。</li><li>人类交流复杂想法通过将单词组合在一起合成更大的单元来传达复杂意思。</li><li>我们需要知道什么联系什么</li></ul><h5 id="Prepositional-phrase-attachment-ambiguity"><a href="#Prepositional-phrase-attachment-ambiguity" class="headerlink" title="Prepositional phrase attachment ambiguity"></a>Prepositional phrase attachment ambiguity</h5><p>介词短语依存歧义</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205143110.png" alt="image-20210205143108203" width="350" /></p><blockquote><ul><li>警察用刀杀了那个男子<ul><li><code>cops</code> 是 <code>kill</code> 的 <code>subject</code> (subject 指 主语)</li><li><code>man</code> 是 <code>kill</code>的 <code>object</code> (object 指 宾语)</li><li><code>knife</code> 是 <code>kill</code> 的 <code>modifier</code> (modifier 指 修饰符)</li></ul></li><li>警察杀了那个有刀的男子<ul><li><code>knife</code> 是 <code>man</code> 的 <code>modifier</code> (名词修饰符，简称为 <code>nmod</code> )</li></ul></li></ul></blockquote><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205143635.png" alt="1560696178556" width="350" /></p><blockquote><p><code>from space</code> 这一介词短语修饰的是前面的动词 <code>count</code> 还是名词 <code>whales</code> ？</p></blockquote><h5 id="PP-attachment-ambiguities-multiply"><a href="#PP-attachment-ambiguities-multiply" class="headerlink" title="PP attachment ambiguities multiply"></a>PP attachment ambiguities multiply</h5><ul><li>关键分析决策是我们怎样”依存“不同成分<ul><li>名词短语，形容词或分词、不定式和协调等。</li></ul></li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205145329.png" alt="1560698493997" width="350" /></p><blockquote><ul><li>上述句子中有四个介词短语</li><li><code>board</code> 是 <code>approved</code> 的 主语，<code>acquisition</code> 是 <code>approved</code> 的谓语</li><li><code>by Royal Trustco Ltd.</code> 是修饰 <code>acquisition</code> 的，即董事会批准了这家公司的收购</li><li><code>of Toronto</code> 可以修饰 <code>approved, acquisition, Royal Trustco Ltd.</code> 之一，经过分析可以得知是修饰 <code>Royal Trustco Ltd.</code> 即表示这家公司的位置</li><li><code>for $27 a share</code> 修饰 <code>acquisition</code></li><li><code>at its monthly meeting</code> 修饰 <code>approved</code> ，即表示批准的时间地点</li></ul><p>面对这样复杂的句子结构，我们需要考虑 <strong>指数级</strong> 的可能结构，这个序列被称为 <strong>Catalan numbers</strong></p><p><strong>Catalan numbers</strong> : $Cn=(2n)!/[(n+1)!n!]$</p><ul><li>一个指数增长的序列，出现在许多类似树的环境中<ul><li>例如，一个 n+2 边的多边形可能的三角剖分的数量<ul><li>出现在概率图形模型的三角剖分中(CS228)</li></ul></li></ul></li></ul></blockquote><h5 id="Coordination-scope-ambiguity"><a href="#Coordination-scope-ambiguity" class="headerlink" title="Coordination scope ambiguity"></a>Coordination scope ambiguity</h5><p>协调范围歧义</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205145740.png" alt="1560699777096" width="350" /></p><ul><li>一个人：[[Shuttle veteran and longtime NASA executive] Fred Gregory] appointed to board（航天飞机老兵、长期担任美国宇航局主管弗雷德-格雷戈里被任命为董事会成员）</li><li>两个人：[Shuttle veteran] and [longtime NASA executive Fred Gregory] appointed to board（航天飞机老兵和长期担任美国宇航局主管弗雷德-格雷戈里被任命为董事会成员）</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205151917.png" alt="1560699972471" width="350" /></p><p>Doctor: No heart, cognitive issues.</p><h5 id="Adjectival-Modifier-Ambiguity"><a href="#Adjectival-Modifier-Ambiguity" class="headerlink" title="Adjectival Modifier Ambiguity"></a>Adjectival Modifier Ambiguity</h5><p>形容词修饰歧义</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205152236.png" alt="1560700236717" width="350" /></p><blockquote><p>例句：Students get first hand job experience</p><p>  first hand</p><p>   表示 第一手的，直接的，即学生获得了直接的工作经验</p><p>first<code>是</code>hand的形容词修饰语</p><ul><li>first 修饰 experience, hand 修饰 job !</li></ul></blockquote><h5 id="Verb-Phrase-VP-attachment-ambiguity"><a href="#Verb-Phrase-VP-attachment-ambiguity" class="headerlink" title="Verb Phrase (VP) attachment ambiguity"></a>Verb Phrase (VP) attachment ambiguity</h5><p>动词依存歧义</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205234617.png" alt="image-20210205234615845" width="350" /></p><blockquote><p>例句：Mutilated body washes up on Rio beach to be used for Olympic beach volleyball.</p><ul><li><code>to be used for Olympic beach volleyball</code> 是 动词短语 (VP)</li><li>修饰的是 <code>body</code> 还是 <code>beach</code></li></ul></blockquote><h5 id="Dependency-paths-identify-semantic-relations-–-e-g-for-protein-interaction"><a href="#Dependency-paths-identify-semantic-relations-–-e-g-for-protein-interaction" class="headerlink" title="Dependency paths identify semantic relations – e.g., for protein interaction"></a>Dependency paths identify semantic relations – e.g., for protein interaction</h5><p>依赖路径识别语义关系</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205152628.png" alt="image-20210205152626132" width="350" /></p><h3 id="2-Dependency-Grammar-and-Dependency-Structure"><a href="#2-Dependency-Grammar-and-Dependency-Structure" class="headerlink" title="2. Dependency Grammar and Dependency Structure"></a>2. Dependency Grammar and Dependency Structure</h3><p>依存语法假定句法结构由词汇项之间的关系构成，通常是二元不对称关系(“箭头”)， 称为依赖关系。</p><p>Dependency Structure有两种表现形式：</p><ul><li>一种是直接在句子上标出依存关系箭头及语法关系</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205153800.png" alt="image-20210205153758811" width="700" /></p><ul><li>另一种是将其做成树状机构（Dependency Tree Graph）</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205153908.png" alt="image-20210205153905754" width="500" /></p><blockquote><ul><li>箭头通常标记(<strong>type</strong>)为语法关系的名称(主题、介词对象、apposition等)。</li><li>依赖关系标签的系统，例如 <strong>universal dependency</strong> 通用依赖</li><li>箭头连接头部( head )和一个依赖(修饰词,下级,下属)<ul><li>A →依赖于 A 的事情</li></ul></li><li>通常,依赖关系形成一棵树(单头 无环 连接图)</li></ul></blockquote><h4 id="Dependency-Grammar-Parsing-History"><a href="#Dependency-Grammar-Parsing-History" class="headerlink" title="Dependency Grammar/Parsing History"></a>Dependency Grammar/Parsing History</h4><blockquote><ul><li>依赖结构的概念可以追溯到很久以前<ul><li>Pāṇini的语法(公元前5世纪)</li><li>一千年 阿拉伯语的语法的基本方法</li></ul></li><li>选区/上下文无关文法是一个新奇的发明<ul><li>20世纪发明(R.S.Wells,1947; then Chomsky)</li></ul></li><li>现代依赖工作经常源于 L. Tesnière(1959)<ul><li>是20世纪“东方”的主导方法(俄罗斯，中国，…)<ul><li>有利于更自由的语序语言</li></ul></li></ul></li><li>NLP中最早类型的解析器在美国<ul><li>David Hays 是美国计算语言学的创始人之一，他很早就(第一个)构建了依赖解析器(Hays 1962)。</li></ul></li></ul></blockquote><h4 id="Dependency-Grammar-and-Dependency-Structure-表示方法"><a href="#Dependency-Grammar-and-Dependency-Structure-表示方法" class="headerlink" title="Dependency Grammar and Dependency Structure  表示方法"></a>Dependency Grammar and Dependency Structure  表示方法</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205154340.png" alt="image-20210205154337913" width="600" /></p><blockquote><ul><li>人们对箭头指向的方式不一致：有些人把箭头朝一个方向画；有人是反过来的<ul><li>Tesnière 从头开始指向依赖，本课使用此种方式</li></ul></li><li>通常添加一个伪造“ROOT”指向整个句子的头部，这样每个单词都精确地依赖于另一个节点</li></ul></blockquote><p><strong>注：从头指向依赖，加上个ROOT</strong></p><h4 id="The-rise-of-annotated-data-Universal-Dependencies-treebanks"><a href="#The-rise-of-annotated-data-Universal-Dependencies-treebanks" class="headerlink" title="The rise of annotated data: Universal Dependencies treebanks"></a>The rise of annotated data: Universal Dependencies treebanks</h4><p>标注数据的崛起：全局依存树库</p><p><a href="https://universaldependencies.org/">Universal Dependencies  </a></p><blockquote><p><strong>Universal Dependencies</strong>：我们想要拥有一个统一的、并行的依赖描述，可用于任何人类语言</p><ul><li>从前手工编写语法然后训练得到可以解析句子的解析器</li><li>用一条规则捕捉很多东西真的很有效率，但是事实证明这在实践中不是一个好主意<ul><li>语法规则符号越来越复杂，并且没有共享和重用人类所做的工作</li></ul></li><li>句子结构上的treebanks 支持结构更有效</li></ul></blockquote><h5 id="The-rise-of-annotated-data"><a href="#The-rise-of-annotated-data" class="headerlink" title="The rise of annotated data"></a>The rise of annotated data</h5><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205155456.png" alt="image-20210205155453514" width="600" /></p><blockquote><p>从一开始，构建 treebank 似乎比构建语法慢得多，也没有那么有用</p><p>但是 treebank 给我们提供了许多东西</p><ul><li>劳动力的可重用性<ul><li>许多解析器、词性标记器等可以构建在它之上</li><li>语言学的宝贵资源</li></ul></li><li>广泛的覆盖面，而不仅仅是一些直觉</li><li>频率和分布信息</li><li>一种评估系统的方法</li></ul></blockquote><h5 id="Dependency-Conditioning-Preferences"><a href="#Dependency-Conditioning-Preferences" class="headerlink" title="Dependency Conditioning Preferences"></a>Dependency Conditioning Preferences</h5><p>依存条件偏好</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205160836.png" alt="image-20210205160831903" width="600" /></p><p>关于依存分析的信息来源是什么？</p><blockquote><ol><li>Bilexical affinities (两个单词间的密切关系)<ul><li>[discussion →→ issues] 是看上去有道理的</li></ul></li><li>Dependency distance 依赖距离<ul><li>主要是与相邻词</li></ul></li><li>Intervening material 介于中间的物质<ul><li>依赖很少跨越介于中间的动词或标点符号</li></ul></li><li>Valency of heads 头部的配价<ul><li>对于头部，多少依赖在哪边是普遍的？</li></ul></li></ol></blockquote><h5 id="Dependency-Parsing-1"><a href="#Dependency-Parsing-1" class="headerlink" title="Dependency Parsing"></a>Dependency Parsing</h5><p>依存分析</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205161742.png" alt="image-20210205161740991" width="600" /></p><blockquote><ul><li>通过为每个单词选择它所依赖的其他单词(包括根)来解析一个句子</li><li>通常有一些限制<ul><li>只有一个单词是依赖于根的</li><li>不存在循环 A→B, A→B,</li></ul></li><li>这使得依赖项成为树</li><li>最后一个问题是箭头是否可以交叉(非投影的 non-projective)<ul><li>没有交叉的就是non-projectice</li></ul></li></ul></blockquote><h4 id="Projectivity"><a href="#Projectivity" class="headerlink" title="Projectivity"></a>Projectivity</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205162057.png" alt="image-20210205162056100" width="600" /></p><blockquote><ul><li>定义：当单词按线性顺序排列时，没有交叉的依赖弧，所有的弧都在单词的上方</li><li>与CFG树并行的依赖关系必须是投影的<ul><li>通过将每个类别的一个子类别作为头来形成依赖关系</li></ul></li><li>但是依赖理论通常允许非投射结构来解释移位的成分<ul><li>如果没有这些非投射依赖关系，就不可能很容易获得某些结构的语义</li></ul></li></ul></blockquote><h4 id="Methods-of-Dependency-Parsing"><a href="#Methods-of-Dependency-Parsing" class="headerlink" title="Methods of Dependency Parsing"></a>Methods of Dependency Parsing</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205162900.png" alt="image-20210205162858011" width="600" /></p><blockquote><ol><li>Dynamic programming<ul><li>Eisner(1996)提出了一种复杂度为 $O(n3)$的精巧算法，它生成头部位于末尾而不是中间的解析项</li></ul></li><li>Graph algorithms<ul><li>为一个句子创建一个最小生成树</li><li>McDonald et al.’s (2005) MSTParser 使用ML分类器独立地对依赖项进行评分(他使用MIRA进行在线学习，但它也可以是其他东西)</li></ul></li><li>Constraint Satisfaction<ul><li>去掉不满足硬约束的边 Karlsson(1990), etc.</li></ul></li><li>“Transition-based parsing” or “deterministic dependency parsing“<ul><li>良好的机器学习分类器 MaltParser(Nivreet al. 2008) 指导下的依存贪婪选择。已证明非常有效。</li></ul></li></ol></blockquote><h3 id="3-Greedy-transition-based-parsing-Nivre-2003"><a href="#3-Greedy-transition-based-parsing-Nivre-2003" class="headerlink" title="3. Greedy transition-based parsing [Nivre 2003]"></a>3. Greedy transition-based parsing [Nivre 2003]</h3><p>基于贪婪转换解析</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205163516.png" alt="image-20210205163514787" width="600" /></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205172509.png" alt="image-20210205172505677" width="600" /></p><p>这部分要求要有数据结构和算法基础。</p><p>Transition-based Dependency Parsing 可以看做是state machine，对于一个序列$S = w_0w_1\cdots w_n$, $w_0$是fake [ROOT], state由3部分组成$(\sigma, \beta, A)$</p><p>其中，</p><ul><li>$\sigma$是S中若干个$w_i$组成的stack</li><li>$\beta$是S中若干个$w_i$组成的buffer，将其看作一个队列，队列模拟的是人眼的阅读顺序，人眼从左到右阅读，系统也从左往右读入单词，未读入的单词就构成一个队列，队列的出口在左边。初始状态下队列就是整个句子，且顺序不变。</li><li>A是依存的arc构成的集合，如$A = {(w_i, r_1, w_j), \cdots, (w_p, r_k, w_q)}$,其中r是两个词之间的依存关系</li></ul><p>初始化状态的时候，</p><ul><li>$\sigma$只包含ROOT $w_o$, $\beta$包含所有单词$w_1, \cdots, w_n$, A是空集 $\empty$ .</li></ul><p>最终状态，</p><ul><li>$\sigma$包含ROOT和所有词，$\beta$清空，A包含所有依存arc，A就是我们需要的依存关系。</li></ul><p>状态转移有三种方式：</p><ol><li>SHFIT: 从buffer移除第一个词，并将其送入栈的顶部(前提：buffer为空)</li><li>LEFT-ARC: 将 $(w_j,r,w_i)$ 加入边的集合 A ，其中 $w_i$是stack上的次顶层的词， $w_j$是stack上的最顶层的词；然后再将$w_i$从栈中移除。（前提条件：栈至少包含两个词，且$w_i$不能是ROOT）</li><li>RIGHT_ARC：将$(w_i,r,w_j) $ 加入边的集合 A ，其中 $w_i$是stack上的次顶层的词， $w_j$是stack上的最顶层的词； 然后再将$w_j$从栈中移除。(前提条件：栈至少包含两个词)</li></ol><p>数学表示，（这样就可以理解整个过程就是上图意思）</p><script type="math/tex; mode=display">\begin{aligned}&\text{SHIFT} \qquad \sigma, w_i \vert \beta, A \quad \to \quad \sigma \vert  w_i,\beta, A \quad \text{意思:将buffer中第一个词}w_i\text{移除到栈的顶部}\\ \\&\text{LEFT-ARC}_r \qquad \sigma \vert w_i \vert w_j, \beta, A  \quad \to \quad \sigma \vert  w_j,\beta, A \cup \{r(w_j, w_i)\} \quad \\&\text{    意思:将}r(w_j, w_i)\text{加入A, 再将栈中次顶层的词}w_i\text{移除}\\\\&\text{RIGHT-ARC}_r \qquad \sigma \vert w_i \vert w_j, \beta, A  \quad \to \quad \sigma \vert  w_i,\beta, A \cup \{r(w_i, w_j)\} \quad \\&\text{    意思:将}r(w_i, w_j)\text{加入A, 再将栈中次顶层的词}w_j\text{移除}\end{aligned}</script><p>还可以看看这里面的详细例子 <a href="https://blog.csdn.net/lairongxuan/article/details/104806128">举例说明</a></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205182327.png" alt="image-20210205182325696" width="600" /></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205182354.png" alt="image-20210205182352998" width="600" /></p><h4 id="MaltParser-Nivreand-Hall-2005"><a href="#MaltParser-Nivreand-Hall-2005" class="headerlink" title="MaltParser [Nivreand Hall 2005]"></a><strong>MaltParser</strong> [Nivreand Hall 2005]</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205182748.png" alt="image-20210205182746397" width="600"  /></p><blockquote><ul><li>我们需要解释如何选择下一步行动<ul><li>Answer：标准回答，机器学习</li></ul></li><li>每个动作都由一个有判别分类器(例如softmax classifier)对每个合法的动作进行预测<ul><li>最多三种无类型的选择，当带有类型时，最多 |R|×2+1种</li><li>Features：栈顶单词，POS；buffer中的第一个单词，POS；等等</li></ul></li><li>（在最简单的形式中是）没有搜索的<ul><li>但是，如果你愿意，你可以有效地执行一个 Beam search 束搜索(虽然速度较慢，但效果更好)：你可以在每个时间步骤中保留 k个好的解析前缀</li></ul></li><li>模型精度是略低于最新的依存解析，但是</li><li>它提供了非常快的线性时间解析，性能非常好</li></ul></blockquote><h5 id="Conventional-Feature-Representation"><a href="#Conventional-Feature-Representation" class="headerlink" title="Conventional Feature Representation"></a>Conventional Feature Representation</h5><p>传统特征表示</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205183403.png" alt="image-20210205183359611" width="600"  /></p><p>栈和队列中单词、词性、依存标签的组合的特征函数，一个超长的稀疏01向量。</p><h5 id="Evaluation-of-Dependency-Parsing-labeled-dependency-accuracy"><a href="#Evaluation-of-Dependency-Parsing-labeled-dependency-accuracy" class="headerlink" title="Evaluation of Dependency Parsing: (labeled) dependency accuracy"></a>Evaluation of Dependency Parsing: (labeled) dependency accuracy</h5><p>依存解析的估计：标签的依存准确性</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205201004.png" alt="image-20210205201001703" width="600" /></p><p>其中，<strong>UAS</strong> (unlabeled attachment score) 指 <strong>无标签依存正确率</strong> ，<strong>LAS</strong> (labeled attachment score) 指 <strong>有标签依存正确率</strong></p><p>一个是LAS（labeled attachment score）即只有arc的箭头方向以及语法关系均正确时才算正确，以及UAS（unlabeled attachment score）即只要arc的箭头方向正确即可。</p><p>例子中，</p><ul><li>只看箭头方向，即Gold和Parsed中数字相同有4个，4/5，即80%</li><li>看箭头和解析后的标签只有2个，即2/5=40%</li></ul><h4 id="Handling-non-projectivity"><a href="#Handling-non-projectivity" class="headerlink" title="Handling non-projectivity"></a>Handling non-projectivity</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205202331.png" alt="image-20210205202329854" width="600"  /></p><blockquote><ul><li>我们提出的弧标准算法只构建投影依赖树</li><li>可能指向头的方向<ul><li>在非投影弧上宣告失败</li><li>只具有投影表示时使用依赖形式<ul><li>CFG只允许投影结构；改进扰乱头</li></ul></li><li>使用投影依赖项解析算法的后处理器来识别和解析非投影链接</li><li>添加额外的转换，至少可以对大多数非投影结构建模（添加一个额外的交换转换，冒泡排序）</li><li>转移到不使用或不需要对投射性进行任何约束的解析机制(例如，基于图的MSTParser)</li></ul></li></ul></blockquote><p>​                </p><h3 id="4-Why-train-a-neural-dependency-parser-Indicator-Features-Revisited"><a href="#4-Why-train-a-neural-dependency-parser-Indicator-Features-Revisited" class="headerlink" title="4. Why train a neural dependency parser? Indicator Features Revisited"></a>4. Why train a neural dependency parser? Indicator Features Revisited</h3><p>为什么训练神经依存解析？重提指示符特征</p><p>传统特征表示稀疏、不完全、计算代价大（SVM之类的线性分类器本身是很快的，而传统parser的95%时间都花在拼装查询特征上了）。</p><h4 id="A-neural-dependency-parser-Chen-and-Manning-2014"><a href="#A-neural-dependency-parser-Chen-and-Manning-2014" class="headerlink" title="A neural dependency parser [Chen and Manning 2014]"></a><strong>A neural dependency parser</strong> [Chen and Manning 2014]</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205203539.png" alt="image-20210205203537218" width="500"  /></p><ul><li>斯坦福依存英语解析<ul><li>未便签依存分数UAS = head</li><li>标签依存分数LAS = head and label</li></ul></li></ul><p>相较于传统的graph-based方法，花了这么多功夫得到的只是0.1%的LAS提升。</p><h4 id="Distributed-Representations"><a href="#Distributed-Representations" class="headerlink" title="Distributed Representations"></a>Distributed Representations</h4><p>分布表示</p><blockquote><ul><li>我们将每个单词表示为一个d维稠密向量（如词向量）<ul><li>相似的单词应该有相近的向量</li></ul></li><li>同时，part-of-speech tags词性标签(POS)和 dependency labels 依存标签也表示为d维向量<ul><li>较小的离散集也表现出许多语义上的相似性。</li></ul></li><li><strong>NNS</strong>(复数名词)应该接近<strong>NN</strong>(单数名词)</li><li><strong>num</strong>(数值修饰语)应该接近<strong>amod</strong>(形容词修饰语)。</li></ul><p>对于Neural Dependency Parser，其输入特征通常包含三种</p><ul><li>stack和buffer中的单词及其dependent word</li><li>单词的part-of-speech tag</li><li>描述语法关系的arc label</li></ul></blockquote><h4 id="Extracting-Tokens-and-then-vector-representations-from-configuration"><a href="#Extracting-Tokens-and-then-vector-representations-from-configuration" class="headerlink" title="Extracting Tokens and then vector representations from configuration"></a>Extracting Tokens and then vector representations from configuration</h4><p>提取令牌和从配置中获得向量表示</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205214125.png" alt="image-20210205214122911" width="500"  /></p><p>将上图中的单词、词性标注和依存标签转换为词向量，并将它们联结起来。</p><h4 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205214241.png" alt="image-20210205214239995" width="500"  /></p><p>具体来说，将嵌入矩阵 $E_w, E_t, E_l$中提取它们对应的稠密的特征的表示，然后将这些向量 <strong>拼接起来</strong> 作为输入 $[ x_w , x_t , x_l ] $。</p><p>网络结构由一个输入层、一个隐含层Relu、一个softmax输出层组成，使用交叉熵损失函数。</p><h4 id="Dependency-parsing-for-sentence-structure"><a href="#Dependency-parsing-for-sentence-structure" class="headerlink" title="Dependency parsing for sentence structure"></a>Dependency parsing for sentence structure</h4><p>神经网络能准确决定句子结构，增加解释性。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205215419.png" alt="image-20210205215417103" width="500"  /></p><blockquote><p>Chen and Manning (2014) was the first simple, successful neural dependency parser  </p></blockquote><p>然后稠密表示，让其在准确性和速度上优于其它贪婪解析器。</p><h4 id="Further-developments-in-transition-based-neural-dependency-parsing"><a href="#Further-developments-in-transition-based-neural-dependency-parsing" class="headerlink" title="Further developments in transition-based neural dependency parsing"></a>Further developments in transition-based neural dependency parsing</h4><blockquote><p>这项工作由其他人进一步开发和改进，特别是在谷歌</p><ul><li>更大、更深的网络中，具有更好调优的超参数</li><li>Beam Search 更多的探索动作序列的可能性，而不是只考虑当前的最优</li><li><p>全局、条件随机场(CRF)的推理出决策序列</p><p><a href="https://ai.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html">这就引出了SyntaxNet和Parsey McParseFace模型</a></p></li></ul></blockquote><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205223105.png" alt="image-20210205223047925" width="500"  /></p><h4 id="Graph-based-dependency-parsers"><a href="#Graph-based-dependency-parsers" class="headerlink" title="Graph-based dependency parsers"></a>Graph-based dependency parsers</h4><blockquote><p>为每条边的每一个可能的依赖关系计算一个分数</p><ul><li>然后将每个单词的边缘添加到其得分最高的候选头部</li><li>并对其它每个单词重复相同的操作</li></ul></blockquote><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205223325.png" alt="image-20210205223324167" width="500" /></p><h4 id="A-Neural-graph-based-dependency-parser"><a href="#A-Neural-graph-based-dependency-parser" class="headerlink" title="A Neural graph-based dependency parser"></a>A Neural graph-based dependency parser</h4><p>[ Dozat and Manning 2017; Dozat, Qi, and Manning 2017 ]</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205223504.png" alt="image-20210205223503026" width="500"  /></p><blockquote><ul><li>在神经模型中为基于图的依赖分析注入活力<ul><li>为神经依赖分析设计一个双仿射评分模型<ul><li>也使用神经序列模型，我们将在下周讨论</li></ul></li></ul></li><li>非常棒的结果<ul><li>但是比简单的基于神经传递的解析器要慢<ul><li>在一个长度为 n 的句子中可能有 $n^2$ 个依赖项</li></ul></li></ul></li></ul></blockquote><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://looperxx.github.io/CS224n-2019-05-Linguistic%20Structure%20Dependency%20Parsing/">05 Linguistic Structure Dependency Parsing</a></p><p>[2] <a href="https://blog.csdn.net/RHJlife/article/details/107132234">2019年CS224N课程笔记-Lecture 5: Linguistic Structure: Dependency Parsing</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/66268929">CS224N笔记(五):Dependency Parsing</a></p><p>[4] <a href="https://blog.csdn.net/lairongxuan/article/details/104806128">CS224n 2019 Winter 笔记（三）：句子依存分析（Dependency Parsing）</a></p><p>[5] <a href="https://www.hankcs.com/nlp/parsing/neural-network-based-dependency-parser.html/2#h2-6">基于神经网络的高性能依存句法分析器</a></p><p>[6] <a href="http://fancyerii.github.io/books/depparser/">依存句法分析 部分代码及分析</a></p><p>[7] <a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2021-lecture04-dep-parsing.pdf">cs224n-2021-lecture04-dep-parsing</a></p><p>[8] <a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes04-dependencyparsing.pdf">cs224n-2019-notes04-dependencyparsing</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CS224n </tag>
            
            <tag> Dependency Parsing </tag>
            
            <tag> CFG </tag>
            
            <tag> Dependency Grammar </tag>
            
            <tag> Dependency Structure </tag>
            
            <tag> Greedy transition-based parsing </tag>
            
            <tag> neural dependency parser </tag>
            
            <tag> Graph-based dependency parser </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N Lecture 3 Backprop and Neural Networks</title>
      <link href="2020/12/06/CS224N%20Lecture%203%20Backprop%20and%20Neural%20Networks/"/>
      <url>2020/12/06/CS224N%20Lecture%203%20Backprop%20and%20Neural%20Networks/</url>
      
        <content type="html"><![CDATA[<h3 id="1-梯度"><a href="#1-梯度" class="headerlink" title="1. 梯度"></a>1. 梯度</h3><p>课程计划</p><p>Lecture 4： 手推梯度和算法</p><ol><li>介绍</li><li>矩阵计算</li><li>反向传播</li></ol><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205231518.png" alt="image-20210205231516555" style="zoom:20%;" /></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205231834.png" alt="image-20210205231832287" style="zoom:20%;" /></p><ul><li>如果给定的函数有一个输出和n个输入</li><li>梯度是相对于每个输入的偏导向量</li></ul><h4 id="Jacobian-Matrix-Generalization-of-the-Gradient"><a href="#Jacobian-Matrix-Generalization-of-the-Gradient" class="headerlink" title="Jacobian Matrix: Generalization of the Gradient"></a>Jacobian Matrix: Generalization of the Gradient</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210205235329.png" alt="image-20210205233107266" style="zoom:20%;" /></p><ul><li>给定一个m个输出和n个输入的函数</li><li>其Jacobian是偏导的$m \times n $矩阵</li></ul><h4 id="Chain-rule"><a href="#Chain-rule" class="headerlink" title="Chain rule"></a>Chain rule</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206194530.png" alt="image-20210206194520526" width="300;" /></p><h4 id="Example-Jacobian-Elementwise-activation-Function"><a href="#Example-Jacobian-Elementwise-activation-Function" class="headerlink" title="Example Jacobian: Elementwise activation Function"></a>Example Jacobian: Elementwise activation Function</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206194653.png" alt="image-20210206194651388" width="300;" /></p><p>因为n个输入n个输出，其Jacobian矩阵应该是$n \times n$</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206194822.png" alt="image-20210206194820897" width="300;" /></p><p>下面是一些公式，利用定义逐个元素求导很容易证明。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206200754.png" alt="image-20210206200713639" style="zoom:20%;"/></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206200752.png" alt="image-20210206200750535" style="zoom:20%;" /></p><h3 id="2-Back-to-our-Neural-Net"><a href="#2-Back-to-our-Neural-Net" class="headerlink" title="2. Back to our Neural Net!"></a>2. Back to our Neural Net!</h3><p>模型如下，一个隐层的简单架构。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206201108.png" alt="image-20210206200922955" style="zoom:20%;" /></p><p>让我们求分数s对b的梯度</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206201034.png" alt="image-20210206201032566" style="zoom:20%;"/></p><h4 id="Break-up-equations-into-simple-pieces"><a href="#Break-up-equations-into-simple-pieces" class="headerlink" title="Break up equations into simple pieces"></a>Break up equations into simple pieces</h4><p>注意，变量和保持其维度!</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206201343.png" alt="image-20210206201342047" style="zoom:20%;" /></p><h4 id="Apply-the-chain-rule"><a href="#Apply-the-chain-rule" class="headerlink" title="Apply the chain rule"></a>Apply the chain rule</h4><p>左边是关系式，右边是链式法则。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206201527.png" alt="image-20210206201519446" style="zoom:20%;"/></p><p>利用方框里的公式，依次对其求导。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206201729.png" alt="image-20210206201726933" style="zoom:20%;"/></p><h4 id="Write-out-the-Jacobians"><a href="#Write-out-the-Jacobians" class="headerlink" title="Write out the Jacobians"></a>Write out the Jacobians</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206201914.png" alt="image-20210206201912613" style="zoom:20%;"/></p><h4 id="Re-using-Computation"><a href="#Re-using-Computation" class="headerlink" title="Re-using Computation"></a>Re-using Computation</h4><p>如果我们要求s对权重w的梯度呢？</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206202131.png" alt="image-20210206202128596" style="zoom:20%;" /></p><p>$\delta$是其局部误差信号，跟上面对b的梯度一样, 推出$\mathbf{u}^T \circ f^{\prime}(z)$</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206202302.png" alt="image-20210206202259920" style="zoom:20%;"/></p><h4 id="关于矩阵的导数：输出形状"><a href="#关于矩阵的导数：输出形状" class="headerlink" title="关于矩阵的导数：输出形状"></a>关于矩阵的导数：输出形状</h4><p>Derivative with respect to Matrix: Output shape , 分数s对权重的梯度形状是什么样的？</p><ul><li>1输出， nm的输入： $1 \times nm$的Jacobian。不方便接下来梯度更新</li><li>让其变为$n \times m$</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206202843.png" alt="image-20210206202841356" style="zoom:20%;" /></p><h4 id="矩阵的导数"><a href="#矩阵的导数" class="headerlink" title="矩阵的导数"></a>矩阵的导数</h4><p>分数s的梯度就等于局部误差  <code>x</code> 输入</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206203252.png" alt="image-20210206203250225" style="zoom:20%;"/></p><h4 id="Deriving-local-input-gradient-in-backprop"><a href="#Deriving-local-input-gradient-in-backprop" class="headerlink" title="Deriving local input gradient in backprop"></a>Deriving local input gradient in backprop</h4><p>在反向传播中，推导局部输入的梯度。</p><ul><li>不妨只考虑单一权重$W_{ij}$</li><li>$W_{ij}$只影响$z_i$</li><li>例如，$W_{23}$只用于祭祀$z_2$而不是$z_1$</li></ul><p>隐层z对权重的梯度，就是每个输入。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206203528.png" alt="image-20210206203526059" style="zoom:20%;"/></p><h4 id="为什么要转置"><a href="#为什么要转置" class="headerlink" title="为什么要转置?"></a>为什么要转置?</h4><p>简单说是为了计算时维度能用，本质是因为在求外积。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206204428.png" alt="image-20210206204421113" style="zoom:20%;"/></p><h4 id="导数的形状应该是什么样的？"><a href="#导数的形状应该是什么样的？" class="headerlink" title="导数的形状应该是什么样的？"></a>导数的形状应该是什么样的？</h4><ul><li>相似地，分数对偏置b的梯度是一个行向量<ul><li>为了形状上的方便，称我们的梯度应该是一个列向量，因为b是一个列向量。</li></ul></li><li>分歧在Jacobian形式和形状方便<ul><li>要求作业遵循形状便利，Jacobian方便计算</li></ul></li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206204627.png" alt="image-20210206204625437" style="zoom:20%;"/></p><p>两个建议：</p><ol><li>尽可能用Jacobian，最后reshape一下遵循形状方便<ul><li>在最后，分数s对偏置的偏导是一个列向量，导致$\delta$要转置</li></ul></li><li>总是遵循形状方便<ul><li>检查维度，来弄清楚是否要转置或重组各个项</li><li>错误信号$\delta$,到隐藏层要跟隐藏层维度一样</li></ul></li></ol><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206205209.png" alt="image-20210206205207422" style="zoom:20%;" /></p><h3 id="3-反向传播"><a href="#3-反向传播" class="headerlink" title="3. 反向传播"></a>3. 反向传播</h3><p>我们几乎给你展示了反向传播。本质就是链式法则的使用。</p><p>技巧：</p><p>我们在计算低层导数时利用对高层的的求导来减少计算量。<br><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206210238.png" alt="image-20210206210236414" style="zoom:20%;" /></p><h4 id="Computation-Graphs-and-Backpropagation"><a href="#Computation-Graphs-and-Backpropagation" class="headerlink" title="Computation Graphs and Backpropagation"></a>Computation Graphs and Backpropagation</h4><p>软件工程中用图来表示神经网络式子：</p><ul><li>源节点：输入</li><li>内部节点：操作</li><li>边传递操作的结果</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206210806.png" alt="image-20210206210804087" style="zoom:20%;"/></p><h4 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h4><p>在边上反向传递梯度</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206211220.png" alt="image-20210206211217584" style="zoom:20%;"/></p><h4 id="单一节点的反向传播"><a href="#单一节点的反向传播" class="headerlink" title="单一节点的反向传播"></a>单一节点的反向传播</h4><ul><li>每个节点接收一个“向上流的梯度”</li><li>目标是传递正确的“向下流的梯度”</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206211520.png" alt="image-20210206211518562"  style="zoom:20%;" /></p><p>每个节点有一个局部的梯度，这个梯度是其输出相对于其输入</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206212009.png" alt="image-20210206212007268" style="zoom:20%;"  /></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206212754.png" alt="image-20210206212751321" style="zoom:20%;"  /></p><p>往下流的梯度 = 往上流的梯度 x 局部的梯度</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206212834.png" alt="image-20210206212832132" style="zoom:20%;" /></p><p>多输入代表多个局部梯度</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206212954.png" alt="image-20210206212952166" style="zoom:20%;"  /></p><p><strong>例子</strong></p><p>函数f(x)的图表示如下，每个节点输入如图。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206213634.png" alt="image-20210206213631261" style="zoom:20%;"  /></p><p>对于第一个<code>加</code>节点，局部梯度: a对x， y分别为1， 1</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206213801.png" alt="image-20210206213756796" style="zoom:20%;"  /></p><p>对于 <code>max</code>节点， 局部梯度： b对y, z分别为1， 0</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206214040.png" alt="image-20210206214007847" style="zoom:20%;"  /></p><p>对于 <code>*</code>节点， 局部梯度：f对a， b分别为2， 3</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206214221.png" alt="image-20210206214217247" style="zoom:20%;"  /></p><p>最终反向梯度如下：</p><ul><li>局部梯度，可以理解为这个节点的梯度，算不清就让其变化0.1(比较小的值)，看对应输出变化多少，局部梯度就等于输出变化量/0.1。<ul><li>加法，输入边都是1</li><li>乘法，互换输入</li><li>max，起作用的那一边是1，不起作用是0</li></ul></li><li>边下面就是反向传播的梯度，整个下一层的反向梯度就是节点局部梯度 x 反向传播的梯度。</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206214941.png" alt="image-20210206214935426" style="zoom:20%;"  /></p><h4 id="Gradients-sum-at-outward-branches"><a href="#Gradients-sum-at-outward-branches" class="headerlink" title="Gradients sum at outward branches"></a>Gradients sum at outward branches</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206222328.png" alt="image-20210206222325837" style="zoom:20%;"  /></p><h4 id="Efficiency-compute-all-gradients-at-once"><a href="#Efficiency-compute-all-gradients-at-once" class="headerlink" title="Efficiency: compute all gradients at once"></a>Efficiency: compute all gradients at once</h4><p>计算反向传播，不正确的方式：</p><ol><li>一开始计算分数s对偏置b的梯度</li><li>然后独立计算，分数s对权重的梯度</li><li>这是重复计算</li></ol><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206223518.png" alt="image-20210206223516711" style="zoom:20%;"  /></p><p>正确方式：</p><ul><li>同时计算所有梯度</li><li>当手工计算梯度是，像上面类似地用$\delta$</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206223654.png" alt="image-20210206223651956" style="zoom:20%;"  /></p><h4 id="Back-Prop-in-General-Computation-Graph"><a href="#Back-Prop-in-General-Computation-Graph" class="headerlink" title="Back-Prop in General Computation Graph"></a>Back-Prop in General Computation Graph</h4><ol><li><p>前向：按拓扑排序访问节点</p><ul><li>计算给定预处理节点的值</li></ul></li><li><p>反向</p><ul><li>初始化 输出梯度 = 1</li><li>逆序访问所有节点</li><li>使用节点的后继的梯度来计算每个节点的梯度</li></ul><p>前向和反向复杂度都是big O()，通常，我们网络是固定层结构，所以我们使用矩阵和Jacobian。</p></li></ol><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206224159.png" alt="image-20210206224157886" style="zoom:20%;" /></p><h4 id="Automatic-Differentiation"><a href="#Automatic-Differentiation" class="headerlink" title="Automatic Differentiation"></a>Automatic Differentiation</h4><p>自动微分：</p><blockquote><ul><li>梯度计算可以从 Fprop 的符号表达式中自动推断</li><li>每个节点类型需要知道如何计算其输出，以及如何在给定其输出的梯度后计算其输入的梯度</li><li>现代DL框架(Tensorflow, Pytoch)反向传播，但主要是令作者手工计算层/节点的局部导数</li></ul></blockquote><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206225148.png" alt="image-20210206225146046" style="zoom:20%;" /></p><h4 id="Backprop-Implementations"><a href="#Backprop-Implementations" class="headerlink" title="Backprop Implementations"></a>Backprop Implementations</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206230001.png" alt="image-20210206225958824" style="zoom:20%;" /></p><h4 id="Implementation-forward-backward-API"><a href="#Implementation-forward-backward-API" class="headerlink" title="Implementation: forward/backward API"></a>Implementation: forward/backward API</h4><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206230042.png" alt="image-20210206230040356" style="zoom:20%;"  /></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206230103.png" alt="image-20210206230101258" style="zoom:20%;"  /></p><h4 id="Manual-Gradient-checking-Numeric-Gradient"><a href="#Manual-Gradient-checking-Numeric-Gradient" class="headerlink" title="Manual Gradient checking: Numeric Gradient"></a>Manual Gradient checking: Numeric Gradient</h4><ul><li>h设置为非常小的数(1e-4)</li><li>容易正确实现</li><li>但接近非常慢</li><li>检查你的作业实现很有用</li></ul><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210206230129.png" alt="image-20210206230127732" style="zoom:20%;"  /></p><h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><blockquote><ul><li>我们已经掌握了神经网络的核心技术</li><li>反向传播：沿计算图递归应用链式法则<ul><li>[downstream gradient] = [upstream gradient] x [local gradient]</li></ul></li><li>前向传递：计算操作结果并保存中间值</li><li>反向传递：应用链式法则计算梯度</li></ul><p><strong>Why learn all these details about gradients?</strong></p><ul><li>现代深度学习框架为您计算梯度</li><li>但是，当编译器或系统为您实现时，为什么要学习它们呢？<ul><li>了解引擎下发生了什么是有用的</li></ul></li><li>反向传播并不总是完美地工作<ul><li>理解为什么对调试和改进模型至关重要</li><li>参见<a href="https://medium.com/@karpathy/yes-you-should-understandbackprop-    e2f06eab496b">Karpathy文章</a> （在教学大纲中）</li></ul></li><li>未来课程的例子:爆炸和消失的梯度</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Backpropagation </tag>
            
            <tag> CS224n </tag>
            
            <tag> gradient </tag>
            
            <tag> Automatic Differentiation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224n Lecture 2 Word Vectors,Word Senses, and Classifier Review</title>
      <link href="2020/12/04/CS224N%20Lecture%202%20Word%20Vectors%202%20and%20Word%20Senses/"/>
      <url>2020/12/04/CS224N%20Lecture%202%20Word%20Vectors%202%20and%20Word%20Senses/</url>
      
        <content type="html"><![CDATA[<h3 id="1-Review-Main-idea-of-word2vec"><a href="#1-Review-Main-idea-of-word2vec" class="headerlink" title="1. Review: Main idea of word2vec"></a>1. Review: Main idea of word2vec</h3><p>如图，中心词预测上下文- Skip-Gram model </p><ul><li>随机一个词向量开始</li><li>在整个语料库上迭代每个词</li><li>试着用中心词预测周围词，如下图</li></ul><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125103022081.png"  style="zoom:20%;" align=center  alt="中心词预测上下文"  /></p><ul><li><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125103617343.png" style="zoom:20%;" align=center  alt="图P(o|c)"  /></p></li><li><p>更新向量</p></li><li>该算法学习词向量， 在词向量空间上，获得相似性和有意义的方向。U和V是长度为N的向量，需要学习得到。</li></ul><h4 id="Word2vec-parameters-and-computations"><a href="#Word2vec-parameters-and-computations" class="headerlink" title="Word2vec parameters and computations"></a>Word2vec parameters and computations</h4><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125103947657.png" style="zoom:20%;" align=center  alt="词向量计算示意" /></p><p>注意：</p><ul><li>在每个方向上都是一样的预测</li><li>期望模型对所有出现在上下文（相当频繁）的词给一个合理的高概率值估计</li></ul><h4 id="Word2vec-maximizes-objective-function-by-putting-similar-words-nearby-in-space"><a href="#Word2vec-maximizes-objective-function-by-putting-similar-words-nearby-in-space" class="headerlink" title="Word2vec maximizes objective function by putting similar words nearby in space"></a>Word2vec maximizes objective function by putting similar words nearby in space</h4><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125104701714.png"  width = "300" align=center  alt="相似词在空间上的示意" /></p><h3 id="2-Optimization-Gradient-Descent"><a href="#2-Optimization-Gradient-Descent" class="headerlink" title="2. Optimization: Gradient Descent"></a>2. Optimization: Gradient Descent</h3><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125104935183.png" alt="梯度下降示意图"  width = "300" align=center /></p><h4 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h4><ul><li><p>更新公式（矩阵形式）</p><script type="math/tex; mode=display">\theta^{new}= \theta^{old} - \alpha \nabla_{\theta} J(\theta) \tag{1}</script><p>其中<script type="math/tex">\alpha</script>是步进或学习率</p></li><li><p>更新公式 （单一参数）</p></li></ul><script type="math/tex; mode=display">\theta_j^{new}= \theta_j^{old} - \alpha \frac{\partial }{\partial \theta_j^{old}} J(\theta) \tag{2}</script><ul><li>算法</li><li><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210126001846860.png" alt="梯度计算code" width = "400"/></li></ul><h4 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h4><ul><li><p>问题： <script type="math/tex">J(\theta)</script>是一个在整个语料集上遍历窗口的目标函数，可能上百万规模</p><ul><li>计算其梯度非常昂贵</li><li>做单词更新就要非常久。</li><li>对于大量神经元更不好</li></ul></li><li><p>措施：</p><ul><li>SGD</li><li>重复同一窗口，然后更新每个梯度</li></ul></li></ul><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125121342109.png" alt="SGD" width = "400" /></p><h4 id="Stochastic-gradients-with-word-vectors"><a href="#Stochastic-gradients-with-word-vectors" class="headerlink" title="Stochastic gradients with word vectors!"></a>Stochastic gradients with word vectors!</h4><ul><li>重复地在每个窗口上取梯度进行SGD</li><li>在每个窗口，只有至多<script type="math/tex">2m+1</script>个词，其梯度是十分稀疏的</li></ul><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125122411252.png" alt="稀疏梯度" width = "230" /></p><ul><li>可能仅更新实际出现的词向量</li><li>Solution: 要么需要稀疏矩阵更新操作来更新整个嵌入矩阵<code>U</code> 和 <code>V</code>的特定行，或者对每个词向量保留周围的哈希值</li></ul><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125123048188.png" alt="词向量" width = "300" /></p><h4 id="Word2vec-More-details"><a href="#Word2vec-More-details" class="headerlink" title="Word2vec: More details"></a>Word2vec: More details</h4><p>Two model variants:</p><ol><li><p>Skip-grams (SG)： Predict context (“outside”) words (position independent) given center word</p></li><li><p>Continuous Bag of Words (CBOW)：Predict center word from (bag of) context words  </p></li></ol><h4 id="The-skip-gram-model-with-negative-sampling"><a href="#The-skip-gram-model-with-negative-sampling" class="headerlink" title="The skip-gram model with negative sampling"></a>The skip-gram model with negative sampling</h4><ul><li>归一化因子计算非常昂贵，指上面图P(o|c)分母。</li><li>因此，用负采样，用下面两个训练一个二分类logistic regression ：<ul><li>真实值：一对中心词和窗口内的上下文</li><li>随机噪声：一对中心词和随机词</li></ul></li><li>从《Distributed Representations of Words and Phrases and their Compositionality 》中，得目标函数为</li></ul><script type="math/tex; mode=display">J(\theta) = \frac{1}{T} \sum_{t=1}^{T}J_t(\theta) \tag{3}</script><p>其中，</p><script type="math/tex; mode=display">J_t(\theta) = \text{log} \ \sigma(u_o^Tv_c) + \sum_{i=1}^{k} \mathbb{E}_j \sim P(w) \left[\text{log} \ \sigma (-u_j^Tv_c) \right] \tag{4}</script><ul><li>式4的第一个对数， 最大化两个词的共现概率</li><li>更像分类的表示</li></ul><p>词向量<script type="math/tex">u_o, \ v_c</script>相邻，<script type="math/tex">(u_o, \ v_c)</script>作为正样本，再选取k个与词c不相邻的词（k一般不超过15）组成k个负样本。目标函数如下：</p><script type="math/tex; mode=display">J_{neg-sample}(o, v_c, U) = -\text{log} \ \sigma(u_o^Tv_c) + \sum_{i=1}^{k}\text{log} \ \sigma (-u_k^Tv_c)  \tag{5}</script><p>词向量<script type="math/tex">u_o, \ v_c</script>如果相近，那么<script type="math/tex">u_o^Tv_c</script>会很大，经过sigmoid后会趋于1，<script type="math/tex">-\text{log} \ \sigma(u_o^Tv_c)</script>大于0但接近0的树。</p><p>同理，设词k与词c不相邻，<script type="math/tex">u_k^Tv_c</script>是一个很小的数。</p><ul><li>词w的词频为<script type="math/tex">U(w)</script>, 选择词w作为c的不相邻概率为：</li></ul><script type="math/tex; mode=display">P(w) = U(w)^{\frac{3}{4}} / Z</script><p>其中，分母Z是归一化因子，使得<script type="math/tex">\sum P(w)=1</script>,所有词被选取概率和为1。</p><blockquote><p>We investigated a number of choices for Pn(w) and found that the unigram distribution U(w) raised to the<br>3/4rd power (i.e.）. U(w)3/4/Z) outperformed significantly the unigram and the uniform distributions, for &gt; &gt; both NCE and NEG on every task we tried including language modeling (not reported here) .——引用自 Distributed Representations of Words and Phrases and their Compositionality</p></blockquote><h3 id="3-Why-not-capture-co-occurrence-counts-directly"><a href="#3-Why-not-capture-co-occurrence-counts-directly" class="headerlink" title="3. Why not capture co-occurrence counts directly?"></a>3. Why not capture co-occurrence counts directly?</h3><h4 id="共现矩阵X"><a href="#共现矩阵X" class="headerlink" title="共现矩阵X"></a>共现矩阵X</h4><ul><li>实现方法：窗口或者全部文档</li><li>窗口：类似word2vec, 对每个词用窗口，获取句法和语义信息</li><li>全部文档共现矩阵将给出普通topics，导致”隐语义分析“</li></ul><h4 id="Example-Window-based-co-occurrence-matrix"><a href="#Example-Window-based-co-occurrence-matrix" class="headerlink" title="Example: Window based co-occurrence matrix"></a>Example: Window based co-occurrence matrix</h4><ul><li>窗口长度1（通常5-10）</li><li>对称</li></ul><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125133712024.png" alt="共现矩阵" width = "700" /></p><h4 id="Problems-with-simple-co-occurrence-vectors"><a href="#Problems-with-simple-co-occurrence-vectors" class="headerlink" title="Problems with simple co-occurrence vectors"></a>Problems with simple co-occurrence vectors</h4><ul><li>随词汇表的大小增大，高纬度， 需要更多存储空间</li><li>子序列分类问题模型有非常稀疏的问题</li><li>导致模型不鲁棒</li></ul><h4 id="Solution-Low-dimensional-vectors"><a href="#Solution-Low-dimensional-vectors" class="headerlink" title="Solution: Low dimensional vectors"></a>Solution: Low dimensional vectors</h4><ul><li>存储大部分重要的信息到， 一个固定的低维度的稠密向量</li><li>通常25-1000维，类似于word2vec</li><li>如何降低维度</li></ul><h4 id="Method-Dimensionality-Reduction-on-X"><a href="#Method-Dimensionality-Reduction-on-X" class="headerlink" title="Method: Dimensionality Reduction on X"></a>Method: Dimensionality Reduction on X</h4><p>奇异值分解，取前k个奇异值，来近似矩阵X，上一篇论文有代码，主要根据奇异值分解的几何意义。<a href="https://aigonna.com/2020/10/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture7/">介绍SVD</a></p><h4 id="Interesting-syntactic-patterns-emerge-in-the-vectors"><a href="#Interesting-syntactic-patterns-emerge-in-the-vectors" class="headerlink" title="Interesting syntactic patterns emerge in the vectors"></a>Interesting syntactic patterns emerge in the vectors</h4><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125134853497.png" alt="COALS model from An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence Rohde et al. ms., 2005  " width = "300"/></p><h3 id="4-Towards-GloVe-Count-based-vs-direct-prediction"><a href="#4-Towards-GloVe-Count-based-vs-direct-prediction" class="headerlink" title="4. Towards GloVe: Count based vs. direct prediction"></a>4. Towards GloVe: Count based vs. direct prediction</h3><p>基于计数的算法（LSA,HAL）和直接预测的算法（skip-gram,CBOW）各有各的优点和缺点。</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125135145433.png" alt="比较" width = "400" /></p><p>共现概率的比值， 比值可以体现两个词之间的类比关系。</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125135646371.png" alt="共现概率比值大小" width = "300" /></p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125135809612.png" alt="共现概率比值数值" width = "300"/></p><p>举例说明就是，冰ice和固体soild与蒸汽steam和固体soild比较，因为是8.9所以说明，前者关系更大，也就是冰与蒸汽相比，冰与固体关系更大，或者说越接近。</p><h4 id="Encoding-meaning-in-vector-differences"><a href="#Encoding-meaning-in-vector-differences" class="headerlink" title="Encoding meaning in vector differences"></a>Encoding meaning in vector differences</h4><p>Q:How can we capture ratios of co-occurrence probabilities as linear meaning components in a word vector space?  </p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125175629339.png" alt="对数一元线性概率比" width = "300"/></p><h4 id="Combining-the-best-of-both-worlds-GloVe-Pennington-et-al-EMNLP-2014"><a href="#Combining-the-best-of-both-worlds-GloVe-Pennington-et-al-EMNLP-2014" class="headerlink" title="Combining the best of both worlds GloVe [Pennington et al., EMNLP 2014]"></a>Combining the best of both worlds GloVe [Pennington et al., EMNLP 2014]</h4><p>Glove每个词只有一个词向量，我们希望词i和词j的词向量可以表示词i在词j周围的概率。</p><script type="math/tex; mode=display">w_iw_j = \text{log} P(i|j)</script><p>如果对于词x，用如下公式来推断x跟a有关还是b有关。</p><script type="math/tex; mode=display">w_x \cdot (w_a - w_b) = \text{log} \frac{P(x|a)}{P(x|b)}</script><p>$w_x \cdot (w_a - w_b) $越大， x与a有关；越小， x与b有关；适中，可能和a，b有关也有可能都无关。</p><p>人为指定一个目标函数，在最小化目标函数的过程中学习词向量。</p><script type="math/tex; mode=display">J = \sum_{i, j=1}^V f(X_{ij})(w_i^T\tilde w_j + b_i + \tilde b_j -\text{log}X_{ij}) \tag{6}</script><p>其中， w是词向量，X是共现矩阵，b是偏置。f(x)是一个人为规定的函数，近似为min(t, 100), 作用是降低常见词the””, “a”的重要性。</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125214149528.png" alt="函数f" style="zoom:50%;" /></p><p>Golve :</p><ul><li>快速向量</li><li>适合当面语料库</li><li>小语料库和短向量也表示不错</li></ul><h3 id="5-How-to-evaluate-word-vector"><a href="#5-How-to-evaluate-word-vector" class="headerlink" title="5. How to evaluate word vector"></a>5. How to evaluate word vector</h3><p>两种方法：</p><ul><li>Intrinsic：专门设计单独的试验，由人工标注词语或句子相似度，与模型结果对比。好处是是计算速度快，但不知道对实际应用有无帮助。有人花了几年时间提高了在某个数据集上的分数，当将其词向量用于真实任务时并没有多少提高效果。</li></ul><ul><li><p>Extrinsic: 通过对外部实际应用的效果提升来体现。耗时较长，不能排除是否是新的词向量与旧系统的某种契合度产生。需要至少两个subsystems同时证明。</p><h4 id="Intrinsic-word-vector-evaluation"><a href="#Intrinsic-word-vector-evaluation" class="headerlink" title="Intrinsic word vector evaluation"></a>Intrinsic word vector evaluation</h4><p><code>a:b::c:？</code>， 词向量类比。通过词向量夹角的余弦来推断。</p></li></ul><script type="math/tex; mode=display">  d = \text{arg } \text{max }_i \frac{(x_b-x_a+x_c)^Tx_i}{\lvert \lvert x_b - x_a +x_c \rvert \rvert} \tag{7}</script><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125214725149.png" alt="类比词向量" style="zoom:20%;" /></p><p>这样的类比在词向量中其实很简单，“woman”词向量减去“man”词向量近似于“king”词向量减去“queen”词向量即可。</p><h4 id="Glove-Visualizations"><a href="#Glove-Visualizations" class="headerlink" title="Glove Visualizations"></a>Glove Visualizations</h4><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125214834182.png" alt="Glove Visualizations" width = "300" /></p><h4 id="Analogy-evaluation-and-hyperparameters"><a href="#Analogy-evaluation-and-hyperparameters" class="headerlink" title="Analogy evaluation and hyperparameters"></a>Analogy evaluation and hyperparameters</h4><p>Glove 词向量估计：</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125215328996.png" alt="结果" width = "300" /></p><p>在不同大小的语料上，训练不同维度的词向量，在语义和语法数据集上的结果。</p><p>下图显示：</p><ul><li>更多的数据是有帮助的</li><li>wiki比新闻文本更好</li><li>维度</li><li>最合适的维度大约300</li></ul><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125215527836.png" alt="image-20210125215527836" style="zoom:30%;" /></p><h4 id="另一个内在词向量估计"><a href="#另一个内在词向量估计" class="headerlink" title="另一个内在词向量估计"></a>另一个内在词向量估计</h4><ul><li>词向量的距离跟人类评判有关</li></ul><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125215933556.png" width = "300" align=center  alt="词与词的距离" /></p><ul><li>词向量距离和人类评判关系</li></ul><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125220136859.png" width = "300" align=center alt="不同模型比较" /> </p><h4 id="外在词向量估计"><a href="#外在词向量估计" class="headerlink" title="外在词向量估计"></a>外在词向量估计</h4><ul><li>所有子序列任务在一类。</li><li>一个好的词向量有直接帮助：命名实体识别：找一个人，一个组织，或地点</li></ul><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125220625418.png" alt="命名实体识别对比" width = "300" align=center/></p><h3 id="6-词义和消歧"><a href="#6-词义和消歧" class="headerlink" title="6. 词义和消歧"></a>6. 词义和消歧</h3><h4 id="Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy——-TACL2018"><a href="#Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy——-TACL2018" class="headerlink" title="Linear Algebraic Structure of Word Senses, with Applications to Polysemy—— TACL2018"></a>Linear Algebraic Structure of Word Senses, with Applications to Polysemy—— TACL2018</h4><p>一个多义词的词向量等于其各个意思的词向量的加权和。</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210125234241554.png" alt="多义词词向量计算" width = "300" align=center /></p><p>待续….</p><h5 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h5><p>[1] <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/readings/cs224n-2019-notes02-wordvecs2.pdf">cs224n-2019-notes02-wordvecs2</a></p><p>[2] <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/slides/cs224n-2020-lecture02-wordvecs2.pdf">cs224n-2020-lecture02-wordvecs2</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/60623973">2019版CS224N中文笔记(2)词向量的计算与评价</a></p><p>[4] <a href="https://zh.d2l.ai/chapter_natural-language-processing/glove.html">全局向量的词嵌入</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Word2vec </tag>
            
            <tag> negative sampling </tag>
            
            <tag> Skip-Gram </tag>
            
            <tag> Glove </tag>
            
            <tag> CS224n </tag>
            
            <tag> 词向量评价 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N Lecture 1 Introduction and Word Vectors</title>
      <link href="2020/12/02/CS224N%20Lecture%201%20Introduction%20and%20Word%20Vectors/"/>
      <url>2020/12/02/CS224N%20Lecture%201%20Introduction%20and%20Word%20Vectors/</url>
      
        <content type="html"><![CDATA[<h3 id="Word-meaning"><a href="#Word-meaning" class="headerlink" title="Word meaning"></a>Word meaning</h3><h4 id="Representing-words-as-discrete-symbols"><a href="#Representing-words-as-discrete-symbols" class="headerlink" title="Representing words as discrete symbols"></a>Representing words as discrete symbols</h4><p>在传统NLP中， 我们将word视为离散的符号。特别是<code>one-hot encoding</code>， 如：</p><script type="math/tex; mode=display">model = [0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 1 \ 0 \ 0 \ 0 \ 0]\\hotel = [0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 1 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0 \ 0] \tag{1}</script><p>其中， 向量维度= 词汇表单词数量(e.g, 500,000)</p><h4 id="Problem-with-words-as-discrete-symbols"><a href="#Problem-with-words-as-discrete-symbols" class="headerlink" title="Problem with words as discrete symbols"></a>Problem with words as discrete symbols</h4><ul><li>式2两个向量正交</li><li>对于one-hot 向量没有相似性的自然理解。</li></ul><p><strong>Solution</strong>:</p><p>替代：学习把相似性编码到向量里。</p><h4 id="Representing-words-by-their-context"><a href="#Representing-words-by-their-context" class="headerlink" title="Representing words by their context"></a>Representing words by their context</h4><ul><li>语义分布：单词的意思有其频繁出现的近邻词来给定</li><li>当一个单词w出现在文本中，其上下文是出现在附近的一系列单词（ 在一个固定的窗口内）</li><li>用许多w的文本向量来构建一个w的表示矩阵</li></ul><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210122163517955.png" alt="banking" style="zoom:20%;" /></p><h3 id="Word2vec：-Overview"><a href="#Word2vec：-Overview" class="headerlink" title="Word2vec： Overview"></a>Word2vec： Overview</h3><p>Word2vec （Mikolov et al. 2013  ） 是为了学习词向量的框架。</p><p>思路：</p><ul><li>我们有一个庞大的文本语料库</li><li>在一个固定词汇表的每个词用一个向量表示</li><li>在文本中检查每个位置<code>t</code>，其中有一个中心词<code>c</code> 和 上下文词<code>o</code></li><li>用对于<code>c</code>和<code>o</code>的词向量的相似度来计算给定中心词c的上下文的概率（或相反）</li><li>调整词向量来最大化这个概率</li></ul><p>示例：窗口和处理计算<script type="math/tex">P(w_{t+j }\vert w_t)</script></p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210122170537194.png" alt="into的窗口和概率P计算" style="zoom:20%;" /></p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210122171012974.png" alt="banking的窗口和概率P计算" style="zoom:20%;" /></p><h4 id="Word2vec-objective-function"><a href="#Word2vec-objective-function" class="headerlink" title="Word2vec: objective function"></a>Word2vec: objective function</h4><p>对于每个位置<script type="math/tex">t = 1, \cdots, T</script>,给定中心词<script type="math/tex">w_j</script> 预测在一个固定大小<code>m</code>窗口上下文</p><script type="math/tex; mode=display">Likelihood = L(\theta) = \prod_{t=1}^{T} \prod_{-m\le j \ge m ,\ j \neq0} P(w_{t+j} \vert w_{t}; \theta) \tag{2}</script><p>最大化的预测准确率式2，等于最小化目标函数：</p><script type="math/tex; mode=display">J(\theta) = -\frac{1}{T}log L(\theta) = -\frac{1}{T}\prod_{t=1}^{T} \prod_{-m\le j \ge m ,\ j \neq0} log \ P(w_{t+j} \vert w_{t}; \theta) \tag{3}</script><h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><h3 id="基于SVD-的方法"><a href="#基于SVD-的方法" class="headerlink" title="基于SVD 的方法"></a>基于SVD 的方法</h3><h4 id="Word-Document-Matrix"><a href="#Word-Document-Matrix" class="headerlink" title="Word-Document Matrix"></a>Word-Document Matrix</h4><p><strong>矩阵X</strong>:</p><ul><li>我们遍历百万级的文档，每次当单词i出现在文档j中，添加一个元素<script type="math/tex">X_{ij}</script>.显然这是一个规模非常大的矩阵<script type="math/tex">\mathbb{R}^{\lvert V \rvert \times M}</script>。</li><li>M是文档数目， V是词汇表的长度</li></ul><p><strong>Notes：</strong></p><p><strong>Distributional semantics</strong> 语义分布: 这个概念表示一个<strong>基于单词通常在上下文的意义</strong>。它是一个<strong>密度</strong>估计，这能更好地获得<strong>相似度</strong>。</p><p>以下语料库包含3句话和窗口大小为1的关系：</p><ol><li>I enjoy flying.</li><li>I like NLP .</li><li>I like deep learning.</li></ol><p>结果的关系矩阵为:</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/Snipaste_2021-01-22_20-40-13.png" alt="Word-Document 关系矩阵" style="zoom: 20%;" /></p><p>Word-Word 共现矩阵的使用：</p><ul><li>生成 <script type="math/tex">\lvert V \rvert \times \lvert V \rvert</script>共现矩阵<script type="math/tex">X</script></li><li>对<script type="math/tex">X</script>使用SVD分解得到<script type="math/tex">X = USV^T</script></li><li>选择<script type="math/tex">U</script>的前<script type="math/tex">k</script>列来得到一个<script type="math/tex">k</script>的词向量</li><li><script type="math/tex">\frac{\sum_{i=1}^{k}\sigma_i}{\sum_{i=1}^{\lvert V \rvert}\sigma_i}</script>表明通过前k维获得的方差数量</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.mat([[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], </span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">u, s, vt = np.linalg.svd(A)</span><br><span class="line">print(<span class="string">&quot;U:&quot;</span>, u,<span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;Sigma:&quot;</span>, s, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;VT:&quot;</span>,vt)</span><br><span class="line"></span><br><span class="line">===============================================================================</span><br><span class="line">U: </span><br><span class="line">[[ <span class="number">0.09455612</span>  <span class="number">0.86336676</span> -<span class="number">0.12220393</span>  <span class="number">0.25575989</span> -<span class="number">0.13285143</span> -<span class="number">0.02767876</span></span><br><span class="line">   <span class="number">0.38326809</span>]</span><br><span class="line"> [ <span class="number">0.8528587</span>  -<span class="number">0.15327167</span> -<span class="number">0.21454303</span>  <span class="number">0.27874733</span>  <span class="number">0.19389029</span> -<span class="number">0.28718474</span></span><br><span class="line">  -<span class="number">0.07309257</span>]</span><br><span class="line"> [ <span class="number">0.37014698</span> -<span class="number">0.05682519</span>  <span class="number">0.17128669</span>  <span class="number">0.04188288</span> -<span class="number">0.63749818</span>  <span class="number">0.64020128</span></span><br><span class="line">  -<span class="number">0.11138756</span>]</span><br><span class="line"> [ <span class="number">0.08992443</span>  <span class="number">0.42456305</span>  <span class="number">0.30406875</span> -<span class="number">0.05063662</span>  <span class="number">0.45258658</span>  <span class="number">0.24113937</span></span><br><span class="line">  -<span class="number">0.67353925</span>]</span><br><span class="line"> [ <span class="number">0.1664946</span>   <span class="number">0.01282527</span> -<span class="number">0.45011352</span> -<span class="number">0.52355753</span>  <span class="number">0.38881548</span>  <span class="number">0.50010667</span></span><br><span class="line">   <span class="number">0.30678371</span>]</span><br><span class="line"> [ <span class="number">0.04823424</span>  <span class="number">0.20744275</span> -<span class="number">0.41149144</span> -<span class="number">0.53891401</span> -<span class="number">0.42697718</span> -<span class="number">0.36324401</span></span><br><span class="line">  -<span class="number">0.42500792</span>]</span><br><span class="line"> [ <span class="number">0.29757182</span>  <span class="number">0.06652636</span>  <span class="number">0.66731789</span> -<span class="number">0.53668218</span> -<span class="number">0.00561351</span> -<span class="number">0.25826353</span></span><br><span class="line">   <span class="number">0.32703632</span>]] </span><br><span class="line"> Sigma: </span><br><span class="line"> [<span class="number">2.72252553</span> <span class="number">2.49475</span> <span class="number">1.84142251</span> <span class="number">1.58016483</span> <span class="number">1.1834359</span>  <span class="number">0.83631332</span> <span class="number">0.61349731</span>] </span><br><span class="line"> VT: [[ <span class="number">0.76247747</span>  <span class="number">0.10249184</span>  <span class="number">0.05244776</span>  <span class="number">0.3744146</span>   <span class="number">0.1423297</span>   <span class="number">0.42256005</span></span><br><span class="line">   <span class="number">0.24525713</span>  <span class="number">0.07887119</span>]</span><br><span class="line"> [-<span class="number">0.14565329</span>  <span class="number">0.86232953</span>  <span class="number">0.42922518</span> -<span class="number">0.05629678</span>  <span class="number">0.19684915</span> -<span class="number">0.03477114</span></span><br><span class="line">   <span class="number">0.00388863</span>  <span class="number">0.08829262</span>]</span><br><span class="line"> [-<span class="number">0.14000011</span>  <span class="number">0.03239935</span> -<span class="number">0.28982777</span> -<span class="number">0.36094733</span>  <span class="number">0.5275197</span>   <span class="number">0.2458832</span></span><br><span class="line">   <span class="number">0.45541128</span> -<span class="number">0.46790183</span>]</span><br><span class="line"> [ <span class="number">0.3793133</span>   <span class="number">0.29166778</span> -<span class="number">0.17919277</span> -<span class="number">0.154927</span>   -<span class="number">0.37168199</span> -<span class="number">0.16323288</span></span><br><span class="line">  -<span class="number">0.31313145</span> -<span class="number">0.67238019</span>]</span><br><span class="line"> [-<span class="number">0.21101068</span>  <span class="number">0.15791622</span> -<span class="number">0.4730536</span>   <span class="number">0.49238473</span>  <span class="number">0.37769099</span>  <span class="number">0.15909335</span></span><br><span class="line">  -<span class="number">0.54342756</span> -<span class="number">0.03224653</span>]</span><br><span class="line"> [ <span class="number">0.07871667</span>  <span class="number">0.22214383</span> -<span class="number">0.46743578</span>  <span class="number">0.25459588</span> -<span class="number">0.02047576</span> -<span class="number">0.65220565</span></span><br><span class="line">   <span class="number">0.45669219</span>  <span class="number">0.16364998</span>]</span><br><span class="line"> [-<span class="number">0.41984324</span>  <span class="number">0.15158491</span> -<span class="number">0.06803587</span>  <span class="number">0.38091632</span> -<span class="number">0.56479943</span>  <span class="number">0.41392806</span></span><br><span class="line">   <span class="number">0.35150727</span> -<span class="number">0.19270534</span>]</span><br><span class="line"> [ <span class="number">0.08333333</span>  <span class="number">0.25</span>       -<span class="number">0.5</span>        -<span class="number">0.5</span>        -<span class="number">0.25</span>        <span class="number">0.33333333</span></span><br><span class="line">  -<span class="number">0.08333333</span>  <span class="number">0.5</span>       ]]</span><br></pre></td></tr></table></figure><h4 id="Applying-SVD-to-the-cooccurrence-matrix"><a href="#Applying-SVD-to-the-cooccurrence-matrix" class="headerlink" title="Applying SVD to the cooccurrence matrix"></a>Applying SVD to the cooccurrence matrix</h4><p>通过对X做SVD， 观察对角阵上对角元素， 基于期望百分比方差获得前k个索引：</p><script type="math/tex; mode=display">\frac{\sum_{i=1}^{k}\sigma_i}{\sum_{i=1}^{\lvert V \rvert}\sigma_i} \tag{4}</script><p><strong>对X应用SVD</strong>：</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210122213039724.png" alt="X的SVD分解维度变化" style="zoom:20%;" /></p><p>通过选择前k个奇异向量:</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210122213205903.png" alt="选取前k个维度后奇异值变化" style="zoom:20%;" /></p><p>这些方法给予我们词向量， 这不仅仅足以编码语义和句法（语音部分）信息，但其引起许多其他问题:</p><ul><li>矩阵维度经常变化（新词频繁加入和语料库大小变化）</li><li>矩阵十分稀疏，大多数词不经常出现</li><li>矩阵通常维度非常高(<script type="math/tex">\approx 10^6 \times 10^6</script>)</li><li>平方次复杂度的训练代价(e.g 执行SVD)</li><li>需要在X上创建一些技巧来解决词频的急剧不平衡</li></ul><p>一些存在来解决这些问题的措施如下：</p><ul><li>忽略一些功能词如”the”, “he”, “has”, etc.  </li><li>应用ramp window—例如，基于文档中词与词距离加上一个共现次数的权重</li><li>用皮尔逊系数将负数设为0而不是使用原始计数</li></ul><h3 id="Iteration-Based-Methods-Word2vec"><a href="#Iteration-Based-Methods-Word2vec" class="headerlink" title="Iteration Based Methods - Word2vec"></a>Iteration Based Methods - Word2vec</h3><ul><li><p><strong>2 algorithms</strong> : continuous bag-of-words (CBOW) and skip-gram . CBOW 目标是用周围上下文的词向量来预测中心词。Skip-gram正好相反，用中心词来预测上下文的分布或者说概率。</p></li><li><p><strong>2 training methods</strong> ：负采样或hierarchical softmax。负采样通过抽取负样本来定义目标，而hierarchical softmax用有效的树结构来对所有词汇计算概率从而定义目标。</p></li></ul><h4 id="Language-Models-Unigrams-Bigrams-etc"><a href="#Language-Models-Unigrams-Bigrams-etc" class="headerlink" title="Language Models (Unigrams, Bigrams, etc.)"></a>Language Models (Unigrams, Bigrams, etc.)</h4><p><strong>Unigram model</strong>: 假设每个词出现的完全独立</p><script type="math/tex; mode=display">p(w_1, w_2, \cdots, w_n) = \prod_{i=1}^{n} P(w_i) \tag{5}</script><p>Bigram model： 假定sequence 的概率取决于一个词在sequence中和其旁边词出现的概率：</p><script type="math/tex; mode=display">p(w_1, w_2, \cdots, w_n) = \prod_{i=1}^{n} P(w_i \vert w_{i-1}) \tag{6}</script><h4 id="Continuous-Bag-of-Words-Model-CBOW"><a href="#Continuous-Bag-of-Words-Model-CBOW" class="headerlink" title="Continuous Bag of Words Model (CBOW)"></a>Continuous Bag of Words Model (CBOW)</h4><p>{“The”, “cat”, ’over”, “the’, “puddle”}  作为上下文，可以预测或生成中心词”jumped”  。——CBOW。</p><p>CBOW细节：</p><ol><li>设置已知参数。令模型中已知参数用one-hot词向量表示。输入one-hot向量或上下文用<script type="math/tex">x^{(c)}</script>，输出为<script type="math/tex">y^{(c)}</script>。</li><li><p>未知参数：</p><ul><li><p>创建两个矩阵，<script type="math/tex">\mathcal{V} \in \mathbb{R}^{n \times \lvert V \rvert}</script>和<script type="math/tex">\mathcal{U} \in \mathbb{R}^{\lvert V \rvert \times n }</script>.其中， <script type="math/tex">n</script>是由embedding 空间定义的任意维度；</p><p> <script type="math/tex">\mathcal{V}</script>是输入词矩阵，如<script type="math/tex">\mathcal{V}</script>的第<script type="math/tex">i</script>列是当其是模型的输入时，对词 <script type="math/tex">w_i</script>的 <script type="math/tex">n</script>维嵌入空间向量。记为<script type="math/tex">n \times 1 列向量v_i</script>。</p><p> <script type="math/tex">\mathcal{U}</script>是输出词矩阵，如<script type="math/tex">\mathcal{U}</script>的第<script type="math/tex">j</script>列是当其是模型的输出时，对词 <script type="math/tex">w_j</script>的 <script type="math/tex">n</script>维嵌入空间向量。记为<script type="math/tex">1 \times n 行向量u_i</script>。</p><p>注意： 对于每个词<script type="math/tex">w_i</script>都学到两个向量</p></li></ul></li></ol><p><strong>CBOW model：</strong></p><p>从周围上下文来预测中心词：对每个词，我们希望学到两个向量</p><ul><li><script type="math/tex">v</script> 输入向量: 当这个词在上下文中</li><li><script type="math/tex">u</script> 输出向量: 当这个词是中心词</li></ul><p><strong>CBOW model 记号：</strong></p><ul><li>$ w_i $ 输入向量: 词汇表<script type="math/tex">V</script>中的单词<script type="math/tex">i</script></li><li><script type="math/tex">\mathcal{V} \in \mathbb{R}^{n \times \lvert V \rvert}</script>: 输入词矩阵</li><li><script type="math/tex">v_i</script> : <script type="math/tex">\mathcal{V}</script>的第<script type="math/tex">i</script>列，单词<script type="math/tex">w_i</script> 输入表示向量</li><li><script type="math/tex">\mathcal{U} \in \mathbb{R}^{\lvert V \rvert \times n }</script>: 输出词矩阵</li><li><script type="math/tex">u_i</script> : <script type="math/tex">\mathcal{U}</script>的第<script type="math/tex">i</script>行，单词<script type="math/tex">w_i</script> 输出表示向量 </li></ul><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210123144120710.png" alt="图1： 证明CBOW怎么工作以及我们怎么学习转换矩阵" style="zoom:28%;" /></p><p><strong>CBOW model步骤分解：</strong></p><ol><li>为输入的上下文生成大小为<script type="math/tex">m</script>的one-hot词向量：<script type="math/tex">(x^{c-m}, \cdots, x^{c-1}, x^{c+1}, \cdots, x^{c+m} \in \mathbb{R}^{\lvert V \rvert})</script>, 注即中心词<script type="math/tex">c</script>附近<script type="math/tex">m</script>窗口内的词的词向量。</li><li>为上下文<script type="math/tex">( v_ {c-m} = \mathcal{V} x^{c-m},  \ v_ {c-m + 1} = \mathcal{V} x^{c-m +1}, \ \cdots , \  v_ {c+m} = \mathcal{V} x^{c+m} \in \mathbb{R}^n)</script>得到嵌入词向量</li><li>取这些向量的均值得到<script type="math/tex">\hat v = \frac{v_ {c-m}  + v_ {c-m+1} + \cdots + v_ {c+m}}{2m}</script></li><li>生成分数向量 <script type="math/tex">z = \mathcal{U} \hat v \in \mathbb{R}^{\lvert V \rvert}</script>. 因为相似向量的点积会高(点积的几何意义)，为了达到高分数， 将会迫使相似词彼此靠近。</li><li>将分数变为概率<script type="math/tex">\hat y = \text{softmax} (z) \in \mathbb{R}^{\lvert V \rvert}</script>。</li><li>要求生成的概率， <script type="math/tex">\hat y \in \mathbb{R}^{\lvert V \rvert}</script>, 来匹配真正的概率， <script type="math/tex">y \in \mathbb{R}^{\lvert V \rvert}</script>, 这也恰好是真实词的one-hot向量。</li></ol><p>现在，我们了解了如果我们有<script type="math/tex">\mathcal{V} \ 和 \ \mathcal{U}</script>模型怎么工作， 我们怎样学到这两个矩阵?</p><p>利用交叉熵来得到到<code>loss function</code>：</p><script type="math/tex; mode=display">H(\hat y, \ y) = -\sum_{j = 1}^{\lvert V \rvert} y_j  \text{log} (\hat y_{j}) \tag{7}</script><p>$ y $是一个one-hot 向量， 因此，可以将式7简化为：</p><script type="math/tex; mode=display">H(\hat y, \ y) = -y_i\text{log} (\hat y_{j}) \tag{8}</script><p><strong>softmax 操作 </strong>：</p><script type="math/tex; mode=display">\frac {e^{\hat y_i}}{\sum_{k = 1}^{\lvert V \rvert}e^{\hat y_k}} \tag{9}</script><p>​            <strong>Note:</strong>： <script type="math/tex">\hat y  \mapsto H(\hat y, y), \ 当 \ \hat y = y取最小值。那么当我们找到H(\hat y, y)最小值，有\hat y \approx y</script>。这意味着我们模型在预测中心词上是非常好的。</p><p>最小化 目标函数：</p><script type="math/tex; mode=display">\begin{aligned}\text{minimize} \ J&=-\log P\left(w_{c} \mid w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}\right) \\&=-\log P\left(u_{c} \mid \hat{v}\right) \\\\&=-\log \frac{\exp \left(u_{c}^{T} \hat{v}\right)}{\sum_{j=1}^{|V|} \exp \left(u_{j}^{T} \hat{v}\right)} \\\\&=-u_{c}^{T} \hat{v}+\log \sum_{j=1}^{|V|} \exp \left(u_{j}^{T} \hat{v}\right)\end{aligned} \tag{10}</script><p>可以用随机梯度下降来更新所有有关词向量<script type="math/tex">u_c \ 和 \  v_j</script>。</p><h4 id="Skip-Gram-Model"><a href="#Skip-Gram-Model" class="headerlink" title="Skip-Gram Model"></a>Skip-Gram Model</h4><p><strong>Skip-Gram Model :</strong></p><ul><li>给定中心词预测周围上下文</li></ul><p>大部分参数设定跟CBOW一样， 不过把<script type="math/tex">x</script>与 <script type="math/tex">y</script>交换下。 输入one-hot向量是中心词用<script type="math/tex">x</script>表示， 输出向量是周围上下文用<script type="math/tex">y</script>表示。<script type="math/tex">\mathcal{V} \ 和 \ \mathcal{U}</script>跟CBOW一样。</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210123143934936.png" alt="图2： 证明Skip-Gram Model怎么工作以及我们怎么学习转换矩阵" style="zoom:28%;"/></p><p><strong>Skip-Gram model步骤分解：</strong></p><ol><li>生成中心词的one-hot输入向量<script type="math/tex">x \in \mathbb{R}^{\lvert V \rvert}</script>。</li><li>中心词<script type="math/tex">v_{c} = \mathcal{V}x \in \mathbb{R}^n</script> 的嵌入向量。</li><li>生成分数向量 <script type="math/tex">z = \mathcal{U}  v_c \in \mathbb{R}^{\lvert V \rvert}</script>.</li><li>将分数变为概率<script type="math/tex">\hat y = \text{softmax} (z)</script>。注:<script type="math/tex">\hat y^{c-m}, \cdots, \hat y^{c-1}, \hat y^{c+1}, \cdots, \hat y^{c+m}</script>是观察的每个上下文词的概率</li><li>要求生成的概率向量来匹配真实的概率 <script type="math/tex">y^{c-m}, \cdots,  y^{c-1}, y^{c+1}, \cdots, y^{c+m}</script>,即真实输出的one-hot向量。</li></ol><p>CBOW中，是生成目标函数来估计模型。Skip-Gram 是用朴素贝叶斯分解概率表达式， 即假定输出词是完全独立的。</p><script type="math/tex; mode=display">\begin{aligned}\text{minimize} \ J&=-\log P\left(  w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}  \mid w_{c}\right) \to \text{给定中心词， 而不是给定上下文} \\&=-\log \prod_{j=0,\ j \neq m}^{2m} P\left(w_{c-m+j} \mid w_c\right) \\ \\&= -\log \prod_{j=0,\ j \neq m}^{2m} P\left(u_{c-m+j} \mid v_c\right) \\\\&=-\log \prod_{j=0,\ j \neq m}^{2m} \frac{\exp \left(u_{c-m+j}^{T} v_c\right)}{\sum_{k=1}^{|V|} \exp \left(u_{k}^{T} v_c\right)} \\\\&=-\sum_{j=0,\ j \neq m}^{2m}u_{c}^{T} v_c+2m\log \sum_{j=1}^{|V|} \exp \left(u_{j}^{T} v_c\right)\end{aligned} \tag{11}</script><p>利用SGD来迭代求出未知参数：</p><script type="math/tex; mode=display">\begin{align}J &=-\sum_{j=0,\ j \neq m}^{2m} \text{log} \ P(u_{c-m+j}|v_c) \\&= \sum_{j=0,\ j \neq m}^{2m} H(\hat y, \ y_{c-m+j})\end{align} \tag{12}</script><p>注： 仅仅只有一个概率向量<script type="math/tex">\hat y</script>要计算。Skip-gram  将每个上下词平等看待：模型计算独立地出现在中心词的每个上下词对中心词距离的概率。</p><h4 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h4><p>注意到对<script type="math/tex">|V|</script>的计算量巨大。做目标函数的任何更新或计算时间复杂度为<code>O(|V|)</code>。采用负采样可以近似，而不是计算整个词汇表。更新以下参数：</p><ul><li>目标函数</li><li>梯度</li><li>更新准则</li></ul><p>Tomas Mikolov  的《Distributed Representations of Words and Phrases and their Compositionality》论文中提出的负采样。虽然负采样是基于Skip-Gram模型，但实际上是优化不同的目标函数。考虑一对中心词和上下文<script type="math/tex">(w, c)</script>。记<script type="math/tex">P(D=1|w, c)</script>表示<script type="math/tex">(w, c)</script>来自语料数据的概率，对应地记<script type="math/tex">P(D=0|w, c)</script>表示<script type="math/tex">(w, c)</script>不来自语料数据的概率。 接下来， 用<code>sigmoid</code>函数对<script type="math/tex">P(D=1|w, c)</script>建模：</p><script type="math/tex; mode=display">P(D=1|w, c, \theta) = \sigma(v_c^T v_w) = \frac{1}{1+e^{-v_c^T v_w}} \tag{13}</script><p>建立一个新的目标函数，如果中心词和上下文词确实在语料库中，就最大化概率<script type="math/tex">P(D=1|w, c)</script> ，如果中心词和上下文词确实不在语料库中，就最大化概率 <script type="math/tex">P(D=0|w, c)</script>。我们对这两个概率用一个简单的极大似然估计的方法（这里把<script type="math/tex">\theta</script>作为模型的参数，在该例子中是<script type="math/tex">\mathcal{V}</script>和 <script type="math/tex">\mathcal{U}</script>）。</p><script type="math/tex; mode=display">\begin{aligned}\theta &=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 \mid w, c, \theta) \prod_{(w, c) \in \tilde{D}} P(D=0 \mid w, c, \theta) \\ \\&=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 \mid w, c, \theta) \prod_{(w, c) \in \tilde{D}}(1-P(D=1 \mid w, c, \theta)) \\ \\&=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log P(D=1 \mid w, c, \theta)+\sum_{(w, c) \in \tilde{D}} \log (1-P(D=1 \mid w, c, \theta)) \\ \\&=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(1-\frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}\right) \\ \\&=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right)\end{aligned}  \tag{14}</script><p>注意：<strong>最大化似然函函数等价于最小化负对数似然</strong></p><script type="math/tex; mode=display">J = - \left( \underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right)\right) \tag{15}</script><p>注：<script type="math/tex">\tilde D</script>是负语料。无意义的句子出现概率是很低的。我们可以从语料库中随机抽取负样例<script type="math/tex">\tilde D</script>。</p><p>对于<strong>Skip-Gram</strong> ，给定中心词<script type="math/tex">c</script>的观察的上下文词<script type="math/tex">c-m+j</script>，新的目标函数为:</p><script type="math/tex; mode=display">-\text{log} \ \sigma (u_{c-m+j}^T \cdot v_c) - \sum_{k=1}^{K} \sigma (- \tilde u_k^{T} \cdot v_c) \tag{16}</script><p>对于<strong>CBOW</strong> ，给定上下文词向量<script type="math/tex">\hat v = \frac{v_{c-m} + v_{c-m+1}+\cdots+v_{c+m}}{2m}</script>观察的中心词<script type="math/tex">u_c</script>的目标函数为：</p><script type="math/tex; mode=display">-\text{log} \ \sigma (u_{c}^T \cdot \hat v) - \sum_{k=1}^{K} \sigma (- \tilde u_k^{T} \cdot \hat v) \tag{17}</script><p><strong>Skip-Gram</strong>对应的softmax损失：</p><script type="math/tex; mode=display">- u_{c-m+j}^T \cdot v_c + \text{log} \sum_{k=1}^{|V|} \text{exp} ^{ u_k^{T} \cdot v_c} \tag{18}</script><p><strong>CBOW</strong>对应的softmax损失：</p><script type="math/tex; mode=display">- u_{c}^T \cdot \hat v+ \text{log} \sum_{j=1}^{|V|} \text{exp} ^{ u_j^{T} \cdot \hat c} \tag{19}</script><p>上面公式中， <script type="math/tex">\{ \tilde u | k = 1 \cdots K \}</script> 是从<script type="math/tex">P_n(w)</script>抽样得到的。实际上效果最好的是取<script type="math/tex">\frac{3}{4}</script>的Unigram model，论文中取值。</p><h4 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h4><p>同样是, Tomas Mikolov  的《Distributed Representations of Words and Phrases and their Compositionality》论文中提出的Hierarchical Softmax。实际上， Hierarchical Softmax对低频词表现更好，而负采样对高频词和低维向量表现更好。</p><p>Hierarchical Softmax用二叉树表示词汇表中的所有词。树的每一个叶子节点都是一个词，并且只有一条路径从根到叶子。在该模型中，没有词的输出表示。相反， 图的每个节点， 除了根节点和叶子节点， 都跟模型要学的向量相关。</p><p>在该模型中，给定向量<script type="math/tex">w_i</script> 单词<script type="math/tex">w</script>的概率，记为<script type="math/tex">P(w|w_i)</script>,等于从根结点开始到对应<script type="math/tex">w</script> 的叶结点的随机的概率。优势是时间复杂度变为<code>O(log(|V|))</code>,对应该路径的长度。</p><p>引入一些记号：</p><ul><li><p><script type="math/tex">L(w)</script>： 从根节点到叶子节点<script type="math/tex">w</script>的路径上的节点数目。下图， <script type="math/tex">w_2</script>对应3.</p></li><li><p><script type="math/tex">v_n(w, i)</script>：该路径上第i个节点向量， 即叶子节点<code>w</code>到跟节点路径的第<code>i</code>个节点。</p><p><script type="math/tex">n(w, 1)</script>是根节点， 而 <script type="math/tex">n(w, L(w))</script>是<code>w</code>的父节点。</p></li><li><p>对于每个内部节点<code>n</code>, 任意选择其中一个孩子节点称为<script type="math/tex">ch(n)</script>，总是左节点。</p></li></ul><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/image-20210123202804537.png" alt="图Hierarchical Softmax的二叉树" style="zoom:20%;" /></p><p>计算概率为:</p><script type="math/tex; mode=display">P(w \vert w_i) = \prod_{j=1}^{L(w)-1} \sigma \left( \left[ n(w, j+1)=ch(n(w, j))  \right] \cdot v_{n(w, j)^{T}v_{w_i}}\right) \tag{20}</script><p>这里，</p><script type="math/tex; mode=display">[x] = {\begin{cases}1,&{\mbox{if }} \ x \text{ is true}\\-1&{\mbox{if }}\ \ x \text{ is otherwise} \end{cases}} \tag{21}</script><p>式20是风采难懂的。</p><ul><li><p>基于从根节点<script type="math/tex">(n(w, 1))</script>到叶子节点<script type="math/tex">(w)</script>的累乘项。如果我们假设<script type="math/tex">ch(n)</script>总是左节点<script type="math/tex">n</script>, <script type="math/tex">[n(w， j+1)=ch(n(w, j))]</script>就返回1， 反之则是-1.</p></li><li><p><script type="math/tex">[n(w， j+1)=ch(n(w, j))]</script>提供标准化。在节点<script type="math/tex">n</script>上， 如果从左节点到右节点的概率求和，对于任意的<script type="math/tex">v_n^Tv_{w_i}</script>有：</p></li></ul><script type="math/tex; mode=display">\sigma(v_n^Tv_{w_i}) + \sigma (-v_n^Tv_{w_i}) = 1 \tag{22}</script><p>​            同时保证了<script type="math/tex">\sum_{w=1}^{|V|} P(w|w_i) = 1</script>， 跟原来的softmax一样。</p><ul><li>使用点积对比输入向量<script type="math/tex">v_{w_i}</script>和每个内部节点向量<script type="math/tex">v_{n(w, j)}^T</script>相似度。例如，上图 <script type="math/tex">w_2</script>, 我们必须要取两条左边和一条右边来从<script type="math/tex">w_2</script>到根节点，即：</li></ul><script type="math/tex; mode=display">P(w_2|w_i) = p(n(w_2|1), \text{ left}) \cdot p(n(w_2|2), \text{ left}) \cdotp(n(w_2|3), \text{ right}) \cdot \\\\= \sigma(v_{(w_2, 1)}^Tv_{w_i} \cdot \sigma(v_{(w_2, 2)}^Tv_{w_i}\cdot \sigma(-v_{(w_2, 3)}^Tv_{w_i}) \tag{23}</script><p>为了训练模型，我们的目标仍然是最小化负对数似然<script type="math/tex">-log \ P(w|w_i)</script>。但不是更新每个输出向量， 而是更新在二叉树上从根到叶子节点的节点向量。</p><p>该方法的速度取决于二叉树构建的方式和分配到叶子节点的单词。Mikolov 使用了哈夫曼树， 将高频词分配到树的最短路径上。</p><h5 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h5><p>[1] <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/readings/cs224n-2019-notes01-wordvecs1.pdf">cs224n-2019-notes01-wordvecs1</a></p><p>[2] <a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1204/slides/cs224n-2020-lecture01-wordvecs1.pdf">cs224n-2020-lecture01-wordvecs1</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/59228268">CS224n课程笔记翻译 词向量I: 简介, SVD和Word2Vec</a></p><p>[4] <a href="http://alexminnaar.com/2015/04/12/word2vec-tutorial-skipgram.html">Word2Vec Tutorial Part I: The Skip-Gram Model</a></p><p>[5] <a href="http://alexminnaar.com/2015/05/18/word2vec-tutorial-continuousbow.html">Word2Vec Tutorial Part II: The Continuous Bag-of-Words Model</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CBOW </tag>
            
            <tag> Word2vec </tag>
            
            <tag> Skip-Gram </tag>
            
            <tag> CS224n </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Efficient Estimation ofWord Representations in Vector Space——Mikolov论文笔记</title>
      <link href="2020/11/14/1.Mikolov%E8%AE%BA%E6%96%87Efficient%20Estimation%20ofWord%20Representations%20in%20Vector%20Space%20%E7%AC%94%E8%AE%B0/"/>
      <url>2020/11/14/1.Mikolov%E8%AE%BA%E6%96%87Efficient%20Estimation%20ofWord%20Representations%20in%20Vector%20Space%20%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="词向量空间中词表示的有效估计"><a href="#词向量空间中词表示的有效估计" class="headerlink" title="词向量空间中词表示的有效估计"></a>词向量空间中词表示的有效估计</h3><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>提出了两种计算连续词向量表示模型。<br>        特点：计算低，在语法和语义上的词相似度任务上精度有所提升</p><h4 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h4><p>以前模型的优点缺点：</p><ul><li>统计翻译模型的缺陷:把单词当做原子单元，作为语料字典的索引失去了相似性。</li><li>N-gram优点能在任何数据集上训练。</li></ul><p>简单模型在任务上的限制。如，语音识别：要求语音数据质量高，规模大，但不超过10亿级别的词汇量。因此简单技术改进不会有任何进步。</p><p>随着机翻技术发展，现在能大规模数据上训练复杂模型。非常成功的概念是分布式词表示。例如，基于神经语言模型明显地优于N-gram模型</p><h5 id="论文目标"><a href="#论文目标" class="headerlink" title="论文目标"></a>论文目标</h5><p>主要目标：从百万级词汇的大数据集中学习高质量的词向量。之前方法不能在百万级别词上学到词向量在50-100维之间。</p><p>词从原空间映射到词向量子空间后，词与词之间不仅更近，而且有更多相似度。</p><p>惊人的是，词表示的相似度超越了简单的语法规则。如:</p><script type="math/tex; mode=display">\vec{\text{King}} - \vec{\text{Man}} + \vec{\text{Woman}} = \vec{\text{Queen}}</script><p>本文，试图通过开发新模型来最大化词向量操作的准确性，以保留词之间的线性规则。我们设计新的全面的测试集用于测量语法和语义规则，并且其展示了许多规则可以高准确率学到。此外，我们讨论训练时间和经典如何依赖词向量的维度和训练数据量。</p><h5 id="之前的工作"><a href="#之前的工作" class="headerlink" title="之前的工作"></a>之前的工作</h5><p>词表示作为联系向量有很长的历史。流行的<strong>NNLM</strong>模型由Bengio提出，是前馈神经网络，有一个线性投影层和非线性隐藏层，用来学习词向量表示的联合分布和统计语言模型。</p><p>另一个有趣的<strong>NNLM</strong>架构是词向量首先用单隐层的神经网络学习到。然后这个词向量被用来训练<strong>NNLM</strong>。这个词向量不是在构建整个<strong>NNLM</strong>中学到的。我们直接扩展了该架构，集中于第一步，用简单模型学到词向量。</p><h3 id="2-模型架构"><a href="#2-模型架构" class="headerlink" title="2.模型架构"></a>2.模型架构</h3><p>以前估计连续词表示，用LSA(Latent Semantic Analysis)和LDA (Latent Dirichlet Allocation).本文用神经网络学习，在保留词之间的线性规则上，表现好于LSA；而LDA在大数据集上计算代价昂贵。</p><p>为了比较不同模型架构，我们首先定义计算复杂度为充分训练模型所需要的参数的数量。接下来，最小化计算复杂度时最大化准确率。</p><p>模型复杂度正比于：</p><script type="math/tex; mode=display">O = E \times T \times Q \tag{1}</script><p>其中，E是训练轮数，T是训练集里词的数目，Q接下来会具体定义。通常，E是3-50轮，T可达到十亿数量级，所有模型都使用随机梯度下降和反向传播。</p><h4 id="前馈神经网络-（NNLM）"><a href="#前馈神经网络-（NNLM）" class="headerlink" title="前馈神经网络 （NNLM）"></a>前馈神经网络 （NNLM）</h4><p>下图是<strong>NNLM</strong>，来源于 <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210305150428.png" alt="image-20210305150250100" style="zoom:28%;" /></p><p>包含输入层， 投影层， 隐藏层和输出层。在输入层， N个预测词之前的单词用1xV的one-hot编码，V是词汇表大小。输入层用共享的投影矩阵，然后被投影到NxD投影层 P,。因为在给定时间只有N个输入有效，构成投影层是相对廉价的操作。     </p><p>NNLM架构在投影层和隐藏层的计算变得复杂， 因为投影层上的值是稠密的。通常选择N=10, 投影层P可能是500-2000，与此同时，隐藏层通常为500-1000.而且，隐藏通常用来计算字典中所有词概率分布， 这导致输出层是维度V。因此，每一个训练样本的计算复杂度为：(H是隐藏层节点个数)</p><script type="math/tex; mode=display">Q = N \times D + N \times D \times H + H\times V \tag{2}</script><p>其中主要计算量来自于$H \times V$。然而，几个实际解决方案提出来避免计算量。比如石油分层版本的softmax，或者避免在训练中通过使用没有归一化的模型来避免完全归一模型。 用二叉树表示词汇表， 需要被估计的输出单元数目能降到$\text{log}_2(V)$。因此，主要计算复杂度源自于$N \times D \times H $项。</p><p>在本文模型中，我们使用层次softmax， 词汇表表示成哈夫曼树。这延续之前的研究， 词频对于在神经网络语言模型中获取类别是有效的。哈夫曼树，分配给高频词端的二进制编码，这进一步减少了要估计的输出单元的数目：虽然平衡二叉树有$\text{log}_2(V) $的输出要估计， 但基于层次的softmax哈夫曼树只需要$\text{log}_2(\text{Unigram_perplexity}(V))$(V的一元困惑度)。 举例来说， 当词汇表是100万， 使用该结构估计时大约快两倍。但对于神经网络语言模型来说，计算瓶颈在$N \times D \times H $ 项， 我们后面将替代不含隐藏层， 严重取决于softmax归一化效率。</p><h4 id="循环神经网络语言模型-（RNNLM）"><a href="#循环神经网络语言模型-（RNNLM）" class="headerlink" title="循环神经网络语言模型 （RNNLM）"></a>循环神经网络语言模型 （RNNLM）</h4><p>基于语言模型的循环神经网络， 提出来克服前馈神经语言模型的限制，比如需要具体的上下文长度(模型的顺序为N)， 因为理论上比起浅层神经网络， RNNs能更有效地表示更复杂的模式。RNN模型没有投影层，只有输入，隐藏层和输出层。 这类模型的特殊之处是连接自身隐藏层间的循环矩阵， 用来形成短期记忆，因为来自过去的信息可以有隐藏层状态表示， 隐藏层状态根据当前输入和上时间中的隐层状态来更新。</p><p>循环神经网络训练每个样本是的复杂度为：</p><script type="math/tex; mode=display">Q = H \times H + H \times V \tag{3}</script><p>其中，词表示D和隐藏层H有同样维度。这样通过使用层次softmax  $H \times V$ 有效地降为$H \times \text{log}_2(V)$。 复杂度主要来自于 $ H \times H$</p><h4 id="平行训练神经网络"><a href="#平行训练神经网络" class="headerlink" title="平行训练神经网络"></a>平行训练神经网络</h4><p>为了在大数据集上训练模型，我们在大规模分布式框架之上实现了几个模型叫做DistBelief, 包括前馈神经语言模型和本文中提出的新模型。该框架允许我们并行地运行同一模型的多个副本，并且每个副本通过保留所有参数的中央服务器来同步更新其梯度。对应这种并行训练，我们使用称为Adagrad的自适应学习速率程序小批次异步梯度下降。在该框架下， 通常使用100或更多模型副本，每个腹部在数据中的不同机器上使用不同<strong>CPU</strong>内核。</p><h3 id="3-新的对数线性模型"><a href="#3-新的对数线性模型" class="headerlink" title="3. 新的对数线性模型"></a>3. 新的对数线性模型</h3><p>在本节中， 我们提出两种新模型架构，用于学习分布式词表示， 并试图最小化计算复杂度。从前一节的主要研究中， 大部分复杂度源自于模型中非线性的隐藏层。虽然这是是的神经网络如此吸引人的地方， 我们觉得探索简单模型， 其可能无法像神经网络一样精确地表示数据，但可以在更多数据上有效地训练。</p><p>新的架构直接遵循我们早期工作中提出的， 其发现神经语言模型可以分成两步成功训练 (1.2节): 首先， 连续词向量用简单模型学习，然后 <strong>N-gram NNLM</strong> 在这些分布式词表示上训练。算人后面大量的工作重点学习词向量，我们认为之前提出的是最简单的。注意，相关模型<strong>Hinton</strong>等人早就提出了。</p><h4 id="连续词袋模型-Bag-of-Words"><a href="#连续词袋模型-Bag-of-Words" class="headerlink" title="连续词袋模型 (Bag-of-Words)"></a>连续词袋模型 (Bag-of-Words)</h4><p>首先提出价格类似于前馈<strong>NNLM</strong>， 其中非线性隐藏层被移除并且投影层为所有词所共享(不仅仅是投影矩阵)：隐藏， 所有词都被投影到相同位置(它们词向量被平均)。我们叫这种架构为词袋模型，因为在历史记录里词的顺序不影响投影。 此外，我们还使用前面的词；我们在下一节要介绍的，用四个前面的词和四个后面的词作为输入建立的一个对数线性分类器任务上得到最佳表现，其中，训练标准是正确分类当前(中间)词。训练复杂度为：</p><script type="math/tex; mode=display">Q = N \times D + D \times \text{log}_2(V) \tag{4}</script><p>其中， N是输入数，D是输入one-hot维度， V是词汇表大小    </p><p>我们把这个模型称为CBOW， 不同于标准词袋模型，它使用上下文的连续分布表示。如下图，注意输入层和投影层的权重矩阵，由所有词共现，跟NNLM一样。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210306103855.png" alt="image-20210306103847361" style="zoom:25%;" /></p><h4 id="连续跳字模型-Continuous-Skip-gram-Model"><a href="#连续跳字模型-Continuous-Skip-gram-Model" class="headerlink" title="连续跳字模型 (Continuous Skip-gram Model)"></a>连续跳字模型 (Continuous Skip-gram Model)</h4><p>第二个类似于CBOW的架构，但不再基于上下文预测中心词，它试着优化一个基于同一个句子中的其他词的词分类器。更精确地说，我们用每一个当前词作为带连续投影层的输入， 然后预测当前词之前和之后一定范围内的词。我们发现增加范围能够梯度词向量的质量，但也增加了计算复杂度。因为距离当前词远的词不如距离它仅的词更相关，所以在我们的训练集中通过采用更少来给更远的词小的权重。</p><p>该架构的训练复杂度正比于：</p><script type="math/tex; mode=display">Q = C \times (D + D \times \text{log}_2(V) )\tag{5}</script><p>这里, C是词与词的最大距离。如果我们选择C=5, 对每个训练的词我们将随机选取一个在<1, C>的数字，然后用当前词的前R个词和后R个词作为当前词的正确标签。这需要我们对$R \times 2$个单词做分类， 当前词作为分类器的输入，当前词的前R个词和后R个词作为输出。在下面实验中，我们取C=10.</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210306121225.png" alt="image-20210306121224008" style="zoom:25%;" /></p><h3 id="4-结果"><a href="#4-结果" class="headerlink" title="4.结果"></a>4.结果</h3><p>通过加大在词向量维度并在大数据集上训练后，相似词之间的词向量在空间上是相近的(文中 提出small相似词的问题，用$ \vec{X} = \vec{\text{biggest}} - \vec{\text{big}} + \vec{\text{small}} $ 找到smallest,然后用余弦相似度找到跟$\vec{X}$ 相近的词向量，就可以回答这个问题)</p><p>词向量在5类语义和9类句法关系测试如下表：</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210306122126.png" alt="image-20210306122123397" style="zoom:25%;" /></p><h4 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h4><p>为了衡量词向量的质量，作者定义了一个综合测试集，包含五种语义问题和九种句法问题。上图显示了每类中的两个示例。总的来说，有8869个语义问题和10675个句法问题。每个类别中的问题都是通过两个步骤创建的：首先，手动创建类似单词对的列表。然后，通过连接两个词对形成一个大的问题列表。例如，列出了68个美国大城市及其所属的州，并通过随机选择两个词对，形成了大约 2.5k 的问题。测试集中只包含单个标记词，因此不存在多词实体（such as New York）。</p><p>评价模型结果好坏的标准就是上述的词向量线性运算，如果通过线性运算得到的单词与正确的单词是完全一致的，那么就代表该词向量是正确的。所以同义词很难被计算出来，因为没有对同义词的输入，所以模型不可能达到 100% 的准确率。但是该模型的准确率与某些任务是正相关的，所以还是有一些用处的。</p><h4 id="最大化准确度"><a href="#最大化准确度" class="headerlink" title="最大化准确度"></a>最大化准确度</h4><p>使用谷歌新闻语料库训练词向量(6B tokens)， 限制词汇表大小为1百万。下表是CBOW在不同维度的词向量和不同数据量上测试结果。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210306125623.png" alt="image-20210306125620794" style="zoom:25%;" /></p><p>可以看到到达某个点之后，增加词向量维度或增加数据量带来的提升很小。但流行的是在相对大的数据上训练词向量。增大数据量由公式4可知，增大2倍数据，差不多也增大2倍复杂度。</p><p>训练每3轮以初始学习率为0.025线性下降的SGD反向传播，最后一轮为0.</p><h4 id="模型架构比较"><a href="#模型架构比较" class="headerlink" title="模型架构比较"></a>模型架构比较</h4><p>传统模型的比较工作，使用相同的数据集，相同 640 维度的词向量，也不仅限使用 30k 的单词，并且使用了全部的测试集数据。以下是训练结果的比较：</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210306132631.png" alt="image-20210306132629622" style="zoom:30%;" /></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210306133007.png" alt="image-20210306132959540" style="zoom:25%;" /></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210306133119.png" alt="image-20210306133118236" style="zoom:25%;" /></p><h3 id="5-学习到的关系示例"><a href="#5-学习到的关系示例" class="headerlink" title="5. 学习到的关系示例"></a>5. 学习到的关系示例</h3><p>表8中是遵循各种关系的词。通过前面定义的：两个词向量相减再将其加上另一个词向量，例如， Paris - France + Italy = Rome. </p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210306133318.png" alt="image-20210306133317569" style="zoom:25%;" /></p><p>另外提高准确性的方法是提供多个关系的例子，然后将得到的关系向量平均，在本文测试中准确性提升了10%左右。</p><h3 id="6-结论"><a href="#6-结论" class="headerlink" title="6. 结论"></a>6. 结论</h3><p>在本文中，在一系列的语法和语义的语言任务上不同模型的表现，研究了词表示向量的质量。我们观察到用简单模型架构而不是流行的神经网络模型（包括前馈和循环）是可能训练得到高质量的词向量。因为更低的计算复杂度，从大规模数据集上训练得到非常精确的高维词向量是可能得到的。使用<strong>DistBelief</strong> 分布式框架，它能在万亿级别的语料上训练<strong>CBOW</strong>和<strong>Skip-gram</strong>模型。这比比类似模型的先前最佳结果大几个数量级。</p><p>词向量可以成功的应用于知识库中事实的自动扩展，还可以用于验证已有事实的准确性。从机翻实验结果来看也非常有前景。在未来，将我们的技术与潜在关系分析或其它进行比较也很有趣。我们想想我们的全面测试集将帮助研究社区提升估计词向量的现有技术。我们也期望高质量词向量将成为未来NLP应用的关键一环。</p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>[1] <a href="https://hiyoungai.com/posts/fcba888f.html">论文阅读《Efficient Estimation of Word Representations in Vector Space》</a></p><p>[2] <a href="http://littlehaes.com/2018/06/06/word2vec%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91-%E9%83%A8%E5%88%86/">word2vec论文翻译(部分)</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/332572542">2.Embedding系列一(word2vec)</a></p><p>[4] <a href="https://spaces.ac.cn/archives/4299">不可思议的Word2Vec 1.数学原理</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CBOW </tag>
            
            <tag> Skip-gram </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4. 类的继承</title>
      <link href="2020/11/01/4.%E7%B1%BB%E7%9A%84%E7%BB%A7%E6%89%BF/"/>
      <url>2020/11/01/4.%E7%B1%BB%E7%9A%84%E7%BB%A7%E6%89%BF/</url>
      
        <content type="html"><![CDATA[<h2 id="4-类的继承"><a href="#4-类的继承" class="headerlink" title="4. 类的继承"></a>4. 类的继承</h2><h3 id="1-继承-Inheritance"><a href="#1-继承-Inheritance" class="headerlink" title="1. 继承 Inheritance"></a>1. 继承 Inheritance</h3><ul><li>继承是面向对象程序设计的精髓之一</li><li>实现了以类为单位的高抽象级别代码复用</li><li>继承是新定义类能够几乎完全使用原有类属性与方法的过程</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://gitee.com/miller999999/pic/raw/master/img/blog1/image-20201110130106601.png?=raw" width=35% height=35%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">继承</div> </center><h3 id="2-子类、父类、超类"><a href="#2-子类、父类、超类" class="headerlink" title="2. 子类、父类、超类"></a>2. 子类、父类、超类</h3><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://gitee.com/miller999999/pic/raw/master/img/blog1/image-20201108210237235.png?=raw" width=35% height=35%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">子类、父类、超类</div> </center><h3 id="3-类继承的构建"><a href="#3-类继承的构建" class="headerlink" title="3. 类继承的构建"></a>3. 类继承的构建</h3><p>在定义类是声明继承关系</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;(<span class="params">&lt;基类名<span class="number">1</span>&gt;, &lt;基类名<span class="number">2</span>&gt;</span>)：</span></span><br><span class="line"><span class="class"><span class="title">def</span> <span class="title">__init__</span>(<span class="params">self, [参数列表]</span>):</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>基类名可以带路径：<code>ModuleName.BaseClassName</code></p><h3 id="4-类继承的使用"><a href="#4-类继承的使用" class="headerlink" title="4. 类继承的使用"></a>4. 类继承的使用</h3><ul><li>基类的属性基本等同于定义在派生类中</li><li>派生类可以直接使用基类的类属性、实例属性</li><li>派生类可以直接使用基类的各种方法</li><li>使用基类的类方法和类属性时，要用基类的类名调用</li></ul><p><code>isinstance(obj, cls)</code> <code>issubclass(子类名，父类名)</code>判断继承关系的内置函数。</p><p><strong>Python采用深度优先、从左至右的方法实施多继承。</strong></p><p><strong>派生类的约束</strong>：</p><ul><li>派生类只能继承基类的公开属性和方法</li><li>派生类不能继承基类的私有属性和私有方法</li></ul><p><code>object</code>类是<code>Python</code>所有类的基类</p><ul><li>object是python最基础类的名字，不要用中文去理解</li><li>所有类定义时默认继承object类</li><li>保留属性和保留方法本质上是object类的属性和方法</li></ul><h3 id="5-Python对象的三个要素"><a href="#5-Python对象的三个要素" class="headerlink" title="5. Python对象的三个要素"></a>5. Python对象的三个要素</h3><ul><li>标识identity：对象一旦构建不会改变，用<code>id()</code>获取</li><li>type：对象的类型，用<code>type()</code>获取</li><li>值value:分为可变mutab与不可变immmutable</li></ul><p><code>id(x)</code>， <code>x is y</code>的使用。</p><h3 id="6-类的属性的重载"><a href="#6-类的属性的重载" class="headerlink" title="6. 类的属性的重载"></a>6. 类的属性的重载</h3><ul><li>属性重载：派生类定义并使用了与基类相同名称的属性</li><li>方法重载：派生类定义并使用了与基类相同名称的方法</li></ul><p><strong>原则</strong>：</p><ul><li>步骤1：优先使用派生类重定义的属性和方法</li><li>步骤2：然后寻找基类的属性和方法</li><li>步骤3：在寻找超类的属性和方法</li></ul><h3 id="7-类的方法重载"><a href="#7-类的方法重载" class="headerlink" title="7. 类的方法重载"></a>7. 类的方法重载</h3><ul><li><p>完全重载：派生类完全重定义与基类名称的方法，直接在派生类中定义同名方法即可</p></li><li><p>增量重载：派生类拓展定义与基类相同名称的方法</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;(<span class="params">&lt;基类名<span class="number">1</span>&gt;, &lt;基类名<span class="number">2</span>&gt;</span>)：</span></span><br><span class="line"><span class="class"><span class="title">def</span> <span class="title">__init__</span>(<span class="params">self, [参数列表]</span>):</span></span><br><span class="line"><span class="built_in">super</span>().&lt;基类方法名&gt;([参数列表])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python面向对象 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> OOP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1. 面向对象</title>
      <link href="2020/10/29/1.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"/>
      <url>2020/10/29/1.%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/</url>
      
        <content type="html"><![CDATA[<h2 id="1-面向对象"><a href="#1-面向对象" class="headerlink" title="1. 面向对象"></a>1. 面向对象</h2><h3 id="1-面向对象编程万物皆对象"><a href="#1-面向对象编程万物皆对象" class="headerlink" title="1. 面向对象编程万物皆对象"></a>1. 面向对象编程万物皆对象</h3><h4 id="1-对象？"><a href="#1-对象？" class="headerlink" title="1. 对象？"></a>1. 对象？</h4><p><strong>对象</strong>：独立的存在 或 作为目标的事物</p><ol><li>独立性：对象都存在清晰的边界，重点在于划分边界</li><li>功能性：对象都能表现出一些功能、操作或行为</li><li>交互性：对象之间存在交换，如：运算和继承</li></ol><h4 id="2-Python语言的“万物皆对象”"><a href="#2-Python语言的“万物皆对象”" class="headerlink" title="2. Python语言的“万物皆对象”"></a>2. Python语言的“万物皆对象”</h4><ul><li>Python语言中所有的数据类型都行对象、函数是对象、模块是对象</li><li>Python所有类都继承与最基础类object</li><li>Python语言中数据类型的操作功能都是类方法的体现</li></ul><h4 id="3-OOP：-Object-Oriented-Programming"><a href="#3-OOP：-Object-Oriented-Programming" class="headerlink" title="3. OOP： Object-Oriented Programming"></a>3. OOP： Object-Oriented Programming</h4><ul><li>OOP: 面向对象编程，一种编程思想，重点在于高抽象的复用代码</li><li>OOP把对象当做程序的基本单元，对象包含数据和操作数据的函数</li><li>OOP本质是把问题解决， 抽象为以对象为中心的计算机程序</li><li>OOP在较大规模或复杂项目中十分有用，OOP可以提高协作产量</li><li>OOP最主要价值在于代码复用</li><li>OOP只是一种编程方式，并非解决问题的高级方法</li></ul><h4 id="4-面向对象-VS-面向过程"><a href="#4-面向对象-VS-面向过程" class="headerlink" title="4. 面向对象 VS 面向过程"></a>4. 面向对象 VS 面向过程</h4><ul><li>面向过程： 以<strong>解决问题的过程步骤</strong>为核心编写程序的方式</li><li>面向对象： 以<strong>问题对象构建和应用</strong>为核心的编程程序的方式</li><li>所有OOP能解决的问题，面向过程都能解决</li></ul><h4 id="5-OOP三个特征"><a href="#5-OOP三个特征" class="headerlink" title="5. OOP三个特征"></a>5. OOP三个特征</h4><ul><li>封装： 属性和方法的抽象，用数据和操作数据的方法来形成对象逻辑</li><li>继承：代码复用的高级抽象，用对象之间的继承关系来形成代码复用</li><li>多态： 方法灵活性的抽象，让对象的操作更加灵活，更多复用代码</li></ul><h4 id="6-封装Encapsulation"><a href="#6-封装Encapsulation" class="headerlink" title="6. 封装Encapsulation"></a>6. 封装Encapsulation</h4><ul><li>属性的抽象： 对象的属性(变量)进行定义、隔离及保护</li><li>方法的抽象： 对类的方法(函数)进行定义、隔离及保护</li><li>目标是形成一个类对外可操作属性和方法的接口</li></ul><h4 id="7-继承-Inheritance"><a href="#7-继承-Inheritance" class="headerlink" title="7. 继承 Inheritance"></a>7. 继承 Inheritance</h4><ul><li>继承是面向对象程序设计的精髓之一</li><li>定义了以类为单位的高级抽象级别代码复用</li><li>继承是新定义类能够几乎完全使用原有类属性与方法的过程</li></ul><h4 id="8-多态-Polymorphism"><a href="#8-多态-Polymorphism" class="headerlink" title="8. 多态 Polymorphism"></a>8. 多态 Polymorphism</h4><ul><li><p>参数类型的多态：一个方法能够处理多个类型的能力</p></li><li><p>参数形式的多态： 一个方法能够接受多个参数的能力</p></li><li><p>多态是OOP的一个传统概念，Python天然支持多态，不需要特殊语法</p></li></ul><h3 id="2-Python-面向对象术语概述"><a href="#2-Python-面向对象术语概述" class="headerlink" title="2. Python 面向对象术语概述"></a>2. Python 面向对象术语概述</h3><h4 id="1-类-class-和-对象-Object"><a href="#1-类-class-和-对象-Object" class="headerlink" title="1. 类 class 和 对象 Object"></a>1. 类 class 和 对象 Object</h4><ul><li>类： 逻辑抽象和产生对象的模板，一组变量和函数的特定编排</li><li>对象： 具体表达数据及操作的实体，相当于程序中的“变量“</li><li>实例化：从类到对象的过程，所有“对象”都源于某个“类”</li></ul><h4 id="2-常用术语"><a href="#2-常用术语" class="headerlink" title="2. 常用术语"></a>2. 常用术语</h4><ul><li><p>对象： 类对象、实例对象</p></li><li><p>属性： 存储数据的“变量”， 包括：类属性、实例属性</p></li><li><p>方法：操作数据的“函数”</p><p>包括：</p><ul><li>类方法 <code>__classmethod__</code></li><li>实例方法</li><li>自由方法</li><li>静态方法 <code>__staticmethod__</code></li><li>保留方法</li></ul></li><li><p>三个特性： 封装、继承、多态</p></li><li><p>继承： 基类、派生类、子类、父类、超类</p></li><li><p>命名空间： 程序元素作用域的表达</p></li><li><p>构造和析构：生成对象和删除对象的过程</p></li></ul><p><strong>类对象 vs 实例对象</strong></p><ul><li><p>类对象：<code>Class Object</code>,维护每个Python类基本信息的数据结构</p></li><li><p>实例对象： Instance Object, Python类实例后产生的对象，简称：对象。</p></li><li><p>这是一组概念，类对象全局只有一个，实例对象可以生产多个。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python面向对象 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> OOP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.Python 类的构建</title>
      <link href="2020/10/29/2.%E7%B1%BB%E7%9A%84%E6%9E%84%E5%BB%BA/"/>
      <url>2020/10/29/2.%E7%B1%BB%E7%9A%84%E6%9E%84%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="2-Python-类的构建"><a href="#2-Python-类的构建" class="headerlink" title="2.Python 类的构建"></a>2.Python 类的构建</h2><h3 id="1-Class"><a href="#1-Class" class="headerlink" title="1. Class"></a>1. Class</h3><h4 id="1-class的定义"><a href="#1-class的定义" class="headerlink" title="1. class的定义"></a>1. class的定义</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名每个单词首字母大写&gt;:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;类描述块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#解释</span></span><br><span class="line">    语句</span><br></pre></td></tr></table></figure><ul><li>​    类描述：在类定义后的首行，以独立字符串形式定义，<code>&lt;类名&gt;.__doc__</code>属性来访问。</li></ul><h4 id="2-初始化方法和self"><a href="#2-初始化方法和self" class="headerlink" title="2. 初始化方法和self"></a>2. 初始化方法和self</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;：</span></span><br><span class="line"><span class="class"><span class="title">def</span> <span class="title">__init__</span>(<span class="params">self, [参数列表]</span>)：</span></span><br><span class="line"><span class="class">语句块</span></span><br></pre></td></tr></table></figure><p>类实例化时所用的方法，可以接收参数并完成初识操作。</p><p><code>__init__</code>:</p><ul><li>参数：第一个参数是<code>self</code>，表示类<strong>实例自身</strong>，<strong>其它参数是实例参数</strong>。</li><li>返回值：构造方法没有返回值，或者返回<code>None</code>，不然<code>TypeError</code>.</li></ul><p><code>self</code>: 在类定义内部代表类的实例</p><ul><li><code>self</code>是Python面向对象约定的一个类参数</li><li><code>self</code>代表类的实例，在类内部，<code>self</code>用于组合访问实例相关的属性和方法</li><li>相比，类名代表类对象本身</li></ul><h4 id="3-类的属性和方法"><a href="#3-类的属性和方法" class="headerlink" title="3. 类的属性和方法"></a>3. 类的属性和方法</h4><ul><li>类的属性：类中定义的变量，用来描述类的一些特性参数</li><li>类的方法：类中定义且与类相关的函数，用来给出类的操作功能</li><li>属性和方法是类对外交互所提供的两种接口方式</li></ul><h5 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h5><ul><li><p>类属性：类对象的属性，由所有实例对象所共享</p></li><li><p>实例属性：实例对象的属性，由各实例对象所共享</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;：</span></span><br><span class="line">&lt;类属性名&gt; = &lt;类属性初值&gt;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, [参数列表]</span>):</span></span><br><span class="line">self.&lt;实例属性名&gt; = &lt;实例属性初值&gt;</span><br></pre></td></tr></table></figure><p><strong>类属性</strong>实例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PeopleClass</span>:</span></span><br><span class="line">    init_num = <span class="number">0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name, age</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.age = age</span><br><span class="line">        PeopleClass.init_num += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">p1 = PeopleClass(<span class="string">&quot;Tom&quot;</span>, <span class="number">23</span>)</span><br><span class="line">p2 = PeopleClass(<span class="string">&quot;Jack&quot;</span>, <span class="number">24</span>)</span><br><span class="line">print(<span class="string">&quot;类总数:&quot;</span>, PeopleClass.init_num)</span><br><span class="line">print(p1.name, p2.name)</span><br><span class="line"></span><br><span class="line">&gt;&gt;</span><br><span class="line">类总数: <span class="number">2</span></span><br><span class="line">Tom Jack</span><br></pre></td></tr></table></figure><p>每次实例化，都会调用<code>__init__</code>方法，那么累变量改变为2，因此类属性也为2。</p><p><strong>属性是类内部定义的变量</strong></p><ul><li>类属性：类对象的属性，有所有实例对象所共享<ul><li>访问：&lt;类名&gt;.&lt;类属性&gt; 或 &lt;对象名&gt;.&lt;类属性&gt; </li></ul></li><li>实例属性：实例对象的属性，由各实例对象所独享<ul><li>访问：&lt;对象名&gt;.&lt;实例属性&gt; </li></ul></li></ul></li></ul><h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><p><strong>方法</strong>是类内部定义的函数</p><ul><li>实例方法：实例对象的方法，由各实例对象独享，最常用的形式</li><li>类方法： <strong>类对象</strong>的方法，由<strong>所有实例对象</strong>共享</li><li>自由方法：类中的一个普通函数，由类所在命名空间管理，<strong>类对象独享</strong></li><li>静态方法：类中的一个普通函数，由<strong>类对象和实例对象共享</strong></li><li>保留方法： 由双下划线开始和结束的方法，保留使用,如<code>__len__()</code></li></ul><p><strong>实例方法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;：</span></span><br><span class="line"><span class="class">    <span class="title">def</span> &lt;方法名&gt;(<span class="params">self, [参数列表]</span>)：</span></span><br><span class="line"><span class="class">        语句</span></span><br><span class="line"><span class="class"></span></span><br></pre></td></tr></table></figure><p>​    实例方法采用&lt;对象名&gt;.&lt;方法名&gt;([参数列表])方式使用</p><p><strong>类方法</strong></p><p>l    类方法是与类对象相关的函数，由所有实例对象共享</p><p>​        </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;:</span></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> &lt;方法名&gt;(<span class="params">cls, [参数列表]</span>):</span></span><br></pre></td></tr></table></figure><p>类方法采用&lt;类名&gt;.&lt;方法名&gt;([参数列表]) 或&lt;对象名&gt;.&lt;方法名&gt;([参数列表])方式使用。</p><ul><li>类方法至少包含一个参数，表示类对象，一般用<code>cls</code></li><li><code>@classmethod</code> 是装饰器，类方法定义所必须</li><li>类方法只能操作类属性和其他类方法，不能操作实例属性和实例方法</li></ul><p><a href="https://www.programiz.com/python-programming/methods/built-in/classmethod">类方法的使用</a></p><p><strong>自由方法</strong></p><p>自由方法是定义在类命名空间中的普通函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> &lt;方法名&gt;(<span class="params">cls, [参数列表]</span>):</span></span><br><span class="line">        语句</span><br></pre></td></tr></table></figure><p>自由方法采用&lt;类名&gt;.&lt;方法名&gt;([参数列表]) 方式使用。&lt;类名&gt;表示命名空间。</p><ul><li>自由方法不需要<code>self</code>、<code>cls</code>这类参数，可以没有参数</li><li>自由方法只能操作类属性和类方法，不能操作实例属性和实例方法</li><li>自由方法的使用只能使用&lt;类名&gt;</li></ul><p><strong>静态方法</strong></p><p>静态方法是定义在类中的普通函数，能够被所有实例对象共享</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;:</span></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> &lt;方法名&gt;(<span class="params"> [参数列表]</span>):</span></span><br><span class="line">        语句</span><br></pre></td></tr></table></figure><p>静态方法采用&lt;类名&gt;.&lt;方法名&gt;([参数列表]) 或&lt;对象名&gt;.&lt;方法名&gt;([参数列表])方式使用。</p><ul><li>静态方法可以没有参数，可以理解为定义在类中的普通函数</li><li>@staticmethod是装饰器，静态方法定义所必须</li><li>静态方法只能操作类属性和其他类方法， 不能操作实例属性和实例方法</li><li>相比自由方法，静态方法能够使用<strong>类名</strong>和<strong>对象名</strong>两种操作方式使用</li></ul><h4 id="4-类的析构函数"><a href="#4-类的析构函数" class="headerlink" title="4. 类的析构函数"></a>4. 类的析构函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__del__</span>(<span class="params">self</span>):</span></span><br><span class="line">        语句</span><br></pre></td></tr></table></figure><p>析构函数在<strong>真实</strong>删除实例对象是被调用。</p><ul><li>函数名和参数：Python解释器内部约定，保留方法</li><li>调用条件：当实例对象被真实删除是，才调用该函数内语句</li><li>真实删除：当前对象的引用次数为0或当前程序退出(垃圾回收机制)</li></ul><p><strong>Python类的内存管理</strong></p><ul><li>实例对象：真实创建的对象，分配对象内存</li><li>实例引用：实例的指针，仅分配指针内存</li></ul><p>内存管理机制：</p><ul><li><p>在删除对象前，Python解释器会检查引用次数</p></li><li><p>引用次数不为0，则仅删除当前引用；0，则删除对象</p></li><li><p>如果程序退出，则由垃圾回收机制删除对象</p><p>利用<code>sys.getrefcount(&lt;对象名&gt;)</code>获取对象引用次数，其返回值为被引用值+1.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python面向对象 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> OOP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3. 类的封装</title>
      <link href="2020/10/29/3.%E7%B1%BB%E7%9A%84%E5%B0%81%E8%A3%85/"/>
      <url>2020/10/29/3.%E7%B1%BB%E7%9A%84%E5%B0%81%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h2 id="3-类的封装"><a href="#3-类的封装" class="headerlink" title="3. 类的封装"></a>3. 类的封装</h2><h3 id="1-封装Encapsulation"><a href="#1-封装Encapsulation" class="headerlink" title="1. 封装Encapsulation"></a>1. 封装Encapsulation</h3><p>封装是属性和方法的抽象。</p><ul><li>属性的抽象：对类的属性(变量)进行定义、隔离和包护</li><li>方法的抽象：对类的方法(函数)进行定义、隔离和包护</li><li>目标是形成一个类对外可操作属性和方法的接口</li></ul><pre class="mermaid">graph LRA((类CLass))-->  B(属性 Attributes)A --> C(方法 Methods)</pre><p><strong>属性：</strong></p><ul><li>私有属性：只能在类内部访问</li><li>公有属性：可以通过类、对象名访问</li></ul><p><strong>方法：</strong></p><ul><li>公有方法：只能在类内部使用</li><li>公开方法：可以通过类、对象名访问</li></ul><h3 id="2-私有类属性和公开类属性"><a href="#2-私有类属性和公开类属性" class="headerlink" title="2. 私有类属性和公开类属性"></a>2. 私有类属性和公开类属性</h3><p>私有属性：仅供当前类访问的类属性，子类也不能访问。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;：</span></span><br><span class="line">&lt;__私有类属性名&gt; = &lt;类属性初值&gt;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, [参数列表]</span>):</span></span><br></pre></td></tr></table></figure><p>私有类属性名开始需要有两个下划线<code>__</code>,如 <code>__count</code>。</p><ul><li>只能在类的内部被方法所访问</li><li>不能通过&lt;类名&gt;.&lt;属性名&gt; 或 &lt;对象名&gt;.&lt;属性名&gt;方式访问</li><li>有效保证了属性维护的可控性</li></ul><p>公开类属性：即类属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;：</span></span><br><span class="line">&lt;类属性名&gt; = &lt;类属性初值&gt;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, [参数列表]</span>):</span></span><br><span class="line">self.&lt;实例属性名&gt; = &lt;实例属性初值&gt;</span><br></pre></td></tr></table></figure><h3 id="3-公开实例属性和私有实例属性"><a href="#3-公开实例属性和私有实例属性" class="headerlink" title="3. 公开实例属性和私有实例属性"></a>3. 公开实例属性和私有实例属性</h3><p><strong>公开实例属性：</strong>即实例属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;：</span></span><br><span class="line">&lt;类属性名&gt; = &lt;类属性初值&gt;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, [参数列表]</span>):</span></span><br><span class="line">self.&lt;实例属性名&gt; = &lt;实例属性初值&gt;</span><br></pre></td></tr></table></figure><p><strong>私有实例属性：</strong>仅供当前类内部访问的实例属性，子类也不能访问</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;：</span></span><br><span class="line"><span class="class"><span class="title">def</span> <span class="title">__init__</span>(<span class="params">self, [参数列表]</span>):</span></span><br><span class="line">self.&lt;__实例属性名&gt; = &lt;实例属性初值&gt;</span><br></pre></td></tr></table></figure><p>私有实例属性名开始需要有两个下划线<code>__</code>,如<code>__name</code>.</p><ul><li>只能在类的内部被方法所访问</li><li>不能通过&lt;类名&gt;.&lt;属性名&gt; 或 &lt;对象名&gt;.&lt;属性名&gt;方式访问</li><li>有效保证了属性维护的可控性</li></ul><h3 id="4-私有方法和公开方法"><a href="#4-私有方法和公开方法" class="headerlink" title="4.私有方法和公开方法"></a>4.私有方法和公开方法</h3><p><strong>私有方法</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> &lt;类名&gt;：</span></span><br><span class="line"><span class="class"><span class="title">def</span> &lt;<span class="title">__</span>方法名&gt;(<span class="params">self, [参数列表]</span>):</span></span><br></pre></td></tr></table></figure><p>私有方法名开始要有2个下划线<code>__</code>,如<code>__getCount()</code>.</p><ul><li>各类方法都可以通过增加双下划线变为私有方法</li><li>私有方法从形式上保护了Python类内部使用的函数逻辑</li><li>私有与公有是程序员逻辑，不是安全逻辑，重视约定</li></ul><h3 id="5-类的保留属性和保留方法"><a href="#5-类的保留属性和保留方法" class="headerlink" title="5. 类的保留属性和保留方法"></a>5. 类的保留属性和保留方法</h3><p><strong>保留属性</strong>：也叫特殊属性，Special Attributes。</p><ul><li>特点：以双下划线开头和结尾。</li><li>作用：为理解Python类提供统一的属性接口</li><li>属性值：具有特定含义，类定义后直接使用</li></ul><div class="table-container"><table><thead><tr><th>保留属性</th><th>描述</th></tr></thead><tbody><tr><td><code>__name__</code></td><td>类的名称</td></tr><tr><td><code>__qualname__</code></td><td>以.分隔从模块全局命名空间开始的类名称</td></tr><tr><td><code>__bases__</code></td><td>类所继承的基类名称</td></tr><tr><td><code>&lt;类&gt;.__dict__</code></td><td>包含类成员信息的字典，key是属性和方法名称，value是地址</td></tr><tr><td><code>&lt;对象&gt;.__dict__</code></td><td>包含对象属性信息的字典，key是属性名称，value是值</td></tr><tr><td><code>__class__</code></td><td>对象所对应的类信息，即type信息</td></tr><tr><td><code>__doc__</code></td><td>类描述，写在类定义下的首行字符串，不能继承</td></tr><tr><td><code>__module__</code></td><td>类所在模块的名称</td></tr></tbody></table></div><p><strong>保留方法：</strong></p><p>也叫特殊方法，Special Methods</p><ul><li>特点：以双下划线开头和结尾。</li><li>作用：为操作Python类提供统一的属性接口</li><li>方法逻辑：具有特定含义，一般与操作符关联，类定义需要重载。</li></ul><div class="table-container"><table><thead><tr><th>保留方法</th><th><code>对应操作</code></th><th><code>描述</code></th></tr></thead><tbody><tr><td><code>obj.__init__()</code></td><td><code>obj=ClasssName()</code></td><td>初始化实例对象的函数逻辑</td></tr><tr><td><code>obj.__del__()</code></td><td><code>del obj</code></td><td>删除实例对象</td></tr><tr><td><code>obj.__repr__()</code></td><td><code>repr(obj)</code></td><td>定义对象可打印字符串的函数逻辑</td></tr><tr><td><code>obj.__str__()</code></td><td><code>str(obj)</code></td><td>定义对象字符串转换操作的逻辑函数</td></tr><tr><td><code>obj.__bytes__()</code></td><td><code>bytes(obj)</code></td><td>定义对象字节串转换操作的函数逻辑</td></tr><tr><td><code>obj.__format__()</code></td><td><code>obj.format()</code></td><td>定义对象格式化输出的函数逻辑</td></tr><tr><td><code>obj.__hash_()</code></td><td><code>hash(obj)</code></td><td>定义对象哈希操作的函数逻辑</td></tr><tr><td><code>obj.__bool__()</code></td><td><code>bool(obj)</code></td><td>定义对象布尔运算的函数逻辑</td></tr><tr><td><code>obj.__len__()</code></td><td><code>len(obj)</code></td><td>定义对象长度操作的函数逻辑</td></tr><tr><td><code>obj.__reversed__()</code></td><td><code>obj.reversed()</code></td><td>定义对象逆序的函数逻辑</td></tr><tr><td><code>obj.__abs__()</code></td><td><code>abs(obj)</code></td><td>定义对象绝对值操作的函数逻辑</td></tr><tr><td><code>obj.__int__()</code></td><td><code>int(obj)</code></td><td>定义对象整数转换的函数逻辑</td></tr></tbody></table></div><p><code>dir(obj)</code>查看对象所有的属性和方法。</p><p><a href="https://diveintopython3.net/special-method-names.html#acts-like-function">特殊方法</a></p>]]></content>
      
      
      <categories>
          
          <category> Python面向对象 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> OOP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 11  PageRank and Ridge Regression</title>
      <link href="2020/10/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture11/"/>
      <url>2020/10/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture11/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-11-PageRank-and-Ridge-Regression"><a href="#Lecture-11-PageRank-and-Ridge-Regression" class="headerlink" title="Lecture 11: PageRank and Ridge Regression"></a>Lecture 11: PageRank and Ridge Regression</h2><h3 id="1-PageRank"><a href="#1-PageRank" class="headerlink" title="1. PageRank"></a>1. PageRank</h3><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog1/image-20201106225438566.png" alt="PageRank" style="zoom:150%;" /></p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://gitee.com/miller999999/pic/raw/master/img/image-20201103131844700.png?=raw" width=35% height=35%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Google PageRank</div> </center><p><a href="http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf">Google原始论文</a></p><ul><li>Imagine surfing web by randomly clicking links. This is called a “random walk” over graph. If you do this long enough eventually reach “steady state” where Probability that you are at site <script type="math/tex">i = \pi_i</script></li></ul><p>让$\tilde A$ 表示这个图的邻近矩阵，其中：</p><script type="math/tex; mode=display">\tilde A = \begin{cases}1, & \text{if   links   from  j  to  i},\\0, & otherwise\end{cases}\tag{1}</script><p>具体地有：</p><script type="math/tex; mode=display">\tilde A =\left[\begin{array}{r} 0 & 1 & 1 & 1 \\   0 & 0 & 1 & 1 \\   1 & 0 &0 & 0 \\   1 & 0 & 1& 0 \\\end{array} \right]\tag{2}</script><p>接下来，让$A = \tilde A $ column-normalized version.</p><script type="math/tex; mode=display">\tilde A =\left[\begin{array}{r} 0 & 1 &  \frac {1}{3} &  \frac {1}{2}\\   0 & 0 &  \frac {1}{3} &  \frac {1}{2} \\   \frac {1}{2} & 0 &0 & 0 \\   \frac {1}{2} & 0 &  \frac {1}{3}& 0 \\\end{array} \right]\tag{3}</script><p>定义：$e_{i} = i^{th}$单位矩阵$I$的第$i$列。</p><p>那么，</p><script type="math/tex; mode=display">Ae_3 =\left[\begin{array}{r} 0 & 1 &  \frac {1}{3} &  \frac {1}{2}\\   0 & 0 &  \frac {1}{3} &  \frac {1}{2} \\   \frac {1}{2} & 0 &0 & 0 \\   \frac {1}{2} & 0 &  \frac {1}{3}& 0 \\\end{array} \right]\left[\begin{array}{r} 0  \\ 0  \\1\\0\end{array} \right]=\left[\begin{array}{r}\frac {1}{3}  \\\frac {1}{3} \\0\\\frac {1}{3}\end{array} \right]\tag{4}</script><script type="math/tex; mode=display">Ae_4 =\left[\begin{array}{r} 0 & 1 &  \frac {1}{3} &  \frac {1}{2}\\   0 & 0 &  \frac {1}{3} &  \frac {1}{2} \\   \frac {1}{2} & 0 &0 & 0 \\   \frac {1}{2} & 0 &  \frac {1}{3}& 0 \\\end{array} \right]\left[\begin{array}{r} 0  \\ 0  \\0\\1\end{array} \right]=\left[\begin{array}{r}\frac {1}{2}  \\\frac {1}{2} \\0\\0\end{array} \right]\tag{5}</script><p>$A_ij= Pro(\text{go  to  site  i  }| \text{  at  site  j})$.我们从$j$到$i$的可能性。</p><p>令：</p><script type="math/tex; mode=display">\underline{\pi}=\left[\begin{array}{r}\pi_{1}  \\\pi_{2}  \\\pi_{3} \\\pi_{4} \end{array} \right]\tag{6}</script><p>$\underline{\pi} $是到下一个网站的可能性，而且有：</p><script type="math/tex; mode=display">A\underline{\pi} =\underline{\pi}</script><p>目标是找到 <script type="math/tex">\underline{\pi}</script> 。</p><script type="math/tex; mode=display">\begin{align}&\underline{\pi}  是 \  A  \ 的特征向量， 令 A的特征向量\  \boldsymbol{v}_k \ 且\ \lVert  \boldsymbol{v}_k\rVert_{2}=1\  。如果\ A\boldsymbol{v}_k = \lambda_k \boldsymbol{v}_{k} ,   \\&对一些标量\ \lambda_k\ 成立。其中\ \lambda_k \ 叫作特征值。 \end{align}</script><p>令 <script type="math/tex">V = [v_1, v_2, \cdots, v_n]</script>是<script type="math/tex">A</script>的特征向量。</p><script type="math/tex; mode=display">AV=A[v_1, v_2, \cdots, v_n]=[\lambda_1v_1, \lambda_2v_2, \cdots, \lambda_nv_n]\\= V\begin{bmatrix}    \lambda_1 & 0 & \cdots & 0 \\    0 & \lambda_2 & \cdots & 0 \\    \vdots & \vdots & \ddots & \vdots \\    0 & 0 & \cdots & \lambda_n \end{bmatrix}=V\Lambda</script><p>对于：</p><script type="math/tex; mode=display">X \in \mathbb{R}^{n \times p}, A = X^TX \in \mathbb{p \times p}.若 X= U \Sigma V^T,\ \ 那么 A = U \Sigma V^TU\Sigma V^T \Rightarrow A = V\Sigma^2 V^T \Rightarrow AV=V\Sigma^2</script><p>$X$的$\sigma$的平方就是$A$的特征值。$X$右奇异向量就是$A$的特征向量。</p><p>PageRank 矩阵$A$有$\lambda_1=1 &gt; \lambda_2 \ge\lambda_3 \ge \cdots\ge \lambda_n$.</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog1/image-20201106225528025.png" alt="算法流程" style="zoom:125%;" /></p><p><strong>算法流程</strong></p><script type="math/tex; mode=display">Let \ \ \underline{\Pi}^{0} \ \ be \  initial \ guess \  of \ \underline{\Pi} \ \ for \ k=1, 2, \cdots \\\underline{\Pi}^{k} = \frac {A\underline{\Pi}^{(k-1)}}{\lVert  A\underline{\Pi}^{(k-1)}\rVert_{2}}</script><script type="math/tex; mode=display">if \ \lVert \underline{\Pi}^{(k)} - \underline{\Pi}^{(k-1)}\rVert_{2}< \epsilon, \ \ then \ stop</script><p>又有：</p><script type="math/tex; mode=display">\begin{aligned}\underline{\Pi}^{1} &\propto A \ \underline{\Pi}^{0} \\\underline{\Pi}^{2}& \propto A \ \underline{\Pi}^{1} \propto A^2 \ \underline{\Pi}^{0}\\\vdots &\\\underline{\Pi}^{k}  &\propto A^k \ \underline{\Pi}^{0}\\\end{aligned}\tag{7}</script><p>和</p><script type="math/tex; mode=display">\begin{aligned}A &= V\Lambda V^T\\A^2 &= V\Lambda V^T V\Lambda V^T = V\Lambda\Lambda V^T\\\vdots &\\A^k &= V\Lambda^{k} V^T\end{aligned}\tag{8}</script><p>接下来证明算法为什么会有效？</p><p>假定特征向量是<strong>orthonormal</strong>($V^TV=VV^T=I$)。</p><p>相应地：</p><script type="math/tex; mode=display">\underline{\Pi}^{O} = c_1v_1 + c2v_2 +\cdots+c_nv_n \ \text{for} \ c_1, c_2, \cdots,c_n.assume c_1 \ne 0.\tag{9}</script><p>其中，$v_1就等于\underline{\Pi}$.那么，</p><script type="math/tex; mode=display">\underline{\Pi}^{k} \propto A^k \underline{\Pi}^{0}=V\Lambda^k V^T( c_1v_1 + c2v_2 +\cdots+c_nv_n)\tag{10}</script><p>而</p><script type="math/tex; mode=display">V^Tv_i =\left[\begin{array}{ll}- &\boldsymbol{v}_{1}^{\top}&- \\- & \boldsymbol{v}_{2}^{\top}&- \\- &    \vdots &-        \\- & \boldsymbol{v}^T_{n}&-\end{array}\right]v_i=\boldsymbol{e}_{i}, Ve_i=v_i\tag{11}</script><p>那么式10等于：</p><script type="math/tex; mode=display">\underline{\Pi}^{k} \propto A^k \underline{\Pi}^{0} = V(\lambda^kc_1e_1+ \cdots + \lambda^nc_ne_n)=\lambda^k_1c_1(v_1 + \frac{\lambda^k_2c_2}{\lambda_1^kc_1}v_2+\cdots+ \frac{\lambda^k_nc_n}{\lambda_1^kc_1}v_n)\tag{12}</script><p>当$k \to \infty$,$\frac{\lambda^k_i}{\lambda_1^k} \to 0 \ for \ i \ne 1$，那么</p><script type="math/tex; mode=display">当 k \to \infty, \lambda^k_1c_1(v_1 + \frac{\lambda^k_2c_2}{\lambda_1^kc_1}v_2+\cdots+ \frac{\lambda^k_nc_n}{\lambda_1^kc_1}v_n) \to \lambda^k_1c_1v_1\tag{13}</script><p>那么式12等于</p><script type="math/tex; mode=display">\frac{\lambda^k_1c_1v_1}{|\lambda^k_1c_1|\lVert \boldsymbol{v}_1\rVert} \to \boldsymbol{v}_1 = \Pi \tag{14}</script><p><a href="https://zh.wikipedia.org/wiki/PageRank">Wiki证明版本</a></p><h3 id="Classical-Ridge-Regression"><a href="#Classical-Ridge-Regression" class="headerlink" title="Classical Ridge Regression"></a>Classical Ridge Regression</h3><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog1/image-20201106225617878.png" alt="Classical Ridge Regression 1" style="zoom:125%;" /></p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog1/image-20201106225713387.png" alt="Classical Ridge Regression 2" style="zoom:125%;" /></p>]]></content>
      
      
      <categories>
          
          <category> CMSC35300 Mathematics Foundations of ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> PageRank </tag>
            
            <tag> Ridge Regression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 10 More on the SVD in Machine Learning</title>
      <link href="2020/10/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture10/"/>
      <url>2020/10/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture10/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-10-More-on-the-SVD-in-Machine-Learning-including-matrix-completion"><a href="#Lecture-10-More-on-the-SVD-in-Machine-Learning-including-matrix-completion" class="headerlink" title="Lecture 10: More on the SVD in Machine Learning including matrix completion"></a>Lecture 10: More on the SVD in Machine Learning including matrix completion</h2><h3 id="1-SVD-in-machine-learning"><a href="#1-SVD-in-machine-learning" class="headerlink" title="1. SVD in machine learning"></a>1. SVD in machine learning</h3><ol><li>Dimensionality Reduction(PCA)</li><li>Principal Components Regression</li><li>Least Squares </li><li>Matrix Completion</li><li>PageRank</li></ol><h3 id="2-Least-Squares-amp-SVD"><a href="#2-Least-Squares-amp-SVD" class="headerlink" title="2. Least Squares &amp; SVD"></a>2. Least Squares &amp; SVD</h3><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog1/image-20201106225042979.png" alt="理解最小二乘" style="zoom:100%;" /></p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog1/image-20201106225211804.png" alt="理解最小二乘续" style="zoom:100%;" /></p><p>跟前面提到的一样有：</p><p>假设$n \ge p$ &amp; $X$有$p$个线性无关的列，那么</p><script type="math/tex; mode=display">\hat w = (X^TX)^{-1}X^Ty \tag{1}</script><h4 id="情形1："><a href="#情形1：" class="headerlink" title="情形1："></a>情形1：</h4><p>如果$X= U \Sigma V^T$,其中$U : n\times n ,\ \ \Sigma: n \times p ,\ \ V^T: p \times p$, 那么</p><script type="math/tex; mode=display">(X^TX)^{-1}X^T = (V \Sigma^T U^T U \Sigma V^T)^{-1}V\Sigma^TU^T=(V \Sigma^T \Sigma V^T)^{-1}V\Sigma^TU^T\tag{2}</script><p>由下面两式,</p><script type="math/tex; mode=display">VV^T = V^TV= I \Rightarrow V^{-1}=V^T\\ (AB)^{-1}=B^{-1}A^{-1}</script><p>可得:</p><script type="math/tex; mode=display">(AV^T)^{-1} = (V^T)^{-1}A^{-1}=VA^{-1}\tag{3}</script><script type="math/tex; mode=display">(VB)^{-1}=B^{-1}V^{T} \tag{4}</script><script type="math/tex; mode=display">(VAV^T)^{-1}= VA^{-1}V^T \tag{5}</script><p>继续化简式2，[利用式5，把  <script type="math/tex">\Sigma^T \Sigma</script>   看作  <script type="math/tex">A</script> ]</p><script type="math/tex; mode=display">(X^TX)^{-1}X^T = V(\Sigma^T \Sigma)^{-1}V^TV\Sigma^TU^T=V(\Sigma^T \Sigma)^{-1}\Sigma^TU^T\tag{6}</script><p>pseudo-inverse:</p><script type="math/tex; mode=display">\Sigma^+ = (\Sigma^T \Sigma)^{-1}\Sigma^T \tag{7}</script><p>并且有上节课里提到的：</p><script type="math/tex; mode=display">\underbrace{ (\Sigma^T \Sigma)^{-1}\Sigma^T}_{p \times n}=\left[\begin{array}{cccc|c}    1/\sigma^{2}_1 & 0 & \cdots & 0  &\\    0 & 1/\sigma^{2}_2 & \cdots & 0 \\    \vdots & \vdots & \ddots & \vdots \\    0 & 0 & \cdots & 1/\sigma^{2}_p \end{array}\right]=\Sigma^+</script><p>由式7得：</p><script type="math/tex; mode=display">(X^TX)^{-1}X^T =V\Sigma^+U^T\tag{8}</script><p>跟上节课一样：</p><script type="math/tex; mode=display">\hat w \approx V\Sigma^+U^Ty\tag{9}</script><h4 id="情形2："><a href="#情形2：" class="headerlink" title="情形2："></a>情形2：</h4><p>$p \ge n$, $X$ 有$n$个独立的行，</p><p>那么有无数解[       无穷 个   <script type="math/tex">w</script>   满足   <script type="math/tex">\hat y = Xw</script>  ].</p><p><strong>Claim</strong>: $\hat w = V \Sigma^+ U^T$ has the smallest norm of any w $s.t: \ y = Xw$。我们从上面的$w$中挑出一个$\hat w$来使其有最小的范数，并且满足约束。</p><script type="math/tex; mode=display">V\Sigma^+U^T = X^T (X^TX)^{-1}\tag{10}</script><p>因为$X$是“瘦长型”矩阵，那么其伪逆为$\Sigma^+= \Sigma^T(\Sigma\Sigma^T)^{-1}$。</p><p>对于任意$w$要有$y = Xw$.我们想要得到   </p><script type="math/tex; mode=display">\lVert  w \rVert^2_{2} \ge \lVert  \hat w \rVert^2_{2}</script><p>而，</p><script type="math/tex; mode=display">\lVert  w \rVert^2_{2} = \lVert  w - \hat w + \hat w \rVert^2_{2}\tag{11}</script><p>回忆一下，</p><script type="math/tex; mode=display">\lVert  a +b \rVert^2_{2} = (a+b)^T(a+b)=a^Ta-2a^Tb+b^Tb=\lVert a\rVert^2_{2}-2a^Tb+\lVert  b \rVert^2_{2}\tag{12}</script><p>式11,变为：</p><script type="math/tex; mode=display">\lVert  w - \hat w + \hat w \rVert^2_{2} = \lVert  w - \hat w \rVert^2_{2} -2(w-\hat w)^T\hat w + \lVert  \hat w \rVert^2_{2} \tag{13}</script><p>我们现在只看中间项，它等价于, 利用 <script type="math/tex">\hat w=X^T(XX^T)^{-1}y</script> 跟伪逆一样理由写成这样化简下 ：</p><script type="math/tex; mode=display">(w-\hat w)^T\hat w =(w-\hat w)^TX^T(XX^T)^{-1}y=\underbrace{(X(w-\hat w))^{T}}(XX^T)^{-1}y\tag{14}</script><p>又因为  <script type="math/tex">y = X w = X\hat w \Rightarrow X\hat w -Xw =X(w-\hat w) =\boldsymbol{0}</script> 。只能是  <script type="math/tex">(w-\hat w) =\boldsymbol{0}</script> .</p><p>所以式12等于：</p><script type="math/tex; mode=display">\begin{aligned}\lVert  w - \hat w + \hat w \rVert^2_{2} &=  \lVert  w - \hat w \rVert^2_{2} +\lVert  \hat w \rVert^2_{2}\\ &\Rightarrow \lVert w \rVert^2_{2} \ge \underbrace{\lVert  w - \hat w \rVert^2_{2}}_{\ge 0} +\lVert  \hat w \rVert^2_{2}\\ &\Rightarrow \lVert  w \rVert^2_{2} \ge \lVert  \hat w \rVert^2_{2} \end{aligned} \tag{15}</script><h3 id="3-Why-is-minimum-norm-good"><a href="#3-Why-is-minimum-norm-good" class="headerlink" title="3.  Why is minimum norm good?"></a>3.  Why is minimum norm good?</h3><p>一些问题</p><p>Why can’t we set $\hat w = X^{-1}y$?</p><ul><li>$X^{-1}$does not exist。</li></ul><p>When $\hat w = (X^TX)^{-1}X^Ty$ vs $\hat w = X^T(XX^T)^{-1}y$？</p><ul><li>$\hat w = (X^TX)^{-1}X^Ty$ :   $n \ge p, \ X \ has \ p \ LI \ cols. $</li><li>$\hat w = X^T(XX^T)^{-1}y$ :   $p \ge n, \ X \ has \ n \ LI \ rows. $</li></ul><p>In both cases,</p><script type="math/tex; mode=display">\hat w = V\Sigma^+U^Ty \tag{16}</script><p>is good.</p><h3 id="4-Matrix-Completion"><a href="#4-Matrix-Completion" class="headerlink" title="4. Matrix Completion"></a>4. Matrix Completion</h3><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog1/image-20201106225306125.png" alt="Matrix Completionimage-20201106225306125" style="zoom: 100%;" /></p><p>$X \in \mathbb{R}^{n \times p}$,assume $rank(X) = r \leq min(n, p)$, we observe $X_ij \ for (i, j)\in \Omega$</p><p><a href="https://web.stanford.edu/~hastie/TALKS/SVD_hastie.pdf">stanford slides</a></p><h3 id="5-Singular-Value-Thresholding"><a href="#5-Singular-Value-Thresholding" class="headerlink" title="5. Singular Value Thresholding"></a>5. Singular Value Thresholding</h3><p><a href="https://statweb.stanford.edu/~candes/software/svt/">stanford讲义</a></p><p><a href="https://arxiv.org/abs/0810.3286">论文</a></p>]]></content>
      
      
      <categories>
          
          <category> CMSC35300 Mathematics Foundations of ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> SVD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 9 The SVD in Machine Learning</title>
      <link href="2020/10/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture9/"/>
      <url>2020/10/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture9/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-9-The-SVD-in-Machine-Learning"><a href="#Lecture-9-The-SVD-in-Machine-Learning" class="headerlink" title="Lecture 9 The SVD in Machine Learning"></a>Lecture 9 The SVD in Machine Learning</h2><p><img src="https://i.loli.net/2020/10/28/6b5vyHQzVTOha4C.png" alt="降维" style="zoom:125%;" /></p><h3 id="矩阵表示"><a href="#矩阵表示" class="headerlink" title="矩阵表示"></a>矩阵表示</h3><p><img src="https://i.loli.net/2020/10/28/ip5EFuTVCfxMIkP.png" alt="示例" style="zoom:125%;" /></p><script type="math/tex; mode=display">X= \left[\begin{array}{r}  1 & -1 & -1 & 1 \\   -1 & 1 & -1 & 1 \\   1 & -1 & -1 & 1 \\   -1 & 1 & -1 & 1 \\1 & -1 & 0 & 0 \end{array} \right]=  \left[\begin{array}{r}   1 & -1  \\   -1 & 1  \\   1 & -1  \\   -1 & 1 \\  1 & -1  \end{array} \right]\left[\begin{array}{r}   1 & -1 &0&0 \\   0 & 0&-1&1  \\   \end{array} \right]\tag{1}</script><p>其实是利用从列向量乘法来分解的,$X$第一列是由：</p><script type="math/tex; mode=display">X[1]=\left[\begin{array}{r}   1 \\   -1 \\   1  \\   -1 \\  1 \end{array} \right]=1*\left[\begin{array}{r}   1 \\   -1 \\   1  \\   -1 \\  1 \end{array} \right] + 0*\left[\begin{array}{r}   -1 \\   1 \\   -1  \\   1 \\  -1 \end{array} \right]</script><p>这不是SVD，不是奇异值。</p><p>$X$的秩是2，列向量、行向量最大为2.</p><p>上面接下来才是SVD。</p><p>Python numpy SVD结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.mat([[<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>], </span><br><span class="line">       [-<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>], </span><br><span class="line">       [<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>], </span><br><span class="line">       [-<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>], </span><br><span class="line">       [<span class="number">1</span>, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">u, s, vt = np.linalg.svd(A)</span><br><span class="line">print(<span class="string">&quot;U:&quot;</span>, u,<span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;Sigma:&quot;</span>, s, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;VT:&quot;</span>,vt)</span><br><span class="line"></span><br><span class="line">OUT:</span><br><span class="line"></span><br><span class="line">U: [[-<span class="number">4.47213595e-01</span> -<span class="number">5.00000000e-01</span>  <span class="number">7.11769812e-01</span>  <span class="number">2.08287624e-01</span></span><br><span class="line">   <span class="number">1.00420616e-17</span>]</span><br><span class="line"> [ <span class="number">4.47213595e-01</span> -<span class="number">5.00000000e-01</span> -<span class="number">3.37852315e-02</span> -<span class="number">1.24600260e-01</span></span><br><span class="line">   <span class="number">7.30296743e-01</span>]</span><br><span class="line"> [-<span class="number">4.47213595e-01</span> -<span class="number">5.00000000e-01</span> -<span class="number">7.00888060e-01</span>  <span class="number">2.34632102e-01</span></span><br><span class="line">  -<span class="number">6.08580619e-02</span>]</span><br><span class="line"> [ <span class="number">4.47213595e-01</span> -<span class="number">5.00000000e-01</span>  <span class="number">2.29034795e-02</span> -<span class="number">3.18319466e-01</span></span><br><span class="line">  -<span class="number">6.69438681e-01</span>]</span><br><span class="line"> [-<span class="number">4.47213595e-01</span> -<span class="number">5.55111512e-17</span> -<span class="number">2.17635041e-02</span> -<span class="number">8.85839452e-01</span></span><br><span class="line">   <span class="number">1.21716124e-01</span>]] </span><br><span class="line"> Sigma: [<span class="number">3.16227766e+00</span> <span class="number">2.82842712e+00</span> <span class="number">2.95877601e-16</span> <span class="number">4.03361097e-17</span>] </span><br><span class="line">    </span><br><span class="line"> VT: [[-<span class="number">7.07106781e-01</span>  <span class="number">7.07106781e-01</span>  <span class="number">0.00000000e+00</span>  <span class="number">0.00000000e+00</span>]</span><br><span class="line"> [-<span class="number">7.02166694e-17</span> -<span class="number">7.02166694e-17</span>  <span class="number">7.07106781e-01</span> -<span class="number">7.07106781e-01</span>]</span><br><span class="line"> [ <span class="number">2.98602000e-01</span>  <span class="number">2.98602000e-01</span> -<span class="number">6.40965557e-01</span> -<span class="number">6.40965557e-01</span>]</span><br><span class="line"> [ <span class="number">6.40965557e-01</span>  <span class="number">6.40965557e-01</span>  <span class="number">2.98602000e-01</span>  <span class="number">2.98602000e-01</span>]]</span><br></pre></td></tr></table></figure><p>$X^T=\tilde V \tilde \Sigma \tilde U^T $</p><h3 id="最小二乘的SVD"><a href="#最小二乘的SVD" class="headerlink" title="最小二乘的SVD"></a>最小二乘的SVD</h3><p><img src="https://i.loli.net/2020/10/28/uN4JBCmSzZceHiP.png" alt="SVD最小二乘" style="zoom:125%;" /></p><script type="math/tex; mode=display">\tilde w =(X^TX)^{-1}X^Ty=(V\Sigma^TU^TU\Sigma V^T)^{-1}V\Sigma^TU^T \tag{2}</script><p>对于均是方阵$(AB)=B^{-1}A^{-1}$。另外$U^TU=U^TU=I=UU^{-1}$。</p><p>式2变为：</p><script type="math/tex; mode=display">\tilde w =(V\Sigma^T\Sigma V^T)^{-1}V\Sigma^TU^T \tag{3}</script><p>$V^TV=VV^T=I=VV^{-1} \Rightarrow V^T=V^{-1}$</p><script type="math/tex; mode=display">\begin{aligned}\tilde w &=(V^T)^{-1}(V\Sigma^T \Sigma)^{-1}V \Sigma^TU^T\\&=V(\Sigma^T \Sigma)^{-1}V^{-1}V\Sigma^TU^T\\&=V(\Sigma^T \Sigma)^{-1}V^{T}V\Sigma^TU^T\\&=V(\Sigma^T \Sigma)^{-1}\Sigma^TU^T\\\end{aligned}</script><p>因为$n \ge p$,$\Sigma$矩阵是瘦长型，下面全是0.$\Sigma$是$n \times p$,那么$\Sigma^T\Sigma$是$p \times p$。即</p><script type="math/tex; mode=display">\Sigma^T \Sigma=\begin{bmatrix}    \sigma^{2}_1 & 0 & \cdots & 0 \\    0 & \sigma^{2}_2 & \cdots & 0 \\    \vdots & \vdots & \ddots & \vdots \\    0 & 0 & \cdots & \sigma^{2}_p \end{bmatrix}</script><script type="math/tex; mode=display">(\Sigma^T \Sigma)^{-1}=\begin{bmatrix}    1/\sigma^{2}_1 & 0 & \cdots & 0 \\    0 & 1/\sigma^{2}_2 & \cdots & 0 \\    \vdots & \vdots & \ddots & \vdots \\    0 & 0 & \cdots & 1/\sigma^{2}_p \end{bmatrix}</script><p>那么，竖线后面全是0</p><script type="math/tex; mode=display">\underbrace{ (\Sigma^T \Sigma)^{-1}\Sigma^T}_{p \times n}=\left[\begin{array}{cccc|c}    1/\sigma^{2}_1 & 0 & \cdots & 0  &\\    0 & 1/\sigma^{2}_2 & \cdots & 0 \\    \vdots & \vdots & \ddots & \vdots \\    0 & 0 & \cdots & 1/\sigma^{2}_p \end{array}\right]=\Sigma^+</script><p>又叫做$\Sigma$的pseudo-inverse。</p><script type="math/tex; mode=display">(X^TX)^{-1}X^T=V\Sigma^{+}U^T</script><script type="math/tex; mode=display">\hat y= V\Sigma U^T\hat w</script><p>$ w$估计为：</p><script type="math/tex; mode=display">\hat w = V\Sigma^{+}U^Ty</script><p>证明：</p><script type="math/tex; mode=display">\begin{aligned}y &\approx  \hat y = U \ Sigma V^T \hat w\\\\&\Rightarrow U^Ty \approx U^TU \Sigma V^T \hat w \quad 而UU^T=I\\\\&\Rightarrow \Sigma^+U^Ty \approx \Sigma^+\Sigma V^T \hat w \quad 因为\Sigma^+\Sigma=I\\\\&\Rightarrow V \Sigma^+U^Ty  \approx VV^T \hat w \quad 而VV^T=I\\\\&\Rightarrow V \Sigma^+U^Ty  \approx \hat w\end{aligned}</script>]]></content>
      
      
      <categories>
          
          <category> CMSC35300 Mathematics Foundations of ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> SVD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 8 The Singular Value Decomposition</title>
      <link href="2020/10/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture8/"/>
      <url>2020/10/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture8/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-8-The-Singular-Value-Decomposition"><a href="#Lecture-8-The-Singular-Value-Decomposition" class="headerlink" title="Lecture 8 The Singular Value Decomposition"></a>Lecture 8 The Singular Value Decomposition</h2><h3 id="1-SVD"><a href="#1-SVD" class="headerlink" title="1. SVD"></a>1. SVD</h3><p>$X \in \mathbb{R}^{n \times p}$有SVD $U\Sigma V^T$,并且满足：</p><ul><li>$U \in \mathbb{R}^{n \times n}$是正交的$UU^T=U^TU=I$ [   $U$的列=左边奇异向量]</li><li>$V \in \mathbb{R}^{p \times p}$是正交的$VVT=V^TV=I$ [   $V$的列=右边奇异向量]</li><li>$\Sigma \in \mathbb{R}^{n \times p}$是对角阵，并且对角元素满足$\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_p$。</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/10/28/szpP8XFyVHhESN4.png?=raw" width=50% height=50%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">SVD示意图</div> </center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.mat([[<span class="number">10</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">u, s, vt = np.linalg.svd(A)</span><br><span class="line">print(<span class="string">&quot;U:&quot;</span>, u,<span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;Sigma:&quot;</span>, s, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;VT:&quot;</span>,vt)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">OUT:</span><br><span class="line">U: [[<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">    [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line">    [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]] </span><br><span class="line"> Sigma: [<span class="number">10.</span>  <span class="number">5.</span>  <span class="number">1.</span>] </span><br><span class="line"> VT: </span><br><span class="line"> [[<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line">  [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line">  [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.mat([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">5</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">10</span>]])</span><br><span class="line">u, s, vt = np.linalg.svd(A)</span><br><span class="line">print(<span class="string">&quot;U:&quot;</span>, u,<span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;Sigma:&quot;</span>, s, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;VT:&quot;</span>,vt)</span><br><span class="line"></span><br><span class="line">OUT：</span><br><span class="line">U: </span><br><span class="line"> [[<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line">  [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line">  [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]] </span><br><span class="line"> Sigma: [<span class="number">10.</span>  <span class="number">5.</span>  <span class="number">1.</span>] </span><br><span class="line"> VT: </span><br><span class="line"> [[<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]</span><br><span class="line">  [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line">  [<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.mat([[-<span class="number">10</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, -<span class="number">5</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, -<span class="number">1</span>]])</span><br><span class="line">u, s, vt = np.linalg.svd(A)</span><br><span class="line">print(<span class="string">&quot;U:&quot;</span>, u,<span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;Sigma:&quot;</span>, s, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&quot;VT:&quot;</span>,vt)</span><br><span class="line"></span><br><span class="line">U: [[<span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span>]] </span><br><span class="line"> Sigma: [<span class="number">10.</span>  <span class="number">5.</span>  <span class="number">1.</span>] </span><br><span class="line"> VT: [[-<span class="number">1.</span> -<span class="number">0.</span> -<span class="number">0.</span>]</span><br><span class="line">   [-<span class="number">0.</span> -<span class="number">1.</span> -<span class="number">0.</span>]</span><br><span class="line">   [-<span class="number">0.</span> -<span class="number">0.</span> -<span class="number">1.</span>]]</span><br></pre></td></tr></table></figure><p>注意上面，SVD 中$U, \Sigma, V$变化</p><p>对于上面第三个例子，$SVD(X)=(U\Sigma V^T)^T=V\Sigma U^T$</p><p>奇异值会按照从大到小排，这跟最小化投影有关。</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/10/28/9Ysda1k6mMwDcQ3.png?=raw" width=60% height=60%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">奇异值变化</div> </center><p>等价地，我们可以用rank-1的矩阵的和表示矩阵$X$</p><script type="math/tex; mode=display">X= \sum^{min(n, p)}_{i=1}\sigma_iU_iV_i^T \tag{1}</script><p>其中，$U_i$是$U$的第$i$列，$V_i^T$是$V$的第$i$列。</p><p><img src="https://i.loli.net/2020/10/28/TRa64gqme1uKYd8.png" alt="SVD分解" style="zoom:100%;" /></p><h3 id="2-The-subspace-approximation-Theorem"><a href="#2-The-subspace-approximation-Theorem" class="headerlink" title="2. The subspace approximation Theorem"></a>2. The subspace approximation Theorem</h3><p>给定$X \in \mathbb{R}^{n \times p}$，$rank (X) = r \le min(p, n)$。矩阵的秩等于奇异值大于0的个数$num(\sigma_i \ge 0)$。</p><p>找一个$X_k \in \mathbb{R}^{n \times p}$,并且$rank (X_k) = k &lt; r$，作为尽可能接近$X$。(as “close “ as possible to X).</p><script type="math/tex; mode=display">min_{Z \in \mathbb{R}^{n \times p}} \lVert  Z-X \rVert^2_{F} \ \text{with rank(Z)=k} \tag{2}</script><p><strong>Frobenius norm</strong> :</p><script type="math/tex; mode=display">\lVert  Z-X \rVert^2_{F} = \left( \sum_{(i, j)}A^2_{i, j} \right)^{\frac{1}{2}} \tag{3}</script><script type="math/tex; mode=display">If A = [A_1, A_2, \cdots, A_p], \ then \ \lVert  A \rVert^2_{F} = \lVert  A \rVert^2_{2} \tag{4}</script><p>近似矩阵$\tilde X$的SVD有：</p><ul><li>$\tilde \Sigma \in \mathbb{R}^{r \times r}对角元素 \tilde \sigma_1 \ge \tilde \sigma_2 \ge \dots \ge \tilde \sigma_p.$</li><li>$\tilde U \in \mathbb{R}^{n \times r}, \tilde U^T\tilde U=I, but \ \tilde U\tilde U^T \ne I $</li><li>$\tilde V \in \mathbb{R}^{p \times r}, \tilde V^{T}\tilde V=I, but \ \tilde V\tilde V^T \ne I $</li></ul><p><img src="https://i.loli.net/2020/10/28/iYI2ZeNqgyufBXJ.png" alt="The subspace approximation Theorem" style="zoom:100%;" /></p><p><strong>Singular Value Spectrum</strong>:</p><h3 id="3-The-“-Economy-SVD”"><a href="#3-The-“-Economy-SVD”" class="headerlink" title="3. The “ Economy SVD”"></a>3. The “ Economy SVD”</h3><p><img src="https://i.loli.net/2020/10/28/rU3scDBv1nh9tyC.png" alt="SVD的作用" style="zoom:100%;" /></p><h3 id="4-降维"><a href="#4-降维" class="headerlink" title="4. 降维"></a>4. 降维</h3><p><img src="https://i.loli.net/2020/10/28/Wp35fln1gKMvG7e.png" alt="降维" style="zoom:100%;" /></p><p>给定$X_i \in \mathbb{R}^p \ for \ i=1, \cdots, n$，找到对于$k &lt; p \ for \  i=1, \dots, n 的Z_i \in \mathbb{R}^k $，有跟$X_i$同样的性质。</p><script type="math/tex; mode=display">X^T_k = V_k{\Sigma}U^T_k</script><p>其中，</p><ul><li>$V_k$: $p \times k$ 对于最好的k-dim子空间的基。</li><li>${\Sigma}U^T_k$ 第i列是对$\tilde X_i$k组基的系数。</li></ul>]]></content>
      
      
      <categories>
          
          <category> CMSC35300 Mathematics Foundations of ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> SVD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 7 Introduction to the Singular Value Decomposition</title>
      <link href="2020/10/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture7/"/>
      <url>2020/10/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture7/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-7-Introduction-to-the-Singular-Value-Decomposition"><a href="#Lecture-7-Introduction-to-the-Singular-Value-Decomposition" class="headerlink" title="Lecture 7 Introduction to the Singular Value Decomposition"></a>Lecture 7 Introduction to the Singular Value Decomposition</h2><h3 id="1-问题引入和投影矩阵的性质"><a href="#1-问题引入和投影矩阵的性质" class="headerlink" title="1. 问题引入和投影矩阵的性质"></a>1. 问题引入和投影矩阵的性质</h3><p>Goal:对于观测值$X_1, X_2,\cdots, X_p \in \mathbb{R}^n$,找到一个一维子空间(可以理解为一根直线)能”最好的拟合数据”</p><p>Solution： </p><script type="math/tex; mode=display">\text{ 把每一个 }X_i \text{ 投影到 } \overrightarrow a上， Proj^{X_i}_{\overrightarrow a} \ , \text{ 投影到让距离 } d^2_i=\lVert  X_i- Proj^{X_i}_{\overrightarrow a}\rVert^2_{2} 和最小。</script><p>复习<strong>Projection Matrices</strong>：</p><p>If $A \in \mathbb{R}^{n \times p}$张成的子空间，那么Projection of $X$ onto $span(cols(A))=Proj_A X$，如果$A$的每列都是线性无关的，并且有：</p><script type="math/tex; mode=display">Proj_A X = A(A^TA)^{-1}X \tag{1}</script><p>其中，$P_A=A(A^TA)^{-1}A^T$。</p><p>还有性质：</p><ol><li><script type="math/tex; mode=display">P_A = P^{2}_A=P_AP_A=P^T_{A}=P^T_{A}P_A \tag{2}</script></li><li><p>If $A = \boldsymbol{a}$,</p><script type="math/tex; mode=display">P_{\boldsymbol{a}} = \boldsymbol{a}(\boldsymbol{a}^T\boldsymbol{a})^{-1}\boldsymbol{a}^T=\frac{\boldsymbol{a}\boldsymbol{a}^T}{\boldsymbol{a}^T\boldsymbol{a}}\tag{3}</script><p>因为$(\boldsymbol{a}^T\boldsymbol{a})^{-1}$是一个标量(可以看详细投影矩阵的证明)。</p></li></ol><ol><li><p>The orthofonal complement of a subspace is the set of all vectors orthogonal to the subspace.</p><p>子空间的正交补 <strong>orthofonal complement</strong> 是所有正交于子空间的向量的集合(这个子空间是正交子空间)。并且：$A^TB=0$（正交）</p><p>(不是这个图的示意，是)如果$A \in \mathbb{R}^{n \times r0}$,那么$B \in \mathbb{R}^{n \times (n-r)}$。维度上和不变，正交补的维度是其差。</p></li></ol><p>   对于任意的$X \in \mathbb{R}^n$,能写作：</p><script type="math/tex; mode=display">   IX = P_AX+P_BX =(P_A + P_B)X \Rightarrow I=P_A+P_B \ \Rightarrow P_B=I-P_A\tag{4}</script>   <center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/10/28/4aoHTGlmw6BF7N3.png?=raw" width=25% height=25%>        <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">正交空间</div> </center><p><img src="https://i.loli.net/2020/10/27/oGJK8lOet1m3fIy.png" alt="SVD几何表示" style="zoom:100%;" /></p><h3 id="2-证明最小距离和、引入奇异向量奇异值"><a href="#2-证明最小距离和、引入奇异向量奇异值" class="headerlink" title="2. 证明最小距离和、引入奇异向量奇异值"></a>2. 证明最小距离和、引入奇异向量奇异值</h3><p><img src="https://i.loli.net/2020/11/06/zwhjaEtkxWKfeid.png" alt="SVD" style="zoom:125%;" /></p><p>将$P_a$用式3代入，然后相当于做了提取公因子$X_i$ [拓展作单位矩阵 $IX_i$ ]，那么$I-\frac{aa^T}{a^Ta}$可以看作投影矩阵$I-P_a$,由式4可得这也是投影矩阵$P_B$,这就是正交补。再写作内积展开式，由性质2公式3可以知道$(I-\frac{aa^T}{a^Ta})^T(I-\frac{aa^T}{a^Ta})=I-\frac{aa^T}{a^Ta}$.</p><p>​    而$X_i^TX$是常数跟$a$无关。即只要最大化$\frac{X_iaa^TX_i}{a^Ta}$.</p><p>​    $a^TX_i$是标量。</p><p>​    </p><script type="math/tex; mode=display">\begin{aligned}XX^T&=\left[\begin{array}{ll}| & |&|&|\\X_{1} & X_{2}&\cdots&X_{p} \\| &| &|&|\end{array}\right]\left[\begin{array}{ll}- & X_{1}^{\top}&- \\- & X_{2}^{\top}&- \\- &    \vdots &-        \\- & X_{p}^{\top}&-\end{array}\right]\\&=X_1X_1^T+X_2X_2^T+\cdots+X_pX_p^T\\&=\sum^{p}_{1}X_iX_i^T \end{aligned}\tag{5}</script><p>​    用式5矩阵的外积，写作矩阵外积去掉求和，只要求$argmax_{a}(\frac{a^TXX^Ta}{a^Ta})$。</p><p>现在定义一些名词：</p><ol><li><p>使得上是取得最大值的向量$\hat{\boldsymbol{a}}$称作$\boldsymbol{X}$的第一个左奇异向量。</p></li><li><p>$\frac{a^TXX^Ta}{a^Ta}=\sigma^{2}_1$(这里没用黑粗体懒得写了)的值称作$X$第一个奇异值。</p><script type="math/tex; mode=display">\begin{aligned}\frac{a^TXX^Ta}{a^Ta}&=\sigma^{2}_1\\&= \lVert X \rVert^2_{op} \ \text{"operator norm"}\\&=\lVert X \rVert^2_{2} (跟向量2范数有别)\end{aligned}\tag{6}</script></li></ol><h3 id="3-奇异值分解"><a href="#3-奇异值分解" class="headerlink" title="3. 奇异值分解"></a>3. 奇异值分解</h3><p><img src="https://i.loli.net/2020/11/03/GTAeDhXVw1gbL7K.png" alt="SVD降维" style="zoom:100%;" /></p><p>$U$的列向量是$X$的列向量的正交基。</p><p>$\Sigma$是没有负元素的对角矩阵，$\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_p$。</p><p>令$U = [U_1, U_2, \cdots, U_n]$,  $U_1$is the best 1d subspace fit to $X_i’$s。也是上面式6的解。</p><p>令 <script type="math/tex">\tilde X_i^{(1)} = X_i - Proj_{U_1}{X_i}</script> ，</p><p>​    </p><script type="math/tex; mode=display">U_2= \text{ the best 1d subspace fit to } \tilde X_i' s.</script><p>若$X_{n \times p} $有秩$r &lt; min(n, p)$。</p><p>thin $\Sigma$。</p><h3 id="4-PCA"><a href="#4-PCA" class="headerlink" title="4. PCA"></a>4. PCA</h3><p><img src="https://i.loli.net/2020/10/27/bz3rqiYtBOsEHwF.png" alt="PCA-1" style="zoom:100%;" /></p><p><img src="https://i.loli.net/2020/10/28/ewo8Ipvx4tluJbj.png" alt="PCA-2" style="zoom:100%;" /></p>]]></content>
      
      
      <categories>
          
          <category> CMSC35300 Mathematics Foundations of ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> SVD </tag>
            
            <tag> PCA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 6 Finding Orthogonal Bases</title>
      <link href="2020/10/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture6/"/>
      <url>2020/10/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture6/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-6-Finding-Orthogonal-Bases"><a href="#Lecture-6-Finding-Orthogonal-Bases" class="headerlink" title="Lecture 6 Finding Orthogonal Bases"></a>Lecture 6 Finding Orthogonal Bases</h2><h3 id="1-怎么求得U"><a href="#1-怎么求得U" class="headerlink" title="1.怎么求得U?"></a>1.怎么求得U?</h3><ol><li>Gram Schemidt Orthogonalization</li><li>Singular Value Decomposition</li></ol><p>有$X$怎么找到$U$?</p><p><img src="https://i.loli.net/2020/10/26/USF1erTsbyMqVJa.png" alt="施密特正交化"></p><p>残差 <script type="math/tex">X^{\prime}_2=X_2-aU_1</script> ,所以 <script type="math/tex">U_2=\frac {X^{\prime}_2} {\lVert  X^{\prime}_2 \rVert^2_{2}}=\left[\begin{array}{c}0 \\b \end{array}\right]/b=\left[\begin{array}{c}0 \\1 \end{array}\right]</script> </p><h3 id="2-斯密特正交化的几何示例"><a href="#2-斯密特正交化的几何示例" class="headerlink" title="2. 斯密特正交化的几何示例"></a>2. 斯密特正交化的几何示例</h3><p><img src="https://i.loli.net/2020/10/26/5WoS1ITcDa3iJ2A.png" alt="施密特正交化计算" style="zoom:100%;" /></p><script type="math/tex; mode=display">\begin{align}&X^{\prime}_2=X_2-Projection(X_2 \ onto \ U_1) \ , \text{best fit of } X_2 \text{ as weighted  }  U_1  。\\ &\text{best  fit} = U_1w = U_1U^{T}_1X_2=P_{U_1}(X_2)\end{align}</script><p>示例，施密特正交化具体过程和想法。</p><script type="math/tex; mode=display">U_1=\left[\begin{array}{c}1/\sqrt{2} \\1/\sqrt{2}\\0\end{array}\right],X_2=\left[\begin{array}{c}1 \\3\\0\end{array}\right]</script><p>  那么，</p><script type="math/tex; mode=display">U_1U_1^TX_2=\left[\begin{array}{c}1/2 & 1/2& 0\\1/2 & 1/2& 0\\0   & 0  & 0\end{array}\right]\left[\begin{array}{c}1\\3\\0\end{array}\right]=\left[\begin{array}{c}2\\2\\0\end{array}\right]</script><script type="math/tex; mode=display">X_2-X_2-U_1U_1^TX_2=\left[\begin{array}{c}1\\3\\0\end{array}\right]-\left[\begin{array}{c}2\\2\\0\end{array}\right]=\left[\begin{array}{c}-1\\1\\0\end{array}\right]\tag{1}</script><script type="math/tex; mode=display">U_2=\left[\begin{array}{c}-1\\1\\0\end{array}\right]/ \sqrt{2}=\left[\begin{array}{c}-\sqrt{2}/2\\\sqrt{2}/2\\0\end{array}\right]</script><h3 id="3-斯密特正交化算法"><a href="#3-斯密特正交化算法" class="headerlink" title="3. 斯密特正交化算法"></a>3. 斯密特正交化算法</h3><p><img src="https://i.loli.net/2020/10/26/9HBc6F81seotMRW.png" alt="施密特正交化算法"></p><p><a href="https://zh.wikipedia.org/wiki/%E6%A0%BC%E6%8B%89%E5%A7%86-%E6%96%BD%E5%AF%86%E7%89%B9%E6%AD%A3%E4%BA%A4%E5%8C%96">施密特正交化wiki</a></p><p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://upload.wikimedia.org/wikipedia/commons/5/56/GSO.png?=raw" width=25% height=25%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">斯密特正交化 引用自wiki https://upload.wikimedia.org/wikipedia/commons/5/56/GSO.png</div> </center><br><img src="https://gitee.com/miller999999/pic/raw/master/img/image-20201026214840407.png" alt="施密特正交化计算" style="zoom:100%;" /></p>]]></content>
      
      
      <categories>
          
          <category> CMSC35300 Mathematics Foundations of ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 正交基 </tag>
            
            <tag> 斯密特正交化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 5 Subspaces, Bases, and Projections</title>
      <link href="2020/10/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture5/"/>
      <url>2020/10/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture5/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-5-Subspaces-Bases-and-Projections"><a href="#Lecture-5-Subspaces-Bases-and-Projections" class="headerlink" title="Lecture 5 Subspaces, Bases, and Projections"></a>Lecture 5 Subspaces, Bases, and Projections</h2><h3 id="1-回想最小二乘的几何意义引入span"><a href="#1-回想最小二乘的几何意义引入span" class="headerlink" title="1.回想最小二乘的几何意义引入span"></a>1.回想最小二乘的几何意义引入span</h3><p><img src="https://i.loli.net/2020/10/26/2K3OkpZTwcEmt4A.png" alt="最小二乘几何表示" style="zoom:80%;" /></p><p>上面的平面可有$X$的列向量的张成空间$X$。</p><script type="math/tex; mode=display">span(cols(X)) =  { v \in \mathbb{R}^n \ v=w_1X_1 + w_2X_2+\cdots+ w_pXp \quad for \ some \ w_1, w_2, \cdots, w_p  }</script><h3 id="2-subspace"><a href="#2-subspace" class="headerlink" title="2. subspace"></a>2. subspace</h3><p><img src="https://i.loli.net/2020/10/26/vau9GQDrsZqHVRL.png" alt="子空间" style="zoom:100%;" /></p><p>If the cols of $X$ are Linearly Independent, $X$ is a subspace. Then  they form a basis for $X$.</p><p>上上图中的(绿色点)$\hat {\underline y}$是$\underline{y}$在$X$上的投影。</p><p><img src="https://i.loli.net/2020/10/26/WtPgoJCYe1KbcNS.png" alt="空间" style="zoom:80%;" /></p><p>vertical plane</p><p>horizontal plane</p><p>子空间永远包含原点或者说$\overrightarrow{0}$。</p><p><img src="https://i.loli.net/2020/10/26/CSrXLAnMBOyw8xY.png" alt="span" style="zoom:100%;" /></p><h3 id="3-怎么表示一个子空间"><a href="#3-怎么表示一个子空间" class="headerlink" title="3. 怎么表示一个子空间?"></a>3. 怎么表示一个子空间?</h3><p><img src="https://i.loli.net/2020/10/26/iOnAXw7rGBfvN4T.png" alt="子空间的表示" style="zoom:100%;" /></p><ul><li>用一组向量的集合作为一个张成子空间</li><li>用一组线性无关的向量张成子空间，这组向量叫一组基</li><li>用一组正交单位向量(orthonormal vector)张成子空间,这组向量又叫正交单位化基，正交基。</li></ul><p>orthonormal: </p><ul><li>ortho : orthogonal</li><li>normal: norm(length=1)</li></ul><p>理解正交、单位向量</p><script type="math/tex; mode=display">如果 \ <u_1, u_2>=u_1^Tu_2=u_2^Tu_1=0\tag{1}</script><p>那么，两个向量$u_1$和$u_2$都是正交的。</p><script type="math/tex; mode=display">如果 \ \lVert  u\rVert_{2} = \lVert  u\rVert^2_{2} =<u_1, u_2>=u^Tu=1 \tag{2}</script><p>那么$u$是单位化的。</p><script type="math/tex; mode=display">如果u_1, u_2, \cdots, u_p满足：<u_i, u_j>={\begin{cases}1,&{\mbox{if }}i=j\\0&{\mbox{if }}i \neq j\end{cases}}</script><p>这组向量$u_1, u_2, \cdots, u_p$是正交单位向量</p><h3 id="4-正交矩阵及其性质"><a href="#4-正交矩阵及其性质" class="headerlink" title="4. 正交矩阵及其性质"></a>4. 正交矩阵及其性质</h3><p><img src="https://i.loli.net/2020/10/26/OgLB42xKds3c1If.png" alt="正交基" style="zoom:80%;" /></p><p>The matrix $U$ is orthogonal in this setting if it is a $p \times p$ square matrix. If it’s an $n \times p$ matrix with $n&gt;p$, then $U $gives a basis for a subspace but it is not an orthogonal matrix. In this case, $U^TU=I$,  but $UU^T$ is not = $I$.</p><p>如果$u_1, \cdots, u_p$是正交单位化的，且$\delta = span (u_1, \cdots, u_p)$，那么</p><script type="math/tex; mode=display">U=\left[\begin{array}{ll}| & | &| \\u_1 & u_2 ...&u_p \\| & | & |\end{array}\right]是正交基矩阵。\tag{3}</script><p>性质1：</p><script type="math/tex; mode=display">U^TU = I\tag{4}</script><p>性质2:</p><script type="math/tex; mode=display">\lVert  Uv\rVert^2_{2} = \lVert  v\rVert^2_{2}\tag{5}</script><p>记个记号：dimension of subspace , $dim(\delta)$=子空间基向量的个数。</p><p>如果一个子空间有矩阵$X$的列向量张成，那么$dim(\delta)=rank(X)$。</p><h3 id="5-矩阵中含有线性无关-列或行-向量的数目"><a href="#5-矩阵中含有线性无关-列或行-向量的数目" class="headerlink" title="5. 矩阵中含有线性无关(列或行)向量的数目?"></a>5. 矩阵中含有线性无关(列或行)向量的数目?</h3><p><img src="https://i.loli.net/2020/10/26/vmoji1UF9BubpIs.png" alt="无线向量" style="zoom:80%;" /></p><p>$X \in \mathbb{R}^{n\times p}$，$r=rank(X)\leq min(n,p)$ , 假设$n \geq p \Rightarrow r \leq p$。</p><p>我们能在$\mathbb{R}^n$上有多于n个线性无关的向量?——不能。</p><h3 id="6-Projection"><a href="#6-Projection" class="headerlink" title="6. Projection"></a>6. Projection</h3><p>一个点的投影到一个集上是这个集上离这个点最近的点。</p><p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/10/26/lRCy7ZQw3t5D1YA.png?=raw" width="50%" height="50%">     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">Projection</div> </center><br><img src="https://i.loli.net/2020/10/26/HNhJmEaA9eZicOM.png" alt="Projection" style="zoom:80%;" /></p><p>$(X^TX)^{-1}X^T$又叫作投影矩阵, 记作$P_X$。</p><p>用投影的定义来最优化解决问题。</p><p>$P_X$的性质：</p><ol><li>方阵</li><li>$P_X=P_XP_X=P^2_X$</li></ol><p>从定义很容易证明，投影的投影就是本身。</p><h3 id="7-正交基子空间和最小二乘法"><a href="#7-正交基子空间和最小二乘法" class="headerlink" title="7. 正交基子空间和最小二乘法"></a>7. 正交基子空间和最小二乘法</h3><p><img src="https://i.loli.net/2020/10/26/rAndYw2IyQNDxlV.png" alt="正交基和最小二乘" style="zoom:80%;" /></p><p>这里用全新的角度来解决上述中非常难计算的$P_X=(X^TX)^{-1}X^T$。</p><p>找到一组正交基向量使得$span(X_1, X_2, \cdots, X_p)=span(U_1, U_2, \cdots, U_p)$即$span(cols(U))=span(cols(X))$。</p><p>那么$P_Xy=P_Uy$，就有$\hat y = UU^Ty$。不要矩阵的逆了。</p><p><img src="https://i.loli.net/2020/10/26/FPwCGDrkfWv7MKR.png" alt="线性表示子空间" style="zoom:80%;" /></p><p><img src="https://i.loli.net/2020/10/26/sSxTEZRGKgA4twh.png" alt="线性表示子空间-2" style="zoom:80%;" /></p><p>注意：最后一行。</p>]]></content>
      
      
      <categories>
          
          <category> CMSC35300 Mathematics Foundations of ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 矩阵论 </tag>
            
            <tag> 子空间 </tag>
            
            <tag> 正交基 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 4 Least squares and  Optimization</title>
      <link href="2020/10/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture4/"/>
      <url>2020/10/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture4/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-4-Least-squares-and-Optimization"><a href="#Lecture-4-Least-squares-and-Optimization" class="headerlink" title="Lecture 4 Least squares and  Optimization"></a>Lecture 4 Least squares and  Optimization</h2><h3 id="1-最小二乘估计"><a href="#1-最小二乘估计" class="headerlink" title="1. 最小二乘估计"></a>1. 最小二乘估计</h3><p><img src="https://i.loli.net/2020/10/25/yDWUbxNtIqdOlhP.png" alt="最小二乘估计" style="zoom:80%;" /></p><p>最小二乘估计(上节课没讲的)：</p><p>如果列向量都是线性无关的找到最小化残差和的$w$是：</p><script type="math/tex; mode=display">\begin{aligned}\hat {\underline w}&=(X^TX)^{-1}X^Ty\\ \Rightarrow \hat {\underline y}&=X \hat {\underline w} =X(X^TX)^{-1}X^Ty\end{aligned}\tag{1}</script><p>其中，$(X^TX)^{-1}X^T$又叫pseudo-inverse， 伪逆。</p><p>$P_{Xy}$投影$y$到$X$上。</p><h3 id="2-最小二乘法分类"><a href="#2-最小二乘法分类" class="headerlink" title="2. 最小二乘法分类"></a>2. 最小二乘法分类</h3><p><img src="https://i.loli.net/2020/10/25/ujR5nvF3pdfcDwH.png" alt="最小二乘分类" style="zoom:80%;" /></p><p>二分类就是把$<X_{new},  \underline{\hat w}>$输入到sign函数得到其输出。</p><h3 id="3-优化方法"><a href="#3-优化方法" class="headerlink" title="3. 优化方法"></a>3. 优化方法</h3><p><img src="https://i.loli.net/2020/10/25/4BMdthxRajYJWUX.png" alt="优化方法" style="zoom:80%;" /></p><p>2范数或者欧几里得范数：</p><script type="math/tex; mode=display">\lVert  x\rVert^2_{2}  = \sum^n_{i=1}x_i^2=x^Tx=<x, x>\tag{2}</script><p>所以：</p><script type="math/tex; mode=display">\begin{aligned}arg \ min_w\lVert  y-Xw\rVert^2_2 &=arg \ min_w \ (y-Xw)^T(y-Xw)\\ &=arg \ min_w (y^Ty-y^TXw-w^TX^Ty+w^TX^TXw)\\ &=arg \ min_w (y^Ty-2w^TX^Ty+w^TX^TXw)\end{aligned}\tag{3}</script><p>$y^TXw和w^TX^Ty$都是标量，而且是一样的。</p><p>Warmup：$f(w) = \frac{1}{2}w^2-w-\frac{1}{2}$。求导求极值点。</p><h3 id="4-正定矩阵"><a href="#4-正定矩阵" class="headerlink" title="4. 正定矩阵"></a>4. 正定矩阵</h3><p><img src="https://i.loli.net/2020/10/25/jfe3tGv7lCqd6IT.png" alt="正定矩阵" style="zoom:80%;" /></p><p>我们需要$X^TX$是可逆的。</p><p>从优化的角度来看这个问题。$X^TX$是正定的</p><p>正定矩阵定义：</p><script type="math/tex; mode=display">\begin{aligned}& \text{给定一个大小为}n \times n \text{的实对称矩阵}Q ，\text{若对于任意长度为}n\text{的非零向量}x ，\text{有}x^TQx>0\text{恒成立，则矩阵}Q\text{是一个正定矩阵}\\&(positive \ definite(p.d)。\text{简记}Q>0)。\\\\&\text{给定一个大小为}n \times n\text{的实对称矩阵}Q ，\text{若对于任意长度为}n\text{的非零向量}x ，\text{有}x^TQx \ge 0\text{恒成立，则矩阵}Q是\text{一个半正定矩}\\&\text{阵}(positive \ semi-definite(p.s.d)。\text{简记}Q \ge 0)。\end{aligned} \tag{4}</script><h4 id="1-详解正定矩阵的作用和凸优化"><a href="#1-详解正定矩阵的作用和凸优化" class="headerlink" title="1. 详解正定矩阵的作用和凸优化"></a>1. 详解正定矩阵的作用和凸优化</h4><p><img src="https://i.loli.net/2020/10/26/la6VyLTRGbF7rj5.png" alt="凸优化" style="zoom:80%;" /></p><p><a href="https://zhuanlan.zhihu.com/p/44860862">正定矩阵详解</a></p><h4 id="2-正定矩阵的性质"><a href="#2-正定矩阵的性质" class="headerlink" title="2. 正定矩阵的性质"></a>2. 正定矩阵的性质</h4><p><img src="https://i.loli.net/2020/10/26/2yfkJQabOdANUjD.png" alt="正定矩阵的性质" style="zoom:80%;" /></p><p>性质3：</p><p>对于任意矩阵$A$，那么$A^TA \ge 0 $和$AA^T \ge 0 $</p><h3 id="5-最优化最小二乘法"><a href="#5-最优化最小二乘法" class="headerlink" title="5. 最优化最小二乘法"></a>5. 最优化最小二乘法</h3><p><img src="https://i.loli.net/2020/10/26/GkB7YAjedviDqp9.png" alt="Least squares optimization problem" style="zoom:80%;" /></p><p>假设$X^TX &gt; 0$,那么：$f(w)=y^Ty-2w^TX^Ty+w^TX^TXw$是凸的。</p><p>计算其导数，让其等于0来求最小值。</p><p>例如$f(w) = c^Tw=c_1w_1+c_2w_2+ \cdots+c_pw_p$</p><p>其梯度为：</p><script type="math/tex; mode=display">\nabla_{w} f=\left[\begin{array}{c}c_{1} \\c_{2} \\\vdots \\c_{p}\end{array}\right]=c\tag{5}</script><p>例如$f(w) =w^Tw=\lVert  w\rVert^2_{2}=w^2_1 + w^2_2 + \cdots+ w^2_p$</p><script type="math/tex; mode=display">\nabla_{w} f=\left[\begin{array}{c}2w_{1} \\2w_{2} \\\vdots \\2w_{p}\end{array}\right]\tag{6}</script><p>总结，</p><script type="math/tex; mode=display">\begin{aligned}\nabla_{w} <c, w> &= c\\\\\nabla_{w} \lVert  w\rVert^2_{2} &= 2w\end{aligned} \tag{7}</script><p><img src="https://i.loli.net/2020/10/26/DW1o93EOgzFs4Yc.png" alt="对称矩阵的性质" style="zoom:80%;" /></p><p>例子：</p><script type="math/tex; mode=display">f(w) = w^TQw=\sum^p_{i=1} \sum^p_{i=1}w_iQ_ijw_j\tag{8}</script><p>那么对$w$的梯度有下面四种情况:</p><script type="math/tex; mode=display">\frac{d\left(w_{i} Q_{i j} w_{j}\right)}{d w_{k}}=\left\{\begin{array}{ll}2Q_{i i} w_{i} & \text { if } k=i=j \\Q_{i j} w_{j} & \text { if } i=k \neq j \\Q_{i j} w_{i} & \text { if } i \neq k=j \\0 & \text { if } k \neq i, k \neq j\end{array}\right.\tag{9}</script><p>这需要把$\boldsymbol{w}^TQ\boldsymbol{w}$展开来理解，下面用一个具体的例子,其中：</p><script type="math/tex; mode=display">Q=\left[\begin{array}{ccc}2 & -1 & 0 \\-1 & 2 & -1 \\0 & -1 & 2\end{array}\right] \in \mathbb{R}^{3 \times 3}</script><script type="math/tex; mode=display">\boldsymbol{w}=\left[\begin{array}{l}w_{1} \\w_{2} \\w_{3}\end{array}\right] \in \mathbb{R}^{3}</script><p>其部分展开得：</p><script type="math/tex; mode=display">\boldsymbol{w}^{T} Q \boldsymbol{w}=\left[\left(2 w_{1}-w_{2}\right) \quad\left(-w_{1}+2 w_{2}-w_{3}\right) \quad-w_{2}+2 w_{3}\right]\left[\begin{array}{l}w_{1} \\w_{2} \\w_{3}\end{array}\right]</script><p>上式,</p><ul><li>第一种情况，可以结合式7中第二个式子理解，$\boldsymbol{w}_k都是变量$。</li><li>第二种情况，可以结合式7中第一个式子理解，$\boldsymbol{w}_k$是变量,$\boldsymbol{w}_j$是常量。</li><li>第三种种情况，可以结合式7中第一个式子理解，$\boldsymbol{w}_k$是变量,$\boldsymbol{w}_i$是常量。</li><li>不含变量$\boldsymbol{w}_k$都是常量。</li></ul><p>所以：</p><script type="math/tex; mode=display">\frac{d f}{d w_{k}}=\sum_{i=1}^{p} \sum_{j=1}^{p} \frac{d\left(w_{i} Q_{i j} w_{j}\right)}{d w_{k}}\\=\sum_{i=1}^{p} \frac{d\left(w_{i} Q_{i j} \right)}{d w_{k}} + \sum_{j=1}^{p} \frac{d\left( Q_{i j} w_{j}\right)}{d w_{k}}\tag{10}</script><p>回想下简单的公式：</p><script type="math/tex; mode=display">\sum_{i=1}^{N} x_{i} y_{i}=\left[\begin{array}{c}x_{1} \\\vdots \\x_{n}\end{array}\right]^{T}\left[\begin{array}{c}y_{1} \\\vdots \\y_{n}\end{array}\right]=\mathbf{x}^{T} \mathbf{y}</script><p>那么：</p><script type="math/tex; mode=display">\nabla_{\boldsymbol{w}} f=\frac{d f}{d w_{k}}=Q\boldsymbol{w}+Q^T\boldsymbol{w}\tag{11}</script><p>如果$Q$是对称矩阵($Q=Q^T$):</p><script type="math/tex; mode=display">\nabla_{\boldsymbol{w}} f=2Q\boldsymbol{w}\tag{12}</script><p><a href="https://math.stackexchange.com/questions/312077/differentiate-fx-xtax">一些其它方法的证明</a></p><p>而$f(w)=y^Ty-2w^TX^Ty+w^TX^TXw$,其梯度为：</p><script type="math/tex; mode=display">\nabla_{w} f = -2X^Ty+2X^TXw\tag{13}</script><p>令其为0，同样可得：</p><script type="math/tex; mode=display">\begin{array}{l}X^{\top} X \hat{\underline{w}}=X^{\top} y \\\Rightarrow\hat{w}=\left(X^{\top} X\right)^{-1} X^{\top} y\end{array}\tag{14}</script>]]></content>
      
      
      <categories>
          
          <category> CMSC35300 Mathematics Foundations of ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 矩阵论 </tag>
            
            <tag> 最小二乘法 </tag>
            
            <tag> 凸优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 3 Least squares and geometry</title>
      <link href="2020/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture3/"/>
      <url>2020/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture3/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-3-Least-squares-and-geometry"><a href="#Lecture-3-Least-squares-and-geometry" class="headerlink" title="Lecture 3 Least squares and geometry"></a>Lecture 3 Least squares and geometry</h2><h3 id="1-最小二乘法"><a href="#1-最小二乘法" class="headerlink" title="1.最小二乘法"></a>1.最小二乘法</h3><p>目标：从一些特征向量$x$学到预测标签$y$。建立一个线性模型，</p><script type="math/tex; mode=display">\begin{aligned}\hat y_i &= \left \langle X_i, w\right \rangle=X_i^Tw=w^TX_i\\&=w_1X_{i1} + w_2X_{i2}+\cdots+w_pX_{ip}\end{aligned}\tag{1}</script><p>需要找到$w$来使得$\hat y_i = y_i \quad for i = 1, 2, \cdots, n$ </p><p>1.最小二乘法怎么来的?</p><p>因为残差residual error为：</p><script type="math/tex; mode=display">r_i=r_i(\underline{w})=y_i - <w, x_i></script><p>找到最小化残差和的$w$。（式2中第一个式子，第二个式欧式距离）</p><script type="math/tex; mode=display">\begin{aligned}\lVert r_i \rVert^2_{2} &= \sum ^n _{i=1}r_i^2 \\  \lVert r_i \rVert_{2}&=\left(\sum ^n _{i=1}r_i^2 \right)^{1/2}  \end{aligned}\tag{2}</script><p>提到$l_p$Norm，对于向量$r$,对其每一个元素的$p$次幂求和再开$p$次方：</p><script type="math/tex; mode=display">\lVert r \rVert_{p} = \left(\sum ^n _{i=1}\lVert r_i \rVert^{p} \right)^{1/p}\tag{3}</script><p>2.为什么用最小二乘法？</p><ul><li>正负残差同样处理</li><li>数学上便利</li><li>好的几何表示</li><li>放大了大误差影响</li><li>和高斯模型噪声一致</li></ul><p><img src="https://i.loli.net/2020/10/25/jbzIwkhWrQfq61p.png" alt="最小二乘估计" style="zoom:125%;" /></p><h4 id="1-1-张成空间-Span"><a href="#1-1-张成空间-Span" class="headerlink" title="1.1 张成空间 Span"></a>1.1 张成空间 Span</h4><p><img src="https://i.loli.net/2020/10/25/VZcLYajxIuOAEzJ.png" alt="Span" style="zoom:125%;" /></p><h4 id="1-2-最小二乘的几何意义"><a href="#1-2-最小二乘的几何意义" class="headerlink" title="1.2 最小二乘的几何意义"></a>1.2 最小二乘的几何意义</h4><p><img src="https://i.loli.net/2020/10/25/6YtRJuFihnfg9wV.png" alt="最小二乘的几何意义" style="zoom:125%;" /></p><p>最小二乘的几何意义：</p><p>$n=3, p = 2$即3个样本，2个特征，那么：</p><script type="math/tex; mode=display">\hat y = Xw=w_1X_1+w_2X_2 \in \mathbb{R}^3即\hat y \in Span(col(X)) \hat y 在X的列向量的张成空间里。</script><p>图中的蓝色$\underline y$就是真实值，考虑对$X$的列向量的张成空间，一些$\tilde{ \underline y}$不是$\hat {\underline  y}$（残差不是垂线或向量不正交)。而且有：</p><script type="math/tex; mode=display">\lVert \tilde{ r} \rVert^2_{2} = \lVert  r \rVert^2_{2} + \lVert d \rVert^2_{2}\tag{4}</script><p>那么$\lVert \tilde{ r} \rVert^2&gt;\lVert  r \rVert^2$,即这不是最优的一组$w_1, w_2$。</p><h4 id="1-3-最小化残差向量"><a href="#1-3-最小化残差向量" class="headerlink" title="1.3 最小化残差向量"></a>1.3 最小化残差向量</h4><p><img src="https://i.loli.net/2020/10/25/3UkMWbDzO7TojVe.png" alt="最小化残差向量" style="zoom:80%;" /></p><p>由上面的几何证明可以知道残差向量是要与$X$的列向量张成空间垂直才最小化。那么有：</p><script type="math/tex; mode=display">X^{T} \hat {\underline r}=0\\\Rightarrow X^{T}(y-X \hat {\underline w})=0\Rightarrow X^Ty=X^TX\hat {\underline w}</script><p>接下来我们想想是不是存在$\hat {\underline w}$使得线性方程组$X^Ty=X^TX\hat {\underline w}$成立?它是不是唯一?</p><h4 id="1-4-线性方程组"><a href="#1-4-线性方程组" class="headerlink" title="1.4 线性方程组"></a>1.4 线性方程组</h4><p><img src="https://i.loli.net/2020/10/25/3InKEOy27iHwArf.png" alt="有解无解?" style="zoom:125%;" /></p><p>线性方程组是否有解？</p><p>有唯一解，无穷解，无解？</p><h4 id="1-5-线性无关"><a href="#1-5-线性无关" class="headerlink" title="1.5 线性无关"></a>1.5 线性无关</h4><p><img src="https://i.loli.net/2020/10/25/bTFMwJ7zDCjdlA8.png" alt="LI" style="zoom:100%;" /></p><p>线性无关的定义：</p><p>若向量 <script type="math/tex">v_1, v_2,\cdots,v_p \in \mathbb{R}^{n}</script>  是线性无关的，那么    <script type="math/tex">\sum^p _ {i=1}a_iv_i=0</script>  当且仅当对于所有的  <script type="math/tex">i</script> ，  <script type="math/tex">a_i=0</script>  时成立。</p><p><img src="https://i.loli.net/2020/10/25/VgasiG9fokCIbKW.png" alt="LI的判定" style="zoom:125%;" /></p><p>线性相关</p><p>这里讲了一个非常重要的概念，矩阵的秩rank。</p><p>矩阵的秩：线性无关列向量的数目=线性无关行向量的数目</p><p>如果$X^T = [x_1, x_2, \cdots, x_n] \in \mathbb{R}^{p \times n}$,那么$rank(X) \leq min(p, n)$。如果$rank(X) = min(p, n)$,那么矩阵满秩full rank。</p><p><img src="https://i.loli.net/2020/10/25/fNoOD2IRlXT7rFs.png" alt="matrix product" style="zoom:100%;" /></p><h4 id="1-6-矩阵的逆-只有方阵有逆"><a href="#1-6-矩阵的逆-只有方阵有逆" class="headerlink" title="1.6 矩阵的逆(只有方阵有逆)"></a>1.6 矩阵的逆(只有方阵有逆)</h4><p><img src="https://i.loli.net/2020/10/25/glp2dxraBH7ItPe.png" alt="matrix inverse" style="zoom:80%;" /></p><p>不是所有矩阵都有逆矩阵。只有矩阵是满秩矩阵，那么它才可能有逆矩阵。</p><p>若$X \in \mathbb{R}^{n \times p}$,假设$n \geq  p , rank(X)=p$($X$有$p$个线性无关的列或者说特征), 就有$rank(X^TX)=p \Rightarrow  X^T X$ 有逆矩阵。</p><p>理由：</p><ol><li>因为$rank(X^TX)\leq min(p, p)=p$,那么$X^TX$有$p$个线性无关的列或者说行，所以矩阵有逆。</li><li>$rank(X^TX)\leq min(p, p)=p$, $X^TX$是满秩矩阵，就有逆矩阵</li></ol><h4 id="1-7-最小二乘线性方程组是否有解"><a href="#1-7-最小二乘线性方程组是否有解" class="headerlink" title="1.7 最小二乘线性方程组是否有解?"></a>1.7 最小二乘线性方程组是否有解?</h4><p><img src="https://i.loli.net/2020/10/25/k9SChvXzWo6UYqa.png" alt="最小二乘线性方程组是否有解-投影矩阵" style="zoom:125%;" /></p><p>$X^{T}(y-X \hat {\underline w})=0<br>\Rightarrow X^Ty=X^TX\hat {\underline w}$中，由于$n \geq p$,那么$rank(X^TX)\leq min(p, p)=p$是满秩的，$X^TX$存在逆。</p><p>可以推出：</p><script type="math/tex; mode=display">\begin{aligned}\hat {\underline w}&=(X^TX)^{-1}X^Ty\\ \Rightarrow \hat {\underline y}&=X \hat {\underline w} =X(X^TX)^{-1}X^Ty\end{aligned}</script>]]></content>
      
      
      <categories>
          
          <category> CMSC35300 Mathematics Foundations of ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 最小二乘法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 2 Vectors and Matrices in Machine learning</title>
      <link href="2020/10/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture2/"/>
      <url>2020/10/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture2/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-2-机器学习中的向量和矩阵"><a href="#Lecture-2-机器学习中的向量和矩阵" class="headerlink" title="Lecture 2 机器学习中的向量和矩阵"></a>Lecture 2 机器学习中的向量和矩阵</h2><h3 id="1-线性模型"><a href="#1-线性模型" class="headerlink" title="1. 线性模型"></a>1. 线性模型</h3><p><img src="https://i.loli.net/2020/10/24/t5ajyXsZi7zUKx9.png" alt="机器学习术语" style="zoom:80%;" /></p><p>我们可以理解$X_i \in \mathbb{R}^p$为$p$个一系列的数值特征，一般写作一个列向量。</p><script type="math/tex; mode=display">{X}_{i}=\left[\begin{array}{c}X_{i 1} \\x_{i 2} \\\vdots \\X_{i p}\end{array}\right] \in \mathbb{R}^{p}\tag{1}</script><p>从训练数据，对于新的新样本$X_0$学习如何预测标签(或者目标值)$\hat{y}$。</p><p>例如线性模型，</p><script type="math/tex; mode=display">\begin{array}{l}\hat{y}=w_{1} x_{01}+w_{2} x_{02}+\cdots w_{p} x_{0 p} \\w_{1}, \ldots, w_{p}=\text { weights to be learned from data }\end{array}\tag{2}</script><p>using training data to find $W$, such that} $\hat{y_i}\approx y_i, \ for \ i=1, \ldots, n $ . want $L(\hat{y_i}, y_i) $smallest </p><p>权重向量weights vector：</p><script type="math/tex; mode=display">\underline W=\left[\begin{array}{c}\mathbb{w}_{1} \\\mathbb{w}_{2} \\\vdots \\\mathbb{w}_{p}\end{array}\right] \in \mathbb{R}^{p}\tag{3}</script><p>特征向量feature vector：</p><script type="math/tex; mode=display">\underline X=\left[\begin{array}{c}\mathbb{w}_{o 1} \\\mathbb{w}_{o 2} \\\vdots \\\mathbb{w}_{o p}\end{array}\right] \in \mathbb{R}^{p}\tag{4}</script><p>我们的模型就可以写作内积的形式。</p><h4 id="例子1-线性模型"><a href="#例子1-线性模型" class="headerlink" title="例子1 线性模型"></a>例子1 线性模型</h4><p><img src="https://i.loli.net/2020/10/24/hn9C8TiKjH4M6av.png" alt="线性模型" style="zoom:80%;" /></p><p>竖轴是第二个特征维度，横轴是第一个特征维度。蓝色直线代表所有在</p><script type="math/tex; mode=display"><X, W>=-2\times x_1+ x_2=0</script><p> 的点。</p><p>我们也可以看作： <script type="math/tex">\hat y_i = x_{i1}w_1+ x_{i2}w_2 + w_0</script> 。我们不防把第一个元素<script type="math/tex">x_1</script>看作1，那么就有：</p><script type="math/tex; mode=display">\underline{x}_{0}=\left[\begin{array}{c}1 \\x_{01} \\x_{02} \\\vdots \\x_{0 p}\end{array}\right] , \underline{w}=\left[\begin{array}{c}w_{0} \\w_{1} \\w_{2} \\\vdots \\w_{p}\end{array}\right] \in \mathbb{R}^{p+1} \tag{5}</script><p>也可以像前面一样写成内积形式。</p><p>接下来就是最小化损失函数。</p><h4 id="线性模型的矩阵表示矩阵表示"><a href="#线性模型的矩阵表示矩阵表示" class="headerlink" title="线性模型的矩阵表示矩阵表示"></a>线性模型的矩阵表示矩阵表示</h4><p><img src="https://i.loli.net/2020/10/24/TOANckqixvmW8E5.png" alt="矩阵化线性模型" style="zoom:80%;" /></p><script type="math/tex; mode=display">\begin{aligned}X&=\left[\begin{array}{ll}- & x_{1}^{\top}&- \\- & x_{2}^{\top}&- \\- & x_{n}^{\top}&-\end{array}\right]\\&=\left[\begin{array}{cccc}x_{11} & x_{12} & \cdots & x_{1 p} \\x_{21} & x_{22} & \cdots & x_{2 p} \\\vdots & & \\x_{n 1} & x_{n 2} & \cdots & x_{n p}\end{array}\right]\end{aligned}\tag{6}</script><p>例如$x<em>{21}$表示第二个训练样本的第1个特征。$x</em>{12}$表示第一个训练样本的第二个特征。</p><script type="math/tex; mode=display">X^T=  \begin{bmatrix}    x_{11}& x_{21}& \cdots  & x_{n1} \\    x_{12}& x_{22}& \cdots  & x_{n2} \\    \vdots & \vdots & \ddots & \vdots \\    x_{1p}& x_{2p}& \cdots  & x_{np}  \end{bmatrix}  \tag{7}</script><p>$X^T$变成$p$行$n$ 列。</p><p>式6，矩阵$X$的第$i $行<strong>row</strong>代表： 第$i$训练样本的$p$个特征。</p><p>​         矩阵$X$的第$j $列<strong>col</strong>代表：所有 $n$个训练样本的第$j$个特征。</p><p>这就是不同角度看待$X$矩阵。</p><p>计算$Xw$意味着：作$X$每一行和$w$作内积然后把结果用向量$\hat y$保存下来。</p><p>教授Rebecca Willett非常详细讲了矩阵行列之间的变化关系。体会矩阵体积的变化，这对代码中间的debug非常有用.</p><h4 id="例子-2：深刻理解Xw"><a href="#例子-2：深刻理解Xw" class="headerlink" title="例子 2：深刻理解Xw"></a>例子 2：深刻理解Xw</h4><p><img src="https://i.loli.net/2020/10/24/hG8q5SWUlVxbP3s.png" alt="Xw的另一种看法" style="zoom:80%;" /></p><p>上面其实是从2个角度来看矩阵乘法：行向量和列向量。</p><p>行向量角度是我们线代教材常说的，列向量有时候会让计算非常简单。(MIT线代课程，Gilbert Strang讲的非常详细。)</p><p>列向量角度：$Xw$表示$X$的列的权重之和。</p><h4 id="例子3：复杂结构的线性模型"><a href="#例子3：复杂结构的线性模型" class="headerlink" title="例子3：复杂结构的线性模型"></a>例子3：复杂结构的线性模型</h4><p><img src="https://i.loli.net/2020/10/26/5SrwFRp6y9q8nBe.png" alt="矩阵化" style="zoom:80%;" /></p><p>This doesn’t look like a straight line,but linear models can still help!</p><p>cubic polynomial：三次多项式。</p><p>范德蒙矩阵：</p><h3 id="2-矩阵与矩阵相乘"><a href="#2-矩阵与矩阵相乘" class="headerlink" title="2. 矩阵与矩阵相乘"></a>2. 矩阵与矩阵相乘</h3><h4 id="1-例子：推荐系统"><a href="#1-例子：推荐系统" class="headerlink" title="1. 例子：推荐系统"></a>1. 例子：推荐系统</h4><p><img src="https://i.loli.net/2020/10/24/GbnMQkD5A1ujYa8.png" alt="推荐系统简化表示" style="zoom:80%;" /></p><p><img src="https://i.loli.net/2020/10/24/OjI7EQWYwJzaHqb.png" alt="UV列角度看待" style="zoom:80%;" /></p><p>推荐系统</p><p>电影名 观众名 数字是评分</p><p>把矩阵$X$写作矩阵$U*V$,其中$X \in n\times p , U \in n\times r,V \in r\times p $。</p><p>矩阵$U$是对于$r$个示例电影的顾客$r$不同评分向量；$v$是每个实际顾客对$r$示例电影的相似度。</p><p>If $X= UV$，那么</p><script type="math/tex; mode=display">X_{ij} = <i^{th} \text{  row  of  U}, j^{th}\text{ col  of  V}></script><p><img src="https://i.loli.net/2020/10/25/fvtUYeNTCkxadsh.png" alt="X的列和行" style="zoom:80%;" /></p><h4 id="2-内积和外积表示"><a href="#2-内积和外积表示" class="headerlink" title="2. 内积和外积表示"></a>2. 内积和外积表示</h4><p><img src="https://i.loli.net/2020/10/25/1fRAWToulPIBMrQ.png" alt="内积和外积表示" style="zoom:80%;" /></p><h4 id="3-怎么找到最小的r"><a href="#3-怎么找到最小的r" class="headerlink" title="3.怎么找到最小的r"></a>3.怎么找到最小的r</h4><p><img src="https://i.loli.net/2020/10/25/AdVpme5LGSlt8HO.png" alt="矩阵的rank" style="zoom:80%;" /></p>]]></content>
      
      
      <categories>
          
          <category> CMSC35300 Mathematics Foundations of ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 线性模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lecture 1 Introduction</title>
      <link href="2020/10/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture1/"/>
      <url>2020/10/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80Lecture1/</url>
      
        <content type="html"><![CDATA[<h2 id="Lecture-1-Introduction"><a href="#Lecture-1-Introduction" class="headerlink" title="Lecture 1 Introduction"></a>Lecture 1 Introduction</h2><p>本节最主要就是引入机器学习的元素。教授Rebecca Willett 用一个区分笑脸的任务介绍了机器学习的基本步骤。</p><p><img src="https://i.loli.net/2020/10/24/E8ql7RHS9sbWGxu.png" alt="机器学习的基本元素" style="zoom:80%;" /></p><p><img src="https://i.loli.net/2020/10/24/qiHJnFjKYhmzUCo.png" alt="示例" style="zoom:80%;" /></p><p><img src="https://i.loli.net/2020/10/24/DvCm1GEyBgfo9VJ.png" style="zoom:80%;" /></p><p>想象我们想用一张脸部图片来决定这个人是否微笑。</p><p>关键想法：我们用一个模型表示脸是否在笑——一种数据的数学表示。</p><p>基本步骤：</p><ol><li><p>收集原始数据——例如， 脸部图像</p></li><li><p>预处理——不丢失相关信息的条件下，做数据变换来简化后面操作。例如，剪裁图像到标准尺寸，一张图一张脸，脸部在图像中间。</p></li><li><p>特征提取——通过提取模型相关的特征或特性来简化数据(基于深度神经网络的现代图像识别系统这个步骤不是必须的)。例如，每对脸部坐标的距离。</p></li><li><p>生成训练样本——我们用来学习模型的大量样本的集合。</p><ul><li>​    $(x_i, y_I) \quad for \quad i=1, \ldots, n$  其中，<script type="math/tex; mode=display">n ： 训练样本数目 \\y_i ： 第i个样本标签\\x_i ： 第i个样本特征\\x_{ij} ：第个样本的第j个特征</script></li></ul></li><li><p>选择loss function——衡量我们模型预测跟真实值之间的接近程度。 <script type="math/tex">loss = \sum^n_{i=1}\left|y_{i}-\hat{y}_{i}\right|^{2}</script></p></li><li><p>学习模型——遍历搜索候选模型或模型参数集合来找到一个能在训练集上损失最小的。</p></li><li><p>泛化误差——我们在新数据集上的的预测误差，不是开始用来训练的数据。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> CMSC35300 Mathematics Foundations of ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Backpropagation 梯度推导 台湾国立大学应用深度学习</title>
      <link href="2020/09/04/2.%20%E5%8F%B0%E6%B9%BE%E5%9B%BD%E7%AB%8B%E5%A4%A7%E5%AD%A6%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/"/>
      <url>2020/09/04/2.%20%E5%8F%B0%E6%B9%BE%E5%9B%BD%E7%AB%8B%E5%A4%A7%E5%AD%A6%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="Backpropagation-算法推导"><a href="#Backpropagation-算法推导" class="headerlink" title="Backpropagation 算法推导"></a>Backpropagation 算法推导</h2><p>本文是NTU陈蕴侬教授，应用深度学习课程反向传播算法推导的笔记。原文在：<a href="https://www.csie.ntu.edu.tw/~miulab/s108-adl/doc/200310_Backprop.pdf">200310_Backprop.pdf</a></p><h3 id="1-全连接神经网络的公式"><a href="#1-全连接神经网络的公式" class="headerlink" title="1. 全连接神经网络的公式"></a>1. 全连接神经网络的公式</h3><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog3/image-20210103142950962.png" alt="台湾国立大学-Applied Deep Learning" style="zoom:20%;" /></p><p>如图所示，全连接前馈神经网络，实现了<script type="math/tex">f: \mathbb{R}^N \to \mathbb{R}^M</script></p><script type="math/tex; mode=display">y = f(x) = \delta(W^L\cdots\delta(W^2\delta(W^1x+b^1)+b^2)\cdots+b^L)</script><p>先定义一些参数：</p><script type="math/tex; mode=display">\begin{aligned}&a_i^l: 第l层第i个神经元的输出\\ &a^l: 第l层的输出向量\\\\ &z_i^l:第l层第i个神经元激活函数的输入\\ &z^l:第l层神经元激活函数的输入向量\\\\ &w_{ij}^l:从i到j的第l层的神经元权重\\ &w^l:第l层的神经元权重向量\\\\ &b_i^l:第l层第i个神经元的偏置\\ &b^l:第l层的神经元偏置向量\end{aligned}</script><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog3/image-20210103150525036.png" alt="台湾国立大学-Applied Deep Learning" style="zoom: 20%;" /></p><p>具体来说， 先看最后一层关系——从a到z,如上图。而从z到a：如下图</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog3/image-20210103150853718.png" alt="台湾国立大学-Applied Deep Learning" style="zoom: 20%;" /></p><p>注意z到a的中间加了激活函数。这里也会产生一个梯度，$ \frac{\partial a}{ \partial z} $</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210131221454.png" alt="image-20210131221452682" style="zoom:20%;" /></p><p>全连接神经网络的前向过程。<br><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210131221644.png" alt="image-20210131221642843" style="zoom:28%;" /></p><p>对神经网络的梯度下降算法及记号。</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210131221811.png" alt="image-20210131221809835" style="zoom:20%;" /></p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210131221911.png" alt="image-20210131221905621" style="zoom:20%;" /></p><p>前向与后向传播比较：</p><ul><li>在前馈神经网络中<ul><li>前向<ul><li>信息流向从输入x到输出y</li><li>在训练中，前向传播能持续知道产生标量$C(\theta)$</li></ul></li><li>反向<ul><li>允许信息从损失反向流过整个网络，放在计算完整的梯度</li><li>适合任何函数<br><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210131222040.png" alt="image-20210131222038385" style="zoom:25%;" /></li></ul></li></ul></li></ul><p>看这里，红色所示到底代表损失函数对单一权重$w^{l}_{ij}$的梯度</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210131224839.png" alt="image-20210131224836877" style="zoom:20%;" /></p><p>具体推导：</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210131224916.png" alt="image-20210131224913140" style="zoom:20%;" /></p><p>这里就等于 </p><script type="math/tex; mode=display">\frac{\partial C(\theta)}{ \partial w_{ij}^l} = \frac{\partial C(\theta)}{ \partial z_i^l} \frac{\partial \partial z_i^l}{ \partial w_{ij}^l}</script><p>其实，就是关于$\theta$损失函数关于参数$w_{ij}^l$的梯度 可以 表达为： <strong>损失函数</strong>对于<strong>激活函数输入z的梯度</strong>，乘以 <strong>激活函数输入z</strong>对<strong>对应权重的梯度</strong>。那后面这一项等于什么呢? 下面两张张slide解释了：</p><ul><li>当层数大于1，等于对应的前一层激活函数的输出a</li><li>当层数为1时， 等于对应的训练的输入input x</li></ul><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210131231326.png" alt="image-20210131225045235" style="zoom:20%;" /></p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210131225902.png" alt="image-20210131225900693" style="zoom:20%;" /></p><p>总结一下，激活函数输出z对权重的梯度,如下。</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210131230227.png" alt="image-20210131230224382" style="zoom:20%;" /></p><p><strong>现在只要求</strong>$\frac{\partial C(\theta)}{ \partial z}$， 如下所示</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210131231520.png" alt="image-20210131231518204" style="zoom:25%;" /></p><p>其中，引入个残差符号$\delta_i^j$, 代表第l层第i个节点的梯度。而且，利用从L层到1层的传播特性。</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210131231635.png" alt="image-20210131231627348" style="zoom:20%;" /></p><p>先计算l层的梯度，因为激活函数输入z，发生$\Delta z$的变化， 激活函数的输出就会发生$\Delta a$的变化， 就等于输出y的变化$\Delta y$,引起$\Delta C$的变化。</p><p>而$    \frac{\partial C}{ \partial y_i}$, 跟损失函数有关，比如均方差损失，交叉熵损失….</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210131232508.png" alt="image-20210131232503772" style="zoom:20%;" /></p><p>而$\frac{\partial y_i}{ \partial z_i^L}$的梯度就是激活函数在$z^L$的梯度，因此总体可以看作是激活函数在$z^L$的梯度与损失函数关于输出的梯度逐元素积。</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210131232708.png" alt="image-20210131232706263" style="zoom:20%;" /></p><p>放在整个网络中看，就是损失函数对第$l+1$层每一个z的梯度乘以每个层对$l$层a的梯度乘以第$l$层a对z的梯度。红色箭头处是$\frac{\partial C}{ \partial z_k^{l+1}}$ 等于 $\delta_k^{l+1}$, 课程上有讲k写成i了。</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210131233723.png" alt="image-20210131233721367" style="zoom:20%;" /></p><p>那么轻松简化成下图中.</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210201001113.png" alt="image-20210201001111396" style="zoom:20%;" /></p><p>回想下梯度传播原理，就是每层梯度等于激活函数在$z_i$处梯度乘以(对应权重和下一层梯度)</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210201002825.png" alt="image-20210201002822378" style="zoom:20%;" /></p><p>写成矩阵形式就是如下</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210201003123.png" alt="image-20210201003120881" style="zoom:20%;" /></p><p>总体来看整个网络中，损失函数对权重的梯度等于损失函数对$l$层激活函数输入z的梯度，以及损失函数对权重的梯度</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210201003244.png" alt="image-20210201003242871" style="zoom:20%;" /></p><p>反向传播中，激活函数输入z对权重的梯度，分为1层网络和大于1层两种情况，分别如下</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210201003520.png" alt="image-20210201003517878" style="zoom:20%;" /></p><p>反向传播中每层激活函数处梯度，</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210201003727.png" alt="image-20210201003725179" style="zoom:20%;" /></p><p>优化中的梯度下降</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210201003842.png" alt="image-20210201003840083" style="zoom:20%;" /></p><p>总结:</p><p>计算损失函数关于权重的梯度基于从前向和反向传播中的两个提前计算量：</p><ul><li>前向传播中$\delta_i^l$等于，第l层激活函数在$z^l$处梯度与 权重w和下一层$\delta$的 Hadamard积(如果是最后一层就是损失函数对z的梯度)</li><li>前向传播中， 激活函数输出或输入x</li></ul><p>而这个梯度就等于上述两者相乘。(本质上是，这个神经元的输入和对应梯度的乘积，而梯度等于方向导数下降的最快方向)</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210201003928.png" alt="image-20210201003921459" style="zoom:25%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 2020 Spring NTU ADL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Backpropagation </tag>
            
            <tag> DL </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2. 二叉树</title>
      <link href="2020/08/15/2.%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
      <url>2020/08/15/2.%E4%BA%8C%E5%8F%89%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<h2 id="2-二叉树"><a href="#2-二叉树" class="headerlink" title="2. 二叉树"></a>2. 二叉树</h2><h3 id="1-二叉树基本概念"><a href="#1-二叉树基本概念" class="headerlink" title="1. 二叉树基本概念"></a>1. 二叉树基本概念</h3><h4 id="1-节点"><a href="#1-节点" class="headerlink" title="1. 节点"></a>1. 节点</h4><ol><li>节点</li><li>根节点</li><li>父节点</li><li>子节点</li><li>兄弟节点</li></ol><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog3/image-20201224153926392.png" alt="树结构示意图" width="800" /></p><p>上图中，A 节点就是 B 节点的<strong>父节点</strong>，B 节点是 A 节点的<strong>子节点</strong>。B、C、D 这三个节点的父节点是同一个节点，所以它们之间互称为<strong>兄弟节点</strong>。我们把没有父节点的节点叫作<strong>根节点</strong>，也就是图中的节点 E。我们把没有子节点的节点叫作<strong>叶子节点</strong>或者<strong>叶节点</strong>，比如图中的 G、H、I、J、K、L 都是叶子节点。</p><p>一棵树没有任何节点，称为空树</p><p>一棵树可以只有一个节点，也就是根节点</p><p>子树， 左子树， 右子树</p><h4 id="2-度"><a href="#2-度" class="headerlink" title="2. 度"></a>2. 度</h4><ul><li><p>节点的度(degree): 子树的个数</p></li><li><p>树的度： 所有节点度中的最大值</p></li><li><p>叶子节点(leaf): 度为0的节点</p></li><li><p>非叶子节点：度不为0的节点</p></li></ul><h4 id="3-层数-和-深度"><a href="#3-层数-和-深度" class="headerlink" title="3. 层数 和 深度"></a>3. 层数 和 深度</h4><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog3/image-20201224154505497.png" alt="高度、深度、层" width="800" /></p><ul><li><p>层数：根节点在第一层， 根节点的子节点在第二层，以此类推(也有从0层计算)</p></li><li><p>节点的深度：从根节点到当前节点的唯一路径上的节点总数 </p></li><li><p>节点的高度：从当前节点到最远叶子节点的路径上的节点总数(边数)</p></li><li><p>树的深度：所有节点深度中的最大值</p></li><li><p>树的高度：所有节点高度的最大值</p></li><li><p>树的深度等于树的高度</p></li></ul><h4 id="4-有序树、无序树、森林"><a href="#4-有序树、无序树、森林" class="headerlink" title="4. 有序树、无序树、森林"></a>4. 有序树、无序树、森林</h4><ul><li>有序树<ul><li>树种任意节点的子节点之间有顺序关系</li></ul></li><li>无序树<ul><li>树中任意节点的子节点之间没有顺序关系</li><li>也称为“自由树”</li></ul></li><li>森林<ul><li>由$m \ge 0$棵互不相交的树组成的集合</li></ul></li></ul><h4 id="5-二叉树-Binary-Tree"><a href="#5-二叉树-Binary-Tree" class="headerlink" title="5. 二叉树 Binary Tree"></a>5. 二叉树 Binary Tree</h4><ul><li><p>二叉树的特点</p><ul><li><p>每个节点的度最大为2(最多有2棵子树)</p></li><li><p>左子树和右子树是由顺序的</p></li><li><p>即使某节点只有一棵子树，也要分左右子树</p></li><li><p>二叉树是有序树</p></li></ul></li><li><p>二叉树的性质</p><ul><li><p>非空二叉树的第$i$层，最多有$2^{i-1} \quad (i  \ge 1)$个节点</p></li><li><p>在高度为$h$的二叉树上最多有$2^h - 1$个节点(1, 2, 4…的等比数列求和)</p></li><li><p>对于一棵非空二叉树，如果叶子节点个数为$n_0$，度为2的节点个数为$n_2$，那么$n_0 = n_2 + 1$</p><ul><li><p>证明：</p><p>​        假设度为1的节点个数为$n_1$，那么二叉树的节点总数为$n = n_0 + n_1 + n_2$；        </p><p>​        二叉树的边数为：$n_1 + 2 * n_2= \text{除了根节点，每个节点都有一条边} = n - 1= n_0+n_1+n_2$即$n_0 = n_2+1$.</p></li></ul></li></ul></li></ul><h4 id="6-真二叉树-Proper-Binary-Tree"><a href="#6-真二叉树-Proper-Binary-Tree" class="headerlink" title="6. 真二叉树     Proper Binary Tree"></a>6. 真二叉树     Proper Binary Tree</h4><pre class="mermaid">graph TB;1((1))==>2((2))1((1))==>3((3))2((2))==>4((4))2((2))==>5((5))5((5))==>6((6))5((5))==>7((7))</pre><p>所有节点的度要么为0要么为2，才为真二叉树。</p><h4 id="7-满二叉树-Full-Binary-Tree"><a href="#7-满二叉树-Full-Binary-Tree" class="headerlink" title="7. 满二叉树  Full Binary Tree"></a>7. 满二叉树  Full Binary Tree</h4><p>所有节点的度要么为0，要么为2，且所有叶子节点都在最后一层</p><pre class="mermaid">graph TB;1((1))==>2((2))1((1))==>3((2))2((2))==>4((4))2((2))==>5((5))3((3))==>6((6))3((3))==>7((7))</pre><ul><li>在同样高度的二叉树中，满二叉树的叶子节点数量最多、总结点数量最多</li><li>满二叉树一定是真二叉树，真二叉树不一定是满二叉树</li><li>假设满二叉树的高度为$h(h \ge 1)$，那么：<ul><li>第$i$层的节点数量为：$2^{i-1}$</li><li>叶子节点数量: $2^{h-1}$</li><li>总结点数量：$2^0+2^1 + \cdots + 2^{h-1}=2^{h}-1$</li></ul></li></ul><h4 id="8-完全二叉树-Complete-Binary-Tree"><a href="#8-完全二叉树-Complete-Binary-Tree" class="headerlink" title="8. 完全二叉树  Complete Binary Tree"></a>8. 完全二叉树  Complete Binary Tree</h4><p>叶子节点只会出现最后2层，且最后一层的叶子节点都靠左对齐</p><pre class="mermaid">graph TB;1((A))==>2((B))1((A))==>3((C))2((B))==>4((D))2((B))==>5((E))3((C))==>6((F))3((C))==>7((G))4((D))==>8((H))4((D))==>9((I))5((E))==>10((J))</pre><p>性质：</p><ol><li>度为1的节点只有左子树</li><li>度为1的节点要么是1个要么是0个</li><li>同样节点数量的二叉树，完全二叉树的高度最小</li><li><p>假设完全二叉树的高度为$h(h \ge 1)$，那么：</p><ul><li>至少有$2^{h-1}$个节点  $2^0+2^1+\cdots+2^{h-1}+1$</li><li>最多有$2^h-1$个节点  $2^0+2^1+ \cdots + 2^{h-1}$满二叉树  </li><li>总结点数量为$n$。其中$2^{h-1} \le n \le 2^{h}$，即$h-1 \le log_2n \le h$，<strong>因为$h$是整数，所以$h=log_2 n向下取整+1=floor(log_2 n) + 1$</strong></li></ul></li><li><p>一棵有$n$个节点的完全二叉树 $n &gt; 0$ ，从上到下，从左到右对节点从1开始进行编号，对任意第$i$个节点：<br><pre class="mermaid">  graph TB;1((1))==>2((2))1((1))==>3((3))2((2))==>4((4))2((2))==>5((5))3((3))==>6((6))3((3))==>7((7))4((4))==>8((8))4((4))==>9((9))5((5))==>10((10))</pre></p><ul><li>如果$i=1$，它是根节点</li><li>如果$i&gt;1$,它的父节点编号为$floor(1/2)$</li><li>如果$2i \le n$,它的左子节点编号为$2i$</li><li>如果$2i &gt; n$，它无左子节点</li><li>如果$2i+1 \le n$,它的右子节点编号为$2i+1$</li><li>如果$2i+1 &gt; n$,它无右子节点</li></ul></li><li><p>一棵有$n$个节点的完全二叉树 $n &gt; 0$ ，从上到下，从左到右对节点从0开始进行编号，对任意第$i$个节点：<br><pre class="mermaid">  graph TB;1((0))==>2((1))1((0))==>3((2))2((1))==>4((3))2((1))==>5((4))3((2))==>6((5))3((2))==>7((6))4((3))==>8((7))4((3))==>9((8))5((4))==>10((9))</pre></p><ul><li>如果$i=0$，它是根节点</li><li>如果$i&gt;0$,它的父节点编号为$floor((i-1)/2)$</li><li>如果$2i  + 1\le n-1$,它的左子节点编号为$2i+1$</li><li>如果$2i +1&gt; n-1$，它无左子节点</li><li>如果<script type="math/tex">2i+2 \le n -1</script>,它的右子节点编号为<script type="math/tex">2i+2</script></li><li>如果<script type="math/tex">2i+2 > n</script>,它无右子节点</li></ul></li></ol><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog3/image-20201224154803781.png" alt="满二叉树和完全二叉树" width="800" /></p><p>上图中，编号 2 的二叉树中，叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫作<strong>满二叉树</strong>。</p><p>编号 3 的二叉树中，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫作<strong>完全二叉树</strong>。</p><p>满二叉树很好理解，也很好识别，但是完全二叉树，有的人可能就分不清了。我画了几个完全二叉树和非完全二叉树的例子，你可以对比着看看。</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog3/image-20201224154938342.png" alt="完全二叉树?" width="800" /></p><p><strong>题目：</strong>一棵完全二叉树有768个节点，求叶子节点数?</p><ul><li>假设叶子节点的个数为<script type="math/tex">n_0</script>, 度为1的节点个数为<script type="math/tex">n_1</script>， 度为2的节点个数为<script type="math/tex">n_2</script><ul><li>​    那么总结点个数为<script type="math/tex">n = n_0+n_1+n_2</script>,而<script type="math/tex">n_0=n_2+1</script>。所以<script type="math/tex">n = 2n_0+n_1-1</script>。</li></ul></li><li>完全二叉树的<script type="math/tex">n_1</script>，要么为0，要么为1；<ul><li>当<script type="math/tex">n_1</script>为1时， <script type="math/tex">n=2n_0</script>, <script type="math/tex">n</script>必然是偶数。此时叶子节点个数为<script type="math/tex">n_0=n/2</script>， 非叶子节点个数为<script type="math/tex">n_1 + n_2=n/2</script>。</li><li>当<script type="math/tex">n_1</script>为0时， <script type="math/tex">n=2n_0-1</script>, <script type="math/tex">n</script>必然是奇数。此时叶子节点个数为<script type="math/tex">n_0=(n+1)/2</script>， 非叶子节点个数为<script type="math/tex">n_1 + n_2=(n-1)/2</script>。</li></ul></li></ul><p><strong>总结：</strong></p><ul><li>叶子节点个数<script type="math/tex">n_0 = floor((n+1)/2)=ceiling(n/2)</script></li><li>非叶子节点个数<script type="math/tex">n_1+n_2 = floor(n/2)=ceiling((n-1)/2)</script> <strong>floor向下取整， ceiling向上取整</strong></li></ul><p>因此上题中叶子节点的个数为384</p><h3 id="2-二叉搜索树"><a href="#2-二叉搜索树" class="headerlink" title="2. 二叉搜索树"></a>2. 二叉搜索树</h3><h4 id="1-二叉搜索树-Binary-Search-Tree"><a href="#1-二叉搜索树-Binary-Search-Tree" class="headerlink" title="1. 二叉搜索树  Binary Search Tree"></a>1. 二叉搜索树  Binary Search Tree</h4><p>二叉搜索树是二叉树的一种，是应用非常广泛的一种二叉树，简称<strong>BST</strong></p><ol><li>又称作：二叉查找树、二叉排序树树</li><li>任意一个节点的值都大于其左子树所有节点</li><li>任意一个节点的值都小于其右子树所有节点</li><li>它的左右子树也是一棵二叉搜索树</li></ol><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog2/image-20201209180616834.png" alt="二叉搜索树" width="500" /></p><p><a href="http://btv.melezinek.cz/binary-search-tree.html">可视化网址</a></p><p>二叉搜索树可以提高搜索效率，但其存储元素必须是可比较对象。不允许其元素为None</p><h4 id="2-二叉树设计"><a href="#2-二叉树设计" class="headerlink" title="2. 二叉树设计"></a>2. 二叉树设计</h4><ol><li>添加  遇到值相等，覆盖或者直接<code>return</code><ul><li>找到父节点<code>parent</code></li><li>创建新节点<code>node</code></li><li><code>parent.left = node</code> 或  <code>parent.right= node</code></li></ul></li></ol><h4 id="3-二叉树的遍历"><a href="#3-二叉树的遍历" class="headerlink" title="3. 二叉树的遍历"></a>3. 二叉树的遍历</h4><h5 id="1-前序遍历-Preorder-Traversal"><a href="#1-前序遍历-Preorder-Traversal" class="headerlink" title="1.  前序遍历 (Preorder Traversal)"></a>1.  前序遍历 (Preorder Traversal)</h5><p><pre class="mermaid">graph TB;1((7))==>2((4))1((7))==>3((9))2((4))==>4((2))2((4))==>5((5))3((9))==>6((8))3((9))==>7((11))4((2))==>8((1))4((2))==>9((3))7((11))==>10((10))7((11))==>11((12))</pre><br>访问顺序：</p><ol><li>根节点</li><li>前序遍历左子树</li><li>前序遍历右子树</li></ol><p>上面树的访问顺序为：<script type="math/tex">7 \to \underbrace{ (4\to2\to1\to3\to 5)}_{左子树}\to \underbrace{ 9\to8\to11\to10\to12}_{右子树}</script></p><h5 id="2-中序遍历-Inorder-Traversal"><a href="#2-中序遍历-Inorder-Traversal" class="headerlink" title="2. 中序遍历(Inorder Traversal)"></a>2. 中序遍历(Inorder Traversal)</h5><p>访问顺序：(根节点必须放中间，可以先右后左)</p><ol><li>中序遍历左子树</li><li>中序遍历根节点</li><li>中序遍历右子树</li></ol><p>上面树的访问顺序为：<script type="math/tex">1 \to 2\to 3\to 4\to 5\to 7\to 8\to 9\to 10\to 11\to12</script>。对于二叉搜索树，从小到大顺序排列，升序。如果先右后左，就变为降序。</p><h5 id="3-后序遍历-Postorder-Traversal"><a href="#3-后序遍历-Postorder-Traversal" class="headerlink" title="3. 后序遍历(Postorder Traversal)"></a>3. 后序遍历(Postorder Traversal)</h5><p>访问顺序：(根节点放最后，左右子树可以互换)</p><ol><li>后序遍历左子树</li><li>后序遍历右子树</li><li>后序遍历根节点</li></ol><p>上面树的访问顺序为：<script type="math/tex">1 \to 3\to 2\to 5\to 4\to 8\to 10\to 12\to 11\to 9\to7</script>。</p><h5 id="4-层序遍历-Level-Order-Traversal"><a href="#4-层序遍历-Level-Order-Traversal" class="headerlink" title="4. 层序遍历(Level Order Traversal)"></a>4. 层序遍历(Level Order Traversal)</h5><p>访问顺序：<br>            从上到下从左到右。<br>            上面树的访问顺序为：<script type="math/tex">7 \to 4\to 9\to 2\to 5\to 8\to 11\to 1\to 3\to 10\to12</script>。</p><h5 id="5-遍历的作用"><a href="#5-遍历的作用" class="headerlink" title="5. 遍历的作用"></a>5. 遍历的作用</h5><ol><li>前序遍历：树状结构的展示</li><li>中序遍历：二叉搜索树的中序遍历按升序或降序处理节点</li><li>后续遍历：适用于一些先子后父的操作</li><li>层序遍历:  计算二叉树高度，判断二叉树为完全二叉树</li></ol><h4 id="4-二叉搜索树的复杂度分析"><a href="#4-二叉搜索树的复杂度分析" class="headerlink" title="4. 二叉搜索树的复杂度分析"></a>4. 二叉搜索树的复杂度分析</h4><p>如果按照7、4、9、2、5、8、11的顺序添加节点，形成满二叉树如下图</p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog2/image-20201209205738371.png" alt="image-20201209205738371" style="zoom:45%;" /></p><p>那么搜索，删除，添加复杂度为$O(h)=O(logn)$等于树的高度。</p><p>如果按照2、5、4、7、8、9、11添加节点，形成链表：$O(h)=O(n)$。</p><h3 id="3-平衡二叉树"><a href="#3-平衡二叉树" class="headerlink" title="3. 平衡二叉树"></a>3. 平衡二叉树</h3><ol><li>平衡<br>当节点数量固定是，左右子树的高度越接近，这棵树就越平衡。</li><li>理想平衡<br>最理想的平衡就是像完全二叉树、满二叉树那样，高度是最小的</li></ol><h4 id="1-平衡二叉搜索树-Balanced-Binary-Search-Tree"><a href="#1-平衡二叉搜索树-Balanced-Binary-Search-Tree" class="headerlink" title="1. 平衡二叉搜索树(Balanced Binary Search Tree)"></a>1. 平衡二叉搜索树(Balanced Binary Search Tree)</h4><ol><li><p>经典常见的平衡二叉树搜索树有:</p><ul><li><p>AVL， Windows NT内核中广泛应用</p></li><li><p>红黑树：</p><ul><li><p>C++ STL中map set</p></li><li><p>Java中TreeMap、TreeSet、HashMap、HashSet</p></li><li>Linux的进程调度</li></ul></li></ul></li></ol><h4 id="2-AVL树"><a href="#2-AVL树" class="headerlink" title="2. AVL树"></a>2. AVL树</h4><ol><li>平衡因子(Balance Factor)：某节点的左右子树的高度差</li></ol><ol><li>AVL树的特点:<ul><li>每个节点的平衡因子只能是1、0、-1(故其绝对值$\le1$，如果超过1，称之为“失衡”)</li><li>每个节点的左右子树高度差不超过1</li><li>添加、搜索、删除时间复杂度是<code>O(logn)</code></li></ul></li><li>最小不平衡树:<ul><li>距离插入节点最近的，且平衡因子的绝对值大于1的节点为根的子树，我们称为最小不平衡树。</li></ul></li></ol><h4 id="3-左单旋-Left-Rotation"><a href="#3-左单旋-Left-Rotation" class="headerlink" title="3. 左单旋 Left Rotation"></a>3. 左单旋 Left Rotation</h4><p>平衡二叉树构建的基本思想就是在构建二叉排序树的过程中，每当插入一个结点时，先检查是否因插入而破坏了树的平衡性，若是，则找出最小不平衡子树。在保持二叉排序树特性的前提下，调整最小不平衡子树中各结点之间的链接关系，进行相应的旋转，使之成为新的平衡子树。</p><ol><li><strong>左旋</strong>：将右子树的左子树链接到父节点的右子树节点， 父节点作为新根节点的左子树节点。如下三图所示，</li></ol><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210201123000.png" alt="image-20210201122958880" width="800" style="zoom: 50%;" /></p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210201123032.png" alt="image-20210201123030650" width="800" style="zoom: 50%;" /></p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210201123102.png" alt="image-20210201123100308" width="800" style="zoom: 50%;" /></p><h4 id="4-右单旋-Right-Rotation"><a href="#4-右单旋-Right-Rotation" class="headerlink" title="4. 右单旋 Right Rotation"></a>4. 右单旋 Right Rotation</h4><ol><li><strong>右旋</strong>：右单旋是左单旋的镜像旋转.<br>将左子树的右子树链接到父节点的左子树节点，父节点作为新根节点的右子树节点。如下三图所示，</li></ol><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210201123330.png" alt="image-20210201123326326" width="800" style="zoom: 50%;" /></p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210201123239.png" alt="image-20210201123237836" width="800" style="zoom: 50%;" /></p><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog5/20210201123405.png" alt="image-20210201123403582" width="800" style="zoom: 50%;" /></p><p>左旋——自己变为右孩子的左孩子；右旋——自己变为左孩子的右孩子</p><p>[1] <a href="https://baozoulin.gitbook.io/-data-structure/di-8-zhang-cha-zhao/86ping-heng-er-cha-shu-ff08-avl-shu-ff09">大话数据结构笔记</a></p>]]></content>
      
      
      <categories>
          
          <category> Data Structure and Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Structure and Algorithm </tag>
            
            <tag> tree </tag>
            
            <tag> BST </tag>
            
            <tag> AVL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Chapter 06 Probability and Distributions</title>
      <link href="2020/07/04/Chapter06-Probability-and-Distributions/"/>
      <url>2020/07/04/Chapter06-Probability-and-Distributions/</url>
      
        <content type="html"><![CDATA[<h2 id="Chapter-06-Probability-and-Distributions"><a href="#Chapter-06-Probability-and-Distributions" class="headerlink" title="Chapter 06: Probability and Distributions"></a>Chapter 06: Probability and Distributions</h2><ul><li><p>中英名词对照：</p><p>PMF：Probability Mass Function(概率质量函数)</p><p>PDF： Probability Density Function(概率密度函数)</p><p>CDF：Cumulative Distribution Function (累积分布函数)</p><p>i.i.d：Independent and identically distributed</p><p>median: 中位数</p><p>mode： 众数</p></li></ul><h3 id="1-Sum-Rule-Product-Rule-and-Bayes’-Theorem"><a href="#1-Sum-Rule-Product-Rule-and-Bayes’-Theorem" class="headerlink" title="1. Sum Rule, Product Rule, and Bayes’ Theorem"></a>1. Sum Rule, Product Rule, and Bayes’ Theorem</h3><ul><li><p><strong>sum rule</strong>:</p><script type="math/tex; mode=display">p(\boldsymbol{x})=\left\{\begin{array}{ll}\sum_{\boldsymbol{y} \in \mathcal{Y}} p(\boldsymbol{x}, \boldsymbol{y}) & \text { if } \boldsymbol{y} \text { is discrete } \\\int_{\mathcal{Y}} p(\boldsymbol{x}, \boldsymbol{y}) \mathrm{d} \boldsymbol{y} & \text { if } \boldsymbol{y} \text { is continuous }\end{array}\right. \tag{1}</script></li><li><p><strong>product rule</strong></p><script type="math/tex; mode=display">p(\boldsymbol{x}) = P \left( \boldsymbol{y} \left|\boldsymbol{x}\right. \right) p(\boldsymbol{x}) \tag{2}</script></li></ul><ul><li><strong>Bayes’ theorem:</strong></li></ul><script type="math/tex; mode=display">\underbrace{p(\boldsymbol{x} \mid \boldsymbol{y})}_{\text {posterior }}=\frac{\overbrace{p(\boldsymbol{y} \mid \boldsymbol{x})}^{\text {likelihood }} \overbrace{p(\boldsymbol{x})}^{\text {prior }}}{\underbrace{p(\boldsymbol{y})}_{\text {evidence }}} \tag{3}</script><p><strong>后验概率正比于似然 $\times$ 先验概率</strong></p><p><strong>likelihood 也可称作measurement model</strong></p><p>  假设我们有一些关于隐变量$\boldsymbol{x}$先验$p(\boldsymbol{x})$，以及关于$\boldsymbol{x}$和第二个变量$\boldsymbol{y}$之间的关系$p \left( \boldsymbol{y} \left|\boldsymbol{x}\right. \right)$。如果我们观察$\boldsymbol{y}$,我们可以使用Bayes’理论来得到公式3.</p><p>  $p(\boldsymbol{x})$，封装了我们在观察任何数据之前对隐变量$\boldsymbol{x}$主观先验知识。我们可以选择任何对我有意义的先验知识，但至关重要的是确保先验在所有可能的$\boldsymbol{x}$有非零的pdf,即使其非常罕见。</p><p>  似然${p(\boldsymbol{y} \mid \boldsymbol{x})}$描述$\boldsymbol{x}$和$\boldsymbol{y}$直接的关系，并且在离散概率分布情况下，如果我们知道隐变量$\boldsymbol{x}$,它是数据$\boldsymbol{y}$的概率。注意，似然不是$\boldsymbol{x}$的分布，而是$\boldsymbol{y}$。<strong>我们称${p(\boldsymbol{y} \mid \boldsymbol{x})}$为给定$\boldsymbol{y}$的$\boldsymbol{x}$的似然，或给定$\boldsymbol{x}$的$\boldsymbol{y}$的概率， 但绝对不能称$\boldsymbol{y}$的似然</strong>。</p><p>后验${p(\boldsymbol{x} \mid \boldsymbol{y})}$得益于大量Bayes统计学，因为它准确表达了我们在观察$\boldsymbol{y}$后了解$\boldsymbol{x}$的兴趣。</p><blockquote><p>根据贝叶斯定理公式3，一个随机变量在给定另一随机变量值之后的后验概率分布可以通过先验概率分布与似然函数相乘并除以归一化常数求得<br>: </p><script type="math/tex; mode=display">f_{X\mid Y=y}(x)={f_X(x) L_{X\mid Y=y}(x) \over {\int_{-\infty}^\infty f_X(u) L_{X\mid Y=y}(u)\,du}} \tag{4}</script><p>上式为给出了随机变量$X$在给定数据$Y=y$后的后验概率分布函数，式中</p><ul><li><script type="math/tex">f_X(x)></script>  为  <script type="math/tex">X</script>  的先验密度函数，</li><li><script type="math/tex">L_{X\mid Y=y}(x) = f_{Y\mid X=x}(y)</script>   为   <script type="math/tex">x</script>   的似然函数，</li><li><script type="math/tex">\int_{-\infty}^\infty f_X(u) L_{X\mid Y=y}(u)du</script>   为归一化常数，</li><li><script type="math/tex">f_{X\mid Y=y}(x)</script> 为考虑了数据 <script type="math/tex">Y=y</script> 后 <script type="math/tex">X</script> 的后验密度函数。</li></ul></blockquote><p><a href="https://en.wikipedia.org/wiki/Posterior_probability">注:引用自维基/后验概率</a></p><h4 id="1-Likelihood-function"><a href="#1-Likelihood-function" class="headerlink" title="1. Likelihood function"></a>1. Likelihood function</h4><blockquote><p>In statistics, the likelihood function often simply called the likelihood <strong>measures the goodness of fit of a statistical model to a sample of data for given values of the unknown parameters</strong>. It is formed from the joint probability distribution of the sample, but viewed and used as a function of the parameters only, thus treating the random variables as fixed at the observed values.</p><p>在数理统计学中，<strong>似然函数</strong>是一种关于统计模型中的<strong>参数</strong>的<strong>函数</strong>，表示模型参数中的<strong>似然性</strong>。似然函数在统计推断中有重大作用，如在最大似然估计和费雪信息之中的应用等等。“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性，但是在统计学中，“似然性”和“概率”（或然性）又有明确的区分：<u>概率，用于在已知一些参数的情況下，预测接下来在观测上所得到的结果；似然性，则是用于在已知某些观测所得到的结果时，对有关事物之性质的参数进行估值</u>。</p><p>在这种意义上，似然函数可以理解为条件概率的逆反。在已知某个参数$B$时，事件$A$会发生的概率写作：</p><script type="math/tex; mode=display">P(A \mid B) = \frac{P(A , B)}{P(B)}</script><p>利用贝叶斯定理，</p><script type="math/tex; mode=display">P(B \mid A) = \frac{P(A \mid B)\;P(B)}{P(A)}</script><p>因此，我们可以反过来构造表示似然性的方法：已知有事件$A$发生，运用似然函数$\mathbb{L}(B \mid A)$，我们估计参数$B$的可能性。形式上，似然函数也是一种条件概率函数，但我们关注的变量改变了：</p><script type="math/tex; mode=display">b\mapsto P(A \mid B=b)</script><p>注意到这里并不要求似然函数满足归一性：$\sum_{b \in \mathcal{B}}P(A \mid B=b) = 1$。一个似然函数乘以一个正的常数之后仍然是似然函数。对所有$\alpha &gt; 0$，都可以有似然函数:</p><script type="math/tex; mode=display">L(b \mid A) = \alpha \; P(A \mid B=b)</script></blockquote><p><a href="[https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0](https://zh.wikipedia.org/wiki/似然函数">注:引用自维基/似然函数</a>)</p><h4 id="2-边缘似然-证据"><a href="#2-边缘似然-证据" class="headerlink" title="2.  边缘似然/证据"></a>2.  边缘似然/证据</h4><ul><li>the marginal likehood/evidence：</li></ul><script type="math/tex; mode=display">p(\boldsymbol{y}):=\int p(\boldsymbol{y} \mid \boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}=\mathbb{E}_{X}[p(\boldsymbol{y} \mid \boldsymbol{x})] \tag{5}</script><p>定义上，边缘似然是对应隐变量$x$数值积分。</p><blockquote><p>在统计学中， 边缘似然函数（marginal likelihood function），或积分似然（integrated likelihood），是一个某些参数变量边缘化的似然函数（likelihood function） 。在贝叶斯统计范畴，它也可以被称作为 证据 或者 模型证据 的。</p></blockquote><p><a href="https://zh.wikipedia.org/wiki/边缘似然">引用:维基/边缘似然</a></p><h3 id="2-均值和协方差"><a href="#2-均值和协方差" class="headerlink" title="2. 均值和协方差"></a>2. 均值和协方差</h3><ul><li>Means and Covariances</li></ul><p>均值和(协)方差经常用来描述概率分布的性质(期望值和范围)。</p><p>期望值的概念是机器学习的核心，并且概率本身的基础概念也源自于它。</p><p>​                                                                                                                        ——(Whittle, 2000).</p><h4 id="1-期望值"><a href="#1-期望值" class="headerlink" title="1. 期望值"></a>1. 期望值</h4><ul><li>Expected Value</li></ul><p>期望值函数$g: \mathbb{R} \rightarrow \mathbb{R}$单一连续随机变量$X \sim p(x)$给定如下：</p><script type="math/tex; mode=display">\mathbb{E}_{X}[g(x)]=\int_{\mathcal{X}} g(x) p(x) \mathrm{d} x \tag{6}</script><p>对应离散随机变量$X \sim p(x)$:</p><script type="math/tex; mode=display">\mathbb{E}_{X}[g(x)]=\sum_{x \in \mathcal{X}} g(x) p(x) \tag{7}</script><p>$\mathcal{X}$：随机变量$X$可能结果的集合 target space</p><p><strong>注意：</strong></p><p>我们把多维随机变量$X$看作有限的单一随机变量的向量$[X_1, \cdots, X_D]^{T}$。对于多维随机变量,定义基于元素的期望值：</p><script type="math/tex; mode=display">\mathbb{E}_{X}[g(\boldsymbol{x})]=\left[\begin{array}{c}\mathbb{E}_{X_{1}}\left[g\left(x_{1}\right)\right] \\\vdots \\\mathbb{E}_{X_{D}}\left[g\left(x_{D}\right)\right]\end{array}\right] \in \mathbb{R}^{D} \tag{8}</script><p>其中：  <script type="math/tex">{\mathbb{E}_{X}}_{d}</script>   表示对应向量  <script type="math/tex">x</script>  的第   <script type="math/tex">d</script>  个元素的期望值。</p><h4 id="2-均值"><a href="#2-均值" class="headerlink" title="2. 均值"></a>2. 均值</h4><p>对于状态量<script type="math/tex">x \in \mathbb{R}^{D}</script> 随机变量 <script type="math/tex">X</script> 的均值是一个平均值，其定义如下：</p><script type="math/tex; mode=display">\begin{align}&\mathbb{E}_{X}[\boldsymbol{x}]=\left[\begin{array}{c}\mathbb{E}_{X_{1}}\left[x_{1}\right] \\\vdots \\\mathbb{E}_{X_{D}}\left[x_{D}\right]\end{array}\right] \in \mathbb{R}^{D}, \\ &\text{where}\\\\ &\mathbb{E}_{x_{d}}\left[x_{d}\right]:=\left\{\begin{array}{ll}\int_{\mathcal{X}} x_{d} p\left(x_{d}\right) \mathrm{d} x_{d} & \text { if } X \text { is a continuous random variable } \\ \sum_{x_{i} \in \mathcal{X}} x_{i} p\left(x_{d}=x_{i}\right) & \text { if } X \text { is a discrete random variable }\end{array}\right. \tag{9}\end{align}</script><p>其中，$d = 1, \cdots, D$;$d$表示对应$x$的维数。遍历随机变量$x$目标空间的状态量$\mathcal{X}$积分或求和。</p><h4 id="3-协方差-Covariance-定义及推导"><a href="#3-协方差-Covariance-定义及推导" class="headerlink" title="3. 协方差(Covariance)定义及推导"></a>3. 协方差(Covariance)定义及推导</h4><p>两个多维随机变量$X, Y \in \mathbb{R}$之间的协方差，给定为它们与均值的偏差的积的期望：</p><script type="math/tex; mode=display">\operatorname{Cov}_{X, Y}[x, y]:=\mathbb{E}_{X, Y}\left[\left(x-\mathbb{E}_{X}[x]\right)\left(y-\mathbb{E}_{Y}[y]\right)\right] \tag{10}\</script><p>因为期望计算是线性的，对于一个实值函数$f(\boldsymbol{x}) = ag(\boldsymbol{x}) + bh(\boldsymbol{x})$， 当$a, b \in \mathbb{R} 且 \boldsymbol{x} \in \mathbb{R}^{D}$我们可以得到：</p><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}_{X}[f(\boldsymbol{x})] &=\int f(\boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} \\&=\int[a g(\boldsymbol{x})+b h(\boldsymbol{x})] p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} \\&=a \int g(\boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} x+b \int h(\boldsymbol{x}) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} \\&=a \mathbb{E}_{X}[g(\boldsymbol{x})]+b \mathbb{E}_{X}[h(\boldsymbol{x})]\end{aligned} \tag{11}</script><p>所以，公式10可以有如下推导：</p><script type="math/tex; mode=display">\begin{align}\operatorname{Cov}_{X, Y}[x, y]&=\mathbb{E}_{X, Y}\left[\left(x-\mathbb{E}_{X}[x]\right)\left(y-\mathbb{E}_{Y}[y]\right)\right]\\ &=\mathbb{E}_{X, Y}\left[\left(xy - y\mathbb{E}_{X}[x] - x\mathbb{E}_{Y}[y] +  \mathbb{E}_{X}[x] \mathbb{E}_{Y}[y]\right)\right]\\ &= \mathbb{E}(xy) -\mathbb{E}[x]\mathbb{E}[y] -\mathbb{E}[x]\mathbb{E}[y] + \mathbb{E}[x]\mathbb{E}[y]\\ &= \mathbb{E}(xy) -\mathbb{E}[x]\mathbb{E}[y] \end{align}</script><p>即：</p><script type="math/tex; mode=display">\begin{align}\operatorname{Cov}_{X, Y}[x, y] = \mathbb{E}(xy) -\mathbb{E}[x]\mathbb{E}[y]  \tag{12}\end{align}</script><h4 id="4-多元随机变量的方差"><a href="#4-多元随机变量的方差" class="headerlink" title="4. 多元随机变量的方差"></a>4. 多元随机变量的方差</h4><ul><li>Covariance  (Multivariate)</li></ul><p>两元随机变量$X$和$Y$及其对应状态$\boldsymbol{x} \in \mathbb{R}^{D} 和 \boldsymbol{y} \in \mathbb{R}^{D}$，$X和 Y$之间的协方差为：</p><script type="math/tex; mode=display">\operatorname{Cov}[\boldsymbol{x}, \boldsymbol{y}] = \mathbb{E}[\boldsymbol{x}\boldsymbol{y}^{T}] - \mathbb{E}[\boldsymbol{x}]\mathbb{E}[\boldsymbol{y}]^{T} = \operatorname{Cov}[\boldsymbol{y}, \boldsymbol{x}]^{T} \in \mathbb{R}^{D \times E} \tag{13}</script><h4 id="5-方差"><a href="#5-方差" class="headerlink" title="5. 方差"></a>5. 方差</h4><ul><li>Variance</li></ul><p>带有状态$\boldsymbol{x} \in \mathbb{R}^{D}$和均值向量$\boldsymbol{\mu} \in \mathbb{R}^{D}$的随机变量$X$方差定义为：</p><script type="math/tex; mode=display">\begin{aligned}\mathbb{V}_{X}[\boldsymbol{x}] &=\operatorname{Cov}_{X}[\boldsymbol{x}, \boldsymbol{x}] \\&=\mathbb{E}_{X}\left[(\boldsymbol{x}-\boldsymbol{\mu})(\boldsymbol{x}-\boldsymbol{\mu})^{\top}\right]=\mathbb{E}_{X}\left[\boldsymbol{x} \boldsymbol{x}^{\top}\right]-\mathbb{E}_{X}[\boldsymbol{x}] \mathbb{E}_{X}[\boldsymbol{x}]^{\top} \\&=\left[\begin{array}{cccc}\operatorname{Cov}\left[x_{1}, x_{1}\right] & \operatorname{Cov}\left[x_{1}, x_{2}\right] & \ldots & \operatorname{Cov}\left[x_{1}, x_{D}\right] \\\operatorname{Cov}\left[x_{2}, x_{1}\right] & \operatorname{Cov}\left[x_{2}, x_{2}\right] & \ldots & \operatorname{Cov}\left[x_{2}, x_{D}\right] \\\vdots & \vdots & \ddots & \vdots \\\operatorname{Cov}\left[x_{D}, x_{1}\right] & \ldots & \ldots & \operatorname{Cov}\left[x_{D}, x_{D}\right]\end{array}\right]\end{aligned} \tag{14}</script><p>$D \times D$的矩阵被称作多元随机变量的协方差矩阵 covariance matrix 。协方差矩阵是对称和半正定的，并且告诉我们数据范围的信息。在对角线上，协方差矩阵含有边缘方差：</p><script type="math/tex; mode=display">p\left(x_{i}\right)=\int p\left(x_{1}, \ldots, x_{D}\right) \mathrm{d} x \backslash_{i}p\left(x_{i}\right)=\int p\left(x_{1}, \ldots, x_{D}\right) \mathrm{d} x \backslash_{i} \tag{15}</script><p>$\backslash_{i} $表示”除了$i$之外的所有变量”。非对角线元素是互协方差项$\operatorname {Cov}[x_i, y_i]$,$i, j = 1, \cdots, D, i \ne j$。</p><h4 id="6-相关"><a href="#6-相关" class="headerlink" title="6. 相关"></a>6. 相关</h4><ul><li>Correlation</li></ul><p>两个随机变量$X,Y$之间的相关性定义为：</p><script type="math/tex; mode=display">\operatorname{corr}[x, y]=\frac{\operatorname{Cov}[x, y]}{\sqrt{\mathbb{V}[x] \mathbb{V}[y]}} \in[-1,1] \tag{16}</script><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/07/02/RTL5JkI9d1GzbQs.png?=raw" width=90% height=90%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">正负相关示意图 引用自https://mml-book.com</div> </center><p>上图中表明，正相关$corr[x, y]$意味着当$x$变大，$y$也随之变大。负相关$corr[x, y]$意味着当$x$变大，$y$也随之变小。</p><h3 id="3-经验均值和协方差"><a href="#3-经验均值和协方差" class="headerlink" title="3. 经验均值和协方差"></a>3. 经验均值和协方差</h3><ul><li>Empirical Means and Covariances</li></ul><p>在机器学习中，我们需要从数据中学习经验的观察结果。</p><p>我们用有限的数据集 size N 来构建经验统计，它是一个同一随机变量有限数值的函数。</p><p>我们观察数据，就是我们看每一个随机变量$x_1, x_2, \cdots, x_N$之间的联系和应用经验统计。</p><p>具体来说,对于均值，给定一个特殊的数据集我们获得均值的估计，这称作经验均值或者样本均值sample mean .经验协方差也一样适用。</p><h4 id="1-经验均值和协方差"><a href="#1-经验均值和协方差" class="headerlink" title="1. 经验均值和协方差"></a>1. 经验均值和协方差</h4><p>经验均值向量是每一个观察变量的算术平均,其定义如下:</p><script type="math/tex; mode=display">\begin{align}\overline{\boldsymbol{x}}:=\frac{1}{N} \sum_{n=1}^{N} \boldsymbol{x}_{n} \tag{17} \\ \\ \text{where } {\boldsymbol{x}_{n} \in \mathbb{R}^{D}}\end{align}</script><p>跟经验均值一样,经验协方差矩阵$D \times D$定义为：</p><script type="math/tex; mode=display">\Sigma:=\frac{1}{N} \sum_{n=1}^{N}\left(x_{n}-\bar{x}\right){\left(x_{n}-\bar{x}\right)}\top \tag{18}</script><p>为了计算特定数据集的统计信息，我们用观察量 <script type="math/tex">\boldsymbol{x}_1, \cdots, \boldsymbol{x}_{N}</script> ，biased estimate 代入公式17.18来计算。经验协方差矩阵是对称半正定的。</p><h4 id="2-方差的3种表达式"><a href="#2-方差的3种表达式" class="headerlink" title="2. 方差的3种表达式"></a>2. 方差的3种表达式</h4><h5 id="1-标准方差定义"><a href="#1-标准方差定义" class="headerlink" title="1. 标准方差定义"></a>1. 标准方差定义</h5><p>对应协方差定义，它是随机变量$X$与其期望值$\mu$偏差平方的期望：</p><script type="math/tex; mode=display">\mathbb{V}_{X}[x]:=\mathbb{E}_{X}\left[(x-\mu)^{2}\right] \tag{19}</script><h5 id="2-方差的原始分数公式"><a href="#2-方差的原始分数公式" class="headerlink" title="2. 方差的原始分数公式"></a>2. 方差的原始分数公式</h5><script type="math/tex; mode=display">\mathbb{V}_{X}[x]=\mathbb{E}_{X}\left[x^{2}\right]-\left(\mathbb{E}_{X}[x]\right)^{2} \tag{20}</script><p>记作：“平方的均值减去均值的平方”。</p><h5 id="3-The-sum-of-N-2-pairwise-differences-is-the-empirical-variance-of-the-observations"><a href="#3-The-sum-of-N-2-pairwise-differences-is-the-empirical-variance-of-the-observations" class="headerlink" title="3. The sum of $N^{2}$ pairwise differences is the empirical variance of the observations"></a>3. The sum of $N^{2}$ pairwise differences is the empirical variance of the observations</h5><script type="math/tex; mode=display">\frac{1}{N^{2}} \sum_{i, j=1}^{N}\left(x_{i}-x_{j}\right)^{2}=2\left[\frac{1}{N} \sum_{i=1}^{N} x_{i}^{2}-\left(\frac{1}{N} \sum_{i=1}^{N} x_{i}\right)^{2}\right] \tag{21}</script><p>当它用到随机变量的仿射变换时，均值和(协)方差表现一些有用的特性。考虑均值为$\boldsymbol{\mu}$、协方差矩阵$\boldsymbol{\Sigma}$随机变量$X$和关于一个$\boldsymbol{x}$(决定论的)仿射变换$\boldsymbol{y} = \boldsymbol{A}\boldsymbol{x} + \boldsymbol{b}$。随机变量$\boldsymbol{y}$，其均值向量和协方差矩阵定义为:</p><script type="math/tex; mode=display">\begin{array}{l}\mathbb{E}_{Y}[\boldsymbol{y}]=\mathbb{E}_{X}[\boldsymbol{A} \boldsymbol{x}+\boldsymbol{b}]=\boldsymbol{A} \mathbb{E}_{X}[\boldsymbol{x}]+\boldsymbol{b}=\boldsymbol{A} \boldsymbol{\mu}+\boldsymbol{b}  \\  \mathbb{V}_{Y}[\boldsymbol{y}]=\mathbb{V}_{X}[\boldsymbol{A} \boldsymbol{x}+\boldsymbol{b}]=\mathbb{V}_{X}[\boldsymbol{A} \boldsymbol{x}]=\boldsymbol{A} \mathbb{V}_{X}[\boldsymbol{x}] \boldsymbol{A}^{\top}=\boldsymbol{A} \boldsymbol{\Sigma} \boldsymbol{A}^{\top}  \tag{22}\end{array}</script><p>而且:</p><script type="math/tex; mode=display">\begin{align}\operatorname{Cov}[\boldsymbol{x}, \boldsymbol{y}] &=\mathbb{E}\left[\boldsymbol{x}(\boldsymbol{A} \boldsymbol{x}+\boldsymbol{b})^{\top}\right]-\mathbb{E}[\boldsymbol{x}] \mathbb{E}[\boldsymbol{A} \boldsymbol{x}+\boldsymbol{b}]^{\top} \\&=\mathbb{E}[\boldsymbol{x}] \boldsymbol{b}^{\top}+\mathbb{E}\left[\boldsymbol{x} \boldsymbol{x}^{\top}\right] \boldsymbol{A}^{\top}-\boldsymbol{\mu} \boldsymbol{b}^{\top}-\boldsymbol{\mu} \boldsymbol{\mu}^{\top} \boldsymbol{A}^{\top} \\&=\boldsymbol{\mu} \boldsymbol{b}^{\top}-\boldsymbol{\mu} \boldsymbol{b}^{\top}+\left(\mathbb{E}\left[\boldsymbol{x} \boldsymbol{x}^{\top}\right]-\boldsymbol{\mu} \boldsymbol{\mu}^{\top}\right) \boldsymbol{A}^{\top} \\& = \boldsymbol{\Sigma} \boldsymbol{A}^{\top} \tag{23}\end{align}</script><p>这里，$\boldsymbol{\Sigma} = \mathbb{E}[\boldsymbol{x}\boldsymbol{x}\top] - \boldsymbol{\mu}\boldsymbol{\mu}\top$是$\boldsymbol{X}$的协方差。</p><h4 id="3-Statistical-Independence-独立"><a href="#3-Statistical-Independence-独立" class="headerlink" title="3. Statistical Independence(独立)"></a>3. Statistical Independence(独立)</h4><h5 id="1-独立"><a href="#1-独立" class="headerlink" title="1. 独立"></a>1. 独立</h5><p>两个随机变量$X, Y$在统计上独立当且仅当:</p><script type="math/tex; mode=display">p(\boldsymbol{x}, \boldsymbol{y}) = p(\boldsymbol{x})p(\boldsymbol{y}) \tag{24}</script><p>直觉地,如果$\boldsymbol{y}$的值不增加$\boldsymbol{x}$任何信息，两个随机变量$X, Y$独立。</p><p>如果$X, Y$统计上独立,有：</p><script type="math/tex; mode=display">\begin{array}{l}p(\boldsymbol{y} \mid \boldsymbol{x})=p(\boldsymbol{y}) \\p(\boldsymbol{x} \mid \boldsymbol{y})=p(\boldsymbol{x}) \\\mathbb{V}_{X, Y}[\boldsymbol{x}+\boldsymbol{y}]=\mathbb{V}_{X}[\boldsymbol{x}]+\mathbb{V}_{Y}[\boldsymbol{y}] \\\operatorname{Cov}_{X, Y}[\boldsymbol{x}, \boldsymbol{y}]=\mathbf{0}\end{array}</script><h5 id="2-条件独立-Conditional-Independence"><a href="#2-条件独立-Conditional-Independence" class="headerlink" title="2.条件独立 Conditional Independence"></a>2.条件独立 Conditional Independence</h5><p>给定$Z$，两个随机变量$X, Y$条件独立,当且仅当:</p><script type="math/tex; mode=display">p(\boldsymbol{x}, \boldsymbol{y} \mid \boldsymbol{z})=p(\boldsymbol{x} \mid \boldsymbol{z}) p(\boldsymbol{y} \mid \boldsymbol{z}) \quad \text { for all } \quad z \in \mathcal{Z} \tag{25}</script><p>$\mathcal{Z}$是随机变量$Z$的状态量集合.$X \perp Y \mid Z$表示给定$Z$情况下$X$条件独立$Y$,或者“我们已知$z$,关于$y$的信息不改变$x$的信息”。</p><p>公式25可以理解为“给定关于$z$的信息,  $x \ and \ y$  的分布的分解”。运用概率的积的法则,公式25左边展开可以得到:</p><script type="math/tex; mode=display">p(\boldsymbol{x}, \boldsymbol{y} \mid \boldsymbol{z})=p(\boldsymbol{x} \mid \boldsymbol{y}, \boldsymbol{z}) p(\boldsymbol{y} \mid \boldsymbol{z}) \tag{26}</script><p>比较公式25、26右边部分,我们得到：</p><script type="math/tex; mode=display">p(\boldsymbol{x} \mid \boldsymbol{y}, \boldsymbol{z}) = p(\boldsymbol{x} \mid \boldsymbol{z}) \tag{27}</script><h4 id="4-随机变量的内积"><a href="#4-随机变量的内积" class="headerlink" title="4. 随机变量的内积"></a>4. 随机变量的内积</h4><h5 id="1-两个随机变量的内积定义"><a href="#1-两个随机变量的内积定义" class="headerlink" title="1. 两个随机变量的内积定义"></a>1. 两个随机变量的内积定义</h5><p>如果我们有两个不相关随机变量$X，Y$,  uncorrelated random variable, covariance value is zero 有：</p><script type="math/tex; mode=display">\mathbb{V}[x+y]=\mathbb{V}[x]+\mathbb{V}[y] \tag{28}</script><p>如果随机变量$X，Y$是不相关的,它们在对应向量空间是正交向量,应用毕达哥拉斯定理可以有如下示意图</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/07/03/YWiBm5kDjK49uSq.png?=raw" width=40% height=40%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">随机变量X,Y内积示意图 引用自https://mml-book.com</div> </center><p>随机变量在向量空间可以被看着向量,并且我们定义内积来获得随机变量的几何性质。</p><script type="math/tex; mode=display">\langle X, Y\rangle:=\operatorname{Cov}[x, y] \tag{29}</script><p>对0均值随机变量$X,Y$，我们得到内积。我们看到协方差矩阵是对称正定的，对每个参数都是线性的。随机变量的长度为：</p><script type="math/tex; mode=display">\|X\|=\sqrt{\operatorname{Cov}[x, x]}=\sqrt{\mathbb{V}[x]}=\sigma[x] \tag{30}</script><p>其标准偏差。随机变量的”长”，是不确定的，但一个长为0的随机变量是确定的。其还有如下性质：</p><script type="math/tex; mode=display">\begin{array}{l}\operatorname{Cov}[x, x]=0 \Longleftrightarrow x=0 \\\operatorname{Cov}[\alpha x+z, y]= \alpha \operatorname{Cov}[x, y]+ \operatorname{Cov}[z, y] \text { for } \alpha \in \mathbb{R}\end{array}</script><p>如果我们观察两个随机变量$X，Y$之间的角度$\theta$,我们得到：</p><script type="math/tex; mode=display">\cos \theta=\frac{\langle X, Y\rangle}{\|X\|\|Y\|}=\frac{\operatorname{Cov}[x, y]}{\sqrt{\mathbb{V}[x] \mathbb{V}[y]}} \tag{31}</script><p>随机变量$X,Y$是正交的当且仅当$\operatorname{Cov}[x, y] = 0$，意味着它们是无关的。</p><h3 id="4-高斯分布"><a href="#4-高斯分布" class="headerlink" title="4. 高斯分布"></a>4. 高斯分布</h3><h4 id="1-什么是高斯分布"><a href="#1-什么是高斯分布" class="headerlink" title="1. 什么是高斯分布?"></a>1. 什么是高斯分布?</h4><p>二元随机变量$x_1, x_2$的高斯分布如下：    </p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/07/04/9hp2zKNoDm1HMXb.png?=raw" width=40% height=40%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">二元随机变量高斯分布示意图 引用自https://mml-book.com</div> </center><p>100个样本的高斯分布：</p><ul><li><p>a）一维实例</p></li><li><p>b）二维实例</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/07/04/5JyxHIbhATvfzZl.png?=raw" width=80% height=80%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">100个样本的高斯分布示意图 引用自https://mml-book.com</div> </center></li></ul><p>对于单一随机变量,高斯分布的概率密度函数定义为：</p><script type="math/tex; mode=display">p\left(x \mid \mu, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right) \tag{32}</script><p>以均值向量$\boldsymbol{\mu}$和协方差矩阵$\boldsymbol{\Sigma}$完全地特征化的多元随机变量的高斯分布可以定义为：</p><script type="math/tex; mode=display">p(\boldsymbol{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})=(2 \pi)^{-\frac{D}{2}}|\boldsymbol{\Sigma}|^{-\frac{1}{2}} \exp \left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right) \tag{33}</script><p>其中, <script type="math/tex">\boldsymbol{x} \in \mathbb{R}^{D}$，$p(\boldsymbol{x})=\mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) \text { or } X \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})</script>。</p><p>当<script type="math/tex">\boldsymbol{\mu}=0</script> 且 <script type="math/tex">\boldsymbol{\Sigma}=\boldsymbol{I}</script>时，称作标准正态分布。</p><h4 id="2-多重高斯分布-Gaussians-的边界-Marginal-和条件-Conditional"><a href="#2-多重高斯分布-Gaussians-的边界-Marginal-和条件-Conditional" class="headerlink" title="2. 多重高斯分布 Gaussians 的边界 Marginal 和条件 Conditional"></a>2. 多重高斯分布 Gaussians 的边界 Marginal 和条件 Conditional</h4><p>我们清晰地按照级联状态  <script type="math/tex">[\boldsymbol{x}, \boldsymbol{y}]^T</script> 写出高斯分布:</p><script type="math/tex; mode=display">p(\boldsymbol{x}, \boldsymbol{y})=\mathcal{N}\left(\left[\begin{array}{l}\boldsymbol{\mu}_{x} \\\boldsymbol{\mu}_{y}\end{array}\right],\left[\begin{array}{cc}\boldsymbol{\Sigma}_{x x} & \boldsymbol{\Sigma}_{x y} \\\boldsymbol{\Sigma}_{y x} & \boldsymbol{\Sigma}_{y y}\end{array}\right]\right) \tag{34}</script><p>其中，  <script type="math/tex">\boldsymbol{\Sigma }_{x x} = \operatorname{Cov}[\boldsymbol{x}, \boldsymbol{x}] \text{ and } \boldsymbol{\Sigma}_{y y}=\operatorname{Cov}[\boldsymbol{y}, \boldsymbol{y}]</script>   是  <script type="math/tex">\boldsymbol{x} \text{ and } \boldsymbol{y}</script>  边缘协方差矩阵。同时,   <script type="math/tex">\boldsymbol{\Sigma }_{x y} = \operatorname{Cov}[\boldsymbol{x}, \boldsymbol{y}]</script>  是  <script type="math/tex">\boldsymbol{x}</script> 和 <script type="math/tex">\boldsymbol{y}</script>  之间的互协方差矩阵。</p><p>条件分布conditional distribution  <script type="math/tex">p(\boldsymbol{x} \mid \boldsymbol{y})</script>  也是高斯分布,其定义为:</p><script type="math/tex; mode=display">\begin{aligned}p(\boldsymbol{x} \mid \boldsymbol{y}) &=\mathcal{N}\left(\boldsymbol{\mu}_{x \mid y}, \boldsymbol{\Sigma}_{x \mid y}\right) \\\boldsymbol{\mu}_{x \mid y} &=\boldsymbol{\mu}_{x}+\boldsymbol{\Sigma}_{x y} \boldsymbol{\Sigma}_{y y}^{-1}\left(\boldsymbol{y}-\boldsymbol{\mu}_{y}\right) \\ \boldsymbol{\Sigma}_{x \mid y} &=\boldsymbol{\Sigma}_{x x}-\boldsymbol{\Sigma}_{x y} \boldsymbol{\Sigma}_{y y}^{-1} \boldsymbol{\Sigma}_{y x}\end{aligned} \tag{35}</script><p>注意，公式35计算均值时,  <script type="math/tex">\boldsymbol{y}-value</script>  是一个观察量而不是变量。</p><p>一个联合高斯分布<script type="math/tex">p(\boldsymbol{x}, \boldsymbol{y})</script>的边缘分布<script type="math/tex">p(\boldsymbol{x})</script>，其高斯分布应用sum rule计算给定为：</p><script type="math/tex; mode=display">p(\boldsymbol{x})=\int p(\boldsymbol{x}, \boldsymbol{y}) d \boldsymbol{y}=\mathcal{N}\left(\boldsymbol{x} \mid \boldsymbol{\mu}_{x}, \boldsymbol{\Sigma}_{x x}\right)  \tag{36}</script><p>对应的结果也适用于$p(\boldsymbol{y})$,这通过边缘化$\boldsymbol{x}$来获得。</p><h4 id="3-高斯分布密度的积"><a href="#3-高斯分布密度的积" class="headerlink" title="3.  高斯分布密度的积"></a>3.  高斯分布密度的积</h4><p>两个高斯分布<script type="math/tex">\mathcal{N}(\boldsymbol{x} \mid \boldsymbol{a}, \boldsymbol{A}) \quad \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{b}, \boldsymbol{B})</script>的积,是按<script type="math/tex">c \in \mathbb{R}</script>成比例的高斯分布,给定为<script type="math/tex">c  \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{c}, \boldsymbol{C})</script>:</p><script type="math/tex; mode=display">\begin{align}\tag{37} \boldsymbol{C} &=\left(\boldsymbol{A}^{-1}+\boldsymbol{B}^{-1}\right)^{-1}  \\\tag{38} \boldsymbol{c} &=\boldsymbol{C}\left(\boldsymbol{A}^{-1} \boldsymbol{a}+\boldsymbol{B}^{-1} \boldsymbol{b}\right) \\\tag{39} c &=(2 \pi)^{-\frac{D}{2}}|\boldsymbol{A}+\boldsymbol{B}|^{-\frac{1}{2}} \exp \left(-\frac{1}{2}(\boldsymbol{a}-\boldsymbol{b})^{\top}(\boldsymbol{A}+\boldsymbol{B})^{-1}(\boldsymbol{a}-\boldsymbol{b})\right)\end{align}</script><p>比例常数$c$，它可以按照高斯密度函数的形式，以$\boldsymbol{a}$或$\boldsymbol{b}$和一个”夸张的”协方差矩阵$\boldsymbol{A+B}$写作，如,$\mathcal{N}(\boldsymbol{a} \mid \boldsymbol{b}, \boldsymbol{A + B}) \text{ 或 } \mathcal{N}(\boldsymbol{b} \mid \boldsymbol{a}, \boldsymbol{A + B})$.</p><p>注意：为了记号简便,我们有时用$\mathcal{N}(\boldsymbol{x} \mid \boldsymbol{m}, \boldsymbol{S})$来描述高斯密度函数的形式，即使$\boldsymbol{x}$不是一个随机变量。当我们写作如下公式时，仅需要做前面的证明。</p><script type="math/tex; mode=display">c=\mathcal{N}(\boldsymbol{a} \mid \boldsymbol{b}, \boldsymbol{A}+\boldsymbol{B})=\mathcal{N}(\boldsymbol{b} \mid \boldsymbol{a}, \boldsymbol{A}+\boldsymbol{B}) \tag{40}</script><h4 id="高斯分布的和、线性变换-Sums-and-Linear-Transformations"><a href="#高斯分布的和、线性变换-Sums-and-Linear-Transformations" class="headerlink" title="高斯分布的和、线性变换 Sums and Linear Transformations"></a>高斯分布的和、线性变换 Sums and Linear Transformations</h4><p>如果<script type="math/tex">X,\,Y</script>是独立的高斯随机变量，那么<script type="math/tex">\boldsymbol{x + y}</script>也是高斯分布，给定为：</p><script type="math/tex; mode=display">p(\boldsymbol{x}+\boldsymbol{y})=\mathcal{N}\left(\boldsymbol{\mu}_{x}+\boldsymbol{\mu}_{y}, \boldsymbol{\Sigma}_{x}+\boldsymbol{\Sigma}_{y}\right) \tag{41}</script><h5 id="两个单一变量混合高斯分布密度"><a href="#两个单一变量混合高斯分布密度" class="headerlink" title="两个单一变量混合高斯分布密度"></a>两个单一变量混合高斯分布密度</h5><script type="math/tex; mode=display">p(x)=\alpha p_{1}(x)+(1-\alpha) p_{2}(x) \tag{42}</script><p>这里标量$0&lt; \alpha &lt; 1$是混合权重,  $p(x<em>1) \text{和} p(x_2)$  是有不同参数的单一高斯分布密度,如， $(\mu</em>{1}, \sigma<em>{1}) \, \neq \, (\mu</em>{2}, \sigma_{2}) $  。</p><p>混合高斯分布密度  $p(x)$  用每个随机变量的均值的权重之和来定义：</p><script type="math/tex; mode=display">\mathbb{E}[x] = \alpha \mu_{1} + (1 - \alpha) \mu_{2} \tag{43}</script><p>混合高斯分布密度$p(x)$的方差给定为：</p><script type="math/tex; mode=display">\mathbb{V}(x) = \left[\alpha \sigma_{1}^{2} + (1-\alpha)\sigma_{2}^{2} \right] + \left( \left[\alpha \mu_{1}^{2} + (1-\alpha)\mu_{2}^{2}\right] - \left[ \alpha \mu_{1} + (1-\alpha)\mu_{2} \right] ^{2}    \right) \tag{44}</script><p>证明:$\mathbb{E}[x]$</p><script type="math/tex; mode=display">\begin{align}\mathbb{E}[x] &=\int_{-\infty}^{\infty} x p(x) \mathrm{d} x \\&=\int_{-\infty}^{\infty} \alpha x p_{1}(x)+(1-\alpha) x p_{2}(x) \mathrm{d} x \\&=\alpha \int_{-\infty}^{\infty} x p_{1}(x) \mathrm{d} x+(1-\alpha) \int_{-\infty}^{\infty} x p_{2}(x) \mathrm{d} x \\&=\alpha \mu_{1}+(1-\alpha) \mu_{2} \tag{45}\end{align}</script><p>而要证明$\mathbb{V}(x)$，得证明$\mathbb{E}(x^{2})$.</p><script type="math/tex; mode=display">\begin{align}\mathbb{E}(x^{2}) &= \int_{-\infty}^{\infty} x^{2}p(x) \mathrm{d}x \\ & = \int_{-\infty}^{\infty} \alpha x^{2} p_{1}(x) + (1 - \alpha)x^{2}p_{2}(x) \mathrm{d}x \\ & = \alpha \int_{-\infty}^{\infty}x^{2}p_{1}(x) \mathrm{d}x +  (1 - \alpha)\int_{-\infty}^{\infty}x^{2}p_{2}(x) \mathrm{d}x \\ & = \alpha(\mu_{1}^{2} + \sigma_{1}^{2}) + (1 - \alpha)(\mu_{2}^{2} + \sigma_{2}^{2}) \tag{46}\end{align}</script><p>公式46最后两步是因为:$\sigma^{2} = \mathbb{E}[x^2] - \mu^{2}$。重新整理就是随机变量平方的均值等于均值和方差平方和。因此，方差可以定义为：</p><script type="math/tex; mode=display">\begin{align}\mathbb{V}[x]=& \mathbb{E}\left[x^{2}\right]-(\mathbb{E}[x])^{2} \\=& \alpha\left(\mu_{1}^{2}+\sigma_{1}^{2}\right)+(1-\alpha)\left(\mu_{2}^{2}+\sigma_{2}^{2}\right)-\left(\alpha \mu_{1}+(1-\alpha) \mu_{2}\right)^{2} \\=&\left[\alpha \sigma_{1}^{2}+(1-\alpha) \sigma_{2}^{2}\right] \\&+\left(\left[\alpha \mu_{1}^{2}+(1-\alpha) \mu_{2}^{2}\right]-\left[\alpha \mu_{1}+(1-\alpha) \mu_{2}\right]^{2}\right) \tag{47}\end{align}</script><p>注意：前面的偏差可以应用到任何密度函数，但自从高斯分布完全由均值和方差决定后，混合密度函数也可以确定了。</p><h5 id="总方差定理-Law-of-total-variance"><a href="#总方差定理-Law-of-total-variance" class="headerlink" title="总方差定理 Law of total variance"></a>总方差定理 Law of total variance</h5><p>通常指对于两个随机变量$X \, 和 \, Y$，有：</p><script type="math/tex; mode=display">\mathbb{V}_{X}[x] = \mathbb{E}_Y \left[\mathbb{V}_{X} \left[x \mid y \right] \right] + \mathbb{V}_{Y} \left[\mathbb{E}_{X} \left[x \mid y \right] \right]</script><p>即条件方差的期望加上条件均值的方差。</p><p>我们认为二元高斯随机变量$X$，对其应用线性变换$\boldsymbol{Ax}$。结果是一个有均值zero协方差为$\boldsymbol{A}\boldsymbol{A}^{\top}$高斯分布。观察到，加上一个常数向量将会改变分布的均值，不影响其方差，那么，随机变量$\boldsymbol{x + \mu}$有均值$\boldsymbol{\mu}$和单位协方差矩阵。因此，高斯分布进行任何线性、仿射变换后都是高斯分布。</p><p>考虑高斯分布随机变量$X \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$,对于给定恰当形状的矩阵$\boldsymbol{A}$，让$Y$成为$\boldsymbol{x}$进行$\boldsymbol{y = Ax}$变换后的的随机变量。我们利用如下线性操作来计算$\boldsymbol{y}$</p><p>的均值：</p><script type="math/tex; mode=display">\mathbb{E}[\boldsymbol{y}] = \mathbb{E}[\boldsymbol{Ax}]= \boldsymbol{A}\mathbb{E}[\boldsymbol{x}] = \boldsymbol{A\mu} \tag{48}</script><p>方差：</p><script type="math/tex; mode=display">\mathbb{V} \left[ \boldsymbol{y}\right] = \mathbb{V} \left[ \boldsymbol{Ax}\right] = \boldsymbol{A} \mathbb{V} \left[ \boldsymbol{x}\right]\boldsymbol{A}^{\top}  =\boldsymbol{A} \boldsymbol{\Sigma} \boldsymbol{A}^{\top} \tag{49}</script><p>随机变量$\boldsymbol{y}$分布服从于：</p><script type="math/tex; mode=display">p(\boldsymbol{y})=\mathcal{N}\left(\boldsymbol{y} \mid \boldsymbol{A} \boldsymbol{\mu}, \boldsymbol{A} \boldsymbol{\Sigma} \boldsymbol{A}^{\top}\right) \tag{50}</script><p>现在，我们再考虑相反的变换：当一个随机变量有均值， 且是另一个随机变量的线性变换。对于给定一个满秩矩阵$\boldsymbol{A} \in \mathbb{R}^{M \times N}$,当 $M \ge N$, $\boldsymbol{y} \in \mathbb{R}^{M}$是一个均值为$\boldsymbol{Ax}$的高斯随机变量， 即：</p><script type="math/tex; mode=display">p(\boldsymbol{y})=\mathcal{N}\left(\boldsymbol{y} \mid \boldsymbol{Ax},\, \boldsymbol{\Sigma} \right) \tag{51}</script><p>其对应概率分布$p(\boldsymbol{x})$是什么？如果 $p(\boldsymbol{A})$是可逆的，我们可以写作$\boldsymbol{x} = \boldsymbol{A}^{-1}\boldsymbol{y}$,应用前面提到的变换。然而，通常$\boldsymbol{A}$是不可逆，我们使用相似的方法即伪逆。我们两边同乘$\boldsymbol{A}^{\top}$,然后逆变换$\boldsymbol{A}^{\top}\boldsymbol{A}$,它是对称正定的，给我们如下联系：</p><script type="math/tex; mode=display">\boldsymbol{y}=\boldsymbol{A} \boldsymbol{x} \Longleftrightarrow\left(\boldsymbol{A}^{\top} \boldsymbol{A}\right)^{-1} \boldsymbol{A}^{\top} \boldsymbol{y}=\boldsymbol{x} \tag{52}</script><p>因此，$\boldsymbol{y}$的逆变换是$\boldsymbol{x}$，我们可以得到：</p><script type="math/tex; mode=display">p(\boldsymbol{x})=\mathcal{N}\left(\boldsymbol{x} \mid\left(\boldsymbol{A}^{\top} \boldsymbol{A}\right)^{-1} \boldsymbol{A}^{\top} \boldsymbol{y},\left(\boldsymbol{A}^{\top} \boldsymbol{A}\right)^{-1} \boldsymbol{A}^{\top} \boldsymbol{\Sigma} \boldsymbol{A}\left(\boldsymbol{A}^{\top} \boldsymbol{A}\right)^{-1}\right) \tag{53}</script><h3 id="5-Conjugacy-and-the-Exponential-Family"><a href="#5-Conjugacy-and-the-Exponential-Family" class="headerlink" title="5. Conjugacy and the Exponential Family"></a>5. Conjugacy and the Exponential Family</h3>]]></content>
      
      
      <categories>
          
          <category> Mathematic for machine learning 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Probability </tag>
            
            <tag> 高斯分布 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Chapter05:Vector calculus</title>
      <link href="2020/07/01/Chapter05-Vector-Calculus/"/>
      <url>2020/07/01/Chapter05-Vector-Calculus/</url>
      
        <content type="html"><![CDATA[<h2 id="Chapter-05-Vector-calculus"><a href="#Chapter-05-Vector-calculus" class="headerlink" title="Chapter 05: Vector calculus"></a>Chapter 05: Vector calculus</h2><h3 id="1-单变量函数的微分"><a href="#1-单变量函数的微分" class="headerlink" title="1. 单变量函数的微分"></a>1. 单变量函数的微分</h3><h4 id="1-微分"><a href="#1-微分" class="headerlink" title="1. 微分"></a>1. 微分</h4><ul><li>derivative:</li></ul><script type="math/tex; mode=display">\frac{\mathrm{d} f}{\mathrm{d} x}:=\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h} \tag{1}</script><h4 id="2-Taylor-级数"><a href="#2-Taylor-级数" class="headerlink" title="2. Taylor 级数"></a>2. Taylor 级数</h4><script type="math/tex; mode=display">T_{\infty}(x)=\sum_{k=0}^{\infty} \frac{f^{(k)}\left(x_{0}\right)}{k !}\left(x-x_{0}\right)^{k} \tag{2}</script><h4 id="3-微分法则"><a href="#3-微分法则" class="headerlink" title="3. 微分法则"></a>3. 微分法则</h4><ul><li>Differentiation Rules:</li></ul><script type="math/tex; mode=display">\begin{align}\text{Product rule: }  &\quad(f(x) g(x))^{\prime}=f^{\prime}(x) g(x)+f(x) g^{\prime}(x)  \tag{3}\\ \text{Quotient rule: } &\left(\frac{f(x)}{g(x)}\right)^{\prime}=\frac{f^{\prime}(x) g(x)-f(x) g^{\prime}(x)}{(g(x))^{2}} \tag{4}\\ \text{Sum rule: } &(f(x)+g(x))^{\prime}=f^{\prime}(x)+g^{\prime}(x) \tag{5}\\ \text{Chain rule: } &(g(f(x)))^{\prime}=(g \circ f)^{\prime}(x)=g^{\prime}(f(x)) f^{\prime}(x) \tag{6}\end{align}</script><p><strong>注：</strong>$g \circ f$ 记为复合函数$x \mapsto f(x) \mapsto g(f(x))$</p><script type="math/tex; mode=display">\begin{align}\\ \text{例子：}\\ &h(x) = (2x+1)^{4} = g(f(x))\\ \text{即}&f(x) = 2x + 1 \text{,}\qquad g(f) = f^{4}\\ &f^{\prime}(x) = 2 \text{,}\qquad g^{\prime}(f)=4f^{3}\\ \text{所以}&h^{\prime}(x) = g^{\prime}(f)f^{\prime}(x)=4f^{3}\cdot2=8(2x+1)^{3}\end{align}</script><h3 id="2-部分微分和梯度"><a href="#2-部分微分和梯度" class="headerlink" title="2.部分微分和梯度"></a>2.部分微分和梯度</h3><ul><li>Partial Differentiation andGradients</li></ul><h4 id="1-部分微分"><a href="#1-部分微分" class="headerlink" title="1.部分微分"></a>1.部分微分</h4><ul><li>Partial Derivative:</li></ul><script type="math/tex; mode=display">\nabla_{\boldsymbol{x}} f=\operatorname{grad} f=\frac{\mathrm{d} f}{\mathrm{d} \boldsymbol{x}}=\left[\begin{array}{llll}\frac{\partial f(\boldsymbol{x})}{\partial x_{1}} & \frac{\partial f(\boldsymbol{x})}{\partial x_{2}} & \cdots & \frac{\partial f(\boldsymbol{x})}{\partial x_{n}}\end{array}\right] \in \mathbb{R}^{1 \times n} \tag{7}</script><p>即把对$x$的一阶导的集合写在一起形成一行,也可以转置后形成一列。</p><h4 id="2-梯度"><a href="#2-梯度" class="headerlink" title="2. 梯度"></a>2. 梯度</h4><ul><li>Gradient as a Row Vector</li></ul><p>将<strong>Gradient</strong>写作行向量的原因：</p><ol><li><p>我们始终可以把梯度推广到向量值函数$f:\mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$;这能让梯度变成矩阵。</p></li><li><p>我们可以不考虑梯度维数来应用多变量链式法则。</p><script type="math/tex; mode=display">\begin{align}\\ &\text{函数} f(x_{1}, x_{2}) = x_1^{2}x_2 + x_1x_2{3}\quad\text{其梯度为：}\\&\frac{\mathrm{d} f}{\mathrm{d} \boldsymbol{x}} = \begin{bmatrix}\frac{\partial f(x_1, x_2)}{\partial x_1} & \frac{\partial f(x_1, x_2)}{\partial x_2}\end{bmatrix}= \begin{bmatrix}2x_1x_2^{3} & x_1^{2}+3x_1x_2^{2} \end{bmatrix}\in \mathbb{R}^{\color{red} {1 \times 2}}\end{align}</script></li></ol><h4 id="3-Chain-Rule"><a href="#3-Chain-Rule" class="headerlink" title="3. Chain Rule"></a>3. Chain Rule</h4><script type="math/tex; mode=display">\frac{\mathrm{d} f}{\mathrm{d} t}=\left[\begin{array}{ll}\frac{\partial f}{\partial x_{1}} & \frac{\partial f}{\partial x_{2}}\end{array}\right]\left[\begin{array}{l}\frac{\partial x_{1}(t)}{\partial t} \\\frac{\partial x_{2}(t)}{\partial t}\end{array}\right]=\frac{\partial f}{\partial x_{1}} \frac{\partial x_{1}}{\partial t}+\frac{\partial f}{\partial x_{2}} \frac{\partial x_{2}}{\partial t} \tag{8}</script><p>用矩阵乘法获得梯度:</p><script type="math/tex; mode=display">\frac{\mathrm{d} f}{\mathrm{d}(s, t)}=\frac{\partial f}{\partial \boldsymbol{x}} \frac{\partial \boldsymbol{x}}{\partial(s, t)}=\underbrace{\left[\frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}}\right]}_{=\frac{\partial f}{\partial \boldsymbol{x}}}\underbrace{\left[\begin{array}{ll}\frac{\partial x_{1}}{\partial s} & \frac{\partial x_{1}}{\partial t} \\\frac{\partial x_{2}}{\partial s} &\frac{\partial x_{2}}{\partial t}\end{array}\right]}_{=\frac{\partial \boldsymbol{x}}{\partial(s, t)}}</script><h4 id="4-向量值函数的梯度-从实数域推广到vector-field"><a href="#4-向量值函数的梯度-从实数域推广到vector-field" class="headerlink" title="4. 向量值函数的梯度(从实数域推广到vector field)"></a>4. 向量值函数的梯度(从实数域推广到vector field)</h4><script type="math/tex; mode=display">\boldsymbol{f}(\boldsymbol{x})=\left[\begin{array}{c}f_{1}(\boldsymbol{x}) \\\vdots \\f_{m}(\boldsymbol{x})\end{array}\right] \in \mathbb{R}^{m}</script><h4 id="5-Jacobian"><a href="#5-Jacobian" class="headerlink" title="5. Jacobian"></a>5. Jacobian</h4><p>所有向量值函数$\boldsymbol{f}: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$的一阶导数的集合称作$\mathit{Jacobian}$。见如下公式：</p><script type="math/tex; mode=display">\begin{align}J=\nabla_{x} f=\frac{\mathrm{d} f(x)}{\mathrm{d} x}&=\left[\frac{\partial f(x)}{\partial x_{1}} \quad \cdots \quad \frac{\partial f(x)}{\partial x_{n}}\right] \tag{9}\\&=\left[\begin{array}{ccc}\frac{\partial f_{1}(\boldsymbol{x})}{\partial x_{1}} & \cdots & \frac{\partial f_{1}(\boldsymbol{x})}{\partial x_{n}} \\\vdots & & \vdots \\\frac{\partial f_{m}(\boldsymbol{x})}{\partial x_{1}} & \cdots & \frac{\partial f_{m}(\boldsymbol{x})}{\partial x_{n}}\end{array}\right] \tag{10}\\ \boldsymbol{x}=\left[\begin{array}{c}x_{1} \\\vdots \\x_{n}\end{array}\right], \quad J(i, j)&=\frac{\partial f_{i}}{\partial x_{j}} \tag{11}\end{align}</script><p>确定$\boldsymbol{f}(x)$偏导数维度：</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/06/26/bYTjzB1PcOZFhSN.png?=raw" width="50%" height="50%">     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">偏导数维度：引用自https://mml-book.com</div> </center><p>​    偏导数的维度：column由$\boldsymbol{f}(x)$决定；row由$\boldsymbol{x}$决定</p><p>详解：</p><ol><li>$f:\mathbb{R} \rightarrow \mathbb{R}$ 梯度是个标量</li><li>$f:\mathbb{R}^{N} \rightarrow \mathbb{R}$ 梯度是$1\times $N行向量(row)</li><li>$f:\mathbb{R} \rightarrow \mathbb{R}^{M}$ 梯度是$M \times 1$列向量(column)</li><li>$f:\mathbb{R}^{N} \rightarrow \mathbb{R}^{M}$ 梯度是$M \times N$矩阵(matrix)</li></ol><h5 id="可视化矩阵关于Vector的梯度计算（两种等价方法）"><a href="#可视化矩阵关于Vector的梯度计算（两种等价方法）" class="headerlink" title="可视化矩阵关于Vector的梯度计算（两种等价方法）"></a>可视化矩阵关于Vector的梯度计算（两种等价方法）</h5><ul><li>方法(a):先计算$\frac {\partial \boldsymbol{A}} {\partial {x_1}}, \frac {\partial \boldsymbol{A}} {\partial {x_2}}, \frac {\partial \boldsymbol{A}} {\partial {x_3}}$（都是$4 \times 2$的tensor;再整理成$4\times2\times3$的tensor。</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/06/29/itgacHYjVuInxSR.png?=raw" width=90% height=90%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">方法(a) 引用自https://mml-book.com</div> </center><ul><li>方法(b):先把$\boldsymbol {A} \in \mathbb{R}^{4 \times 2}$展平成向量$\tilde{A} \in \mathbb{R}^{8}$，然后计算其梯度$\frac{\mathrm{d} \tilde{\boldsymbol{A}}}{\mathrm{d} \boldsymbol{x}} \in \mathbb{R}^{8 \times 3}$,我们把得到的梯度tensor再reshape成图例形状。</li></ul><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://i.loli.net/2020/06/29/AolB2qsUWfMdJnE.png?=raw" width="90%" height="90%">     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">方法(b) 引用自https://mml-book.com</div> </center>]]></content>
      
      
      <categories>
          
          <category> Mathematic for machine learning 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 微积分 </tag>
            
            <tag> 矩阵求导 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Chapter04:Matrix Decompositions</title>
      <link href="2020/06/30/Chapter04-Matrix-Decompositions/"/>
      <url>2020/06/30/Chapter04-Matrix-Decompositions/</url>
      
        <content type="html"><![CDATA[<h2 id="Matrix-Decompositions"><a href="#Matrix-Decompositions" class="headerlink" title="Matrix Decompositions"></a>Matrix Decompositions</h2><h3 id="3-Cholesky-Decomposition"><a href="#3-Cholesky-Decomposition" class="headerlink" title="3. Cholesky Decomposition"></a>3. Cholesky Decomposition</h3><p><strong>Cholesky分解</strong> : 一个对称，正定的矩阵$A$</p><p>能被因式分解成一个积：$A = L L^{T}$,$L$是一个有正的对角元素的下三角矩阵：</p><script type="math/tex; mode=display">\begin{bmatrix}    a_{11} & \cdots & a_{1n} \\    \vdots & \ddots & \vdots \\    a_{n1} & \cdots & a_{nn}   \end{bmatrix}=\begin{bmatrix}    l_{11} & \cdots & 0 \\    \vdots & \ddots & \vdots \\   l_{n1} & \cdots & l_{nn}   \end{bmatrix} \begin{bmatrix}  l_{11} & \cdots & l_{n1} \\    \vdots & \ddots & \vdots \\    0 & \cdots & l_{nn}   \end{bmatrix}</script><p>$L$被称作$A$科斯基因子，并且$L$是唯一的。</p><h3 id="4-特征分解和对角化"><a href="#4-特征分解和对角化" class="headerlink" title="4. 特征分解和对角化"></a>4. 特征分解和对角化</h3><h4 id="1-对角阵（A-diagonal-matrix-is-a-matrix-that-has-value-zero-on-all-off-diagonal-elements-）"><a href="#1-对角阵（A-diagonal-matrix-is-a-matrix-that-has-value-zero-on-all-off-diagonal-elements-）" class="headerlink" title="1. 对角阵（A diagonal matrix is a matrix that has value zero on all off-diagonal elements ）"></a>1. 对角阵（A diagonal matrix is a matrix that has value zero on all off-diagonal elements ）</h4><script type="math/tex; mode=display">\begin{bmatrix}    c_{1} & \cdots & 0 \\    \vdots & \ddots & \vdots \\    0 & \cdots & c_{n}   \end{bmatrix}</script><h4 id="2-相似"><a href="#2-相似" class="headerlink" title="2. 相似"></a>2. 相似</h4><p>Marices $A, D$ are similar if there exists an invertible matrix P, such that :</p><script type="math/tex; mode=display">\boldsymbol{D} =\boldsymbol{P}^{-1} \boldsymbol{A} \boldsymbol{P}   \tag{1}</script><h4 id="3-对角化"><a href="#3-对角化" class="headerlink" title="3.对角化"></a>3.对角化</h4><ul><li>Diagonalizable</li></ul><p>矩阵$A \in \mathbb{R}^{n \times n}$如果相似于对角阵就能对角化。即：如果存在可逆矩阵$P \in \mathbb {R}^{n \times n}$如$D = P^{-1}AP$</p><p><strong>注:</strong></p><script type="math/tex; mode=display">A \in \mathbb{R}^{n \times n}, \quad \lambda_{1}, \cdots, \lambda_{n} \ 是标量的集合, p_{1}, \cdots, p_{n} 是在\mathbb{R}^{n} \ 上的向量集合。\\定义： P:=[p_{1}, \cdots, p_{n}]；\ D \in \mathbb{R}^{n \times n}\ 是含\lambda_{1}, \cdots,\lambda{n}\ 对角元素的对角阵。</script><p>则由公式1有：</p><script type="math/tex; mode=display">\begin{align}\boldsymbol{A}\boldsymbol{P} &= \boldsymbol{P}\boldsymbol{D} \tag{2}\\ \boldsymbol{A}\boldsymbol{P} &= \boldsymbol{A}[\boldsymbol{p}_{1}, \cdots, \boldsymbol{p}_{n}] =\begin{bmatrix} \boldsymbol{A}\boldsymbol{p}_{1}, \cdots, {A}\boldsymbol{p}_{n} \tag{3} \end{bmatrix}\\ \boldsymbol{P}\boldsymbol{D}&= \begin{bmatrix} \boldsymbol{p}_1, \cdots, \boldsymbol{p}_{n}\end{bmatrix}\begin{bmatrix}   \lambda_{1} & \cdots & 0 \\   \vdots & \ddots & \vdots \\    0 & \cdots & \lambda_{n} \end{bmatrix} = \begin{bmatrix} \lambda_{1}\boldsymbol{p}_{1}, \cdots,  \lambda_{n}\boldsymbol{p}_{n}\tag{4}  \end{bmatrix} \end{align}</script><p>由（2）（3）（4）有：</p><script type="math/tex; mode=display">\begin{aligned}\boldsymbol{A} \boldsymbol{p}_{1} &=\lambda_{1} \boldsymbol{p}_{1} \\& \vdots \\\boldsymbol{A} \boldsymbol{p}_{n} &=\lambda_{n} \boldsymbol{p}_{n} \end{aligned}</script><p>因此：$\boldsymbol{P}$的列必须是$\boldsymbol{A}$的特征向量。</p><p>$\boldsymbol{P}$ 必须是可逆的，而且是满秩。</p><h4 id="4-特征分解"><a href="#4-特征分解" class="headerlink" title="4. 特征分解"></a>4. 特征分解</h4><ul><li>Eigendecomposition</li></ul><p>方阵$\boldsymbol{A} \in \mathbb{R}^{n \times n}$能分解成：</p><script type="math/tex; mode=display">\boldsymbol{A} =\boldsymbol{P} \boldsymbol{D} \boldsymbol{P}^{-1}   \tag{6}</script><p><strong>注</strong>：$\boldsymbol{D}$ 是一个对角元素都是$\boldsymbol{A}$的特征值的对角阵，当且仅当$\boldsymbol{A}$的特征向量来自于$\mathbb{R}^{n}$的一组基。</p><h4 id="5-SVD"><a href="#5-SVD" class="headerlink" title="5. SVD"></a>5. SVD</h4><ul><li>singular  value  decomposition</li></ul><p>若$\boldsymbol{A}^{m\times n}$是一个秩为$r \in [0, min(m, n)]$的长方形矩阵， $\boldsymbol {A}$能分解成如下形式：</p><center><img src="https://i.loli.net/2020/06/24/1DOTz93CnfUpgXq.png?=70%" /></center><p>注：<a href="https://mml-book.github.io/">引用自:Chapter4.5 singular value decomposition 4.64:</a></p><p>其中：正交阵$\boldsymbol{U} \in \mathbb{R}^{m \times m}$，其列向量$\boldsymbol{u}_{i}, i = 1, \cdots, m$</p><p>​            正交阵$\boldsymbol{V} \in \mathbb{R}^{n \times n}$，其列向量$\boldsymbol{v}_{j}, j = 1, \cdots, m$</p><p>​            并且，$\boldsymbol \sum$是$m \times n$的矩阵，其$\sum {ii} = \sigma_{i} \ge 0 \text{ and} \sum {ij} = 0, i \neq j$</p>]]></content>
      
      
      <categories>
          
          <category> Mathematic for machine learning 笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 矩阵论 </tag>
            
            <tag> Linear mapping </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6. 基本句法</title>
      <link href="2020/06/12/6.%20%E5%9F%BA%E6%9C%AC%E5%8F%A5%E6%B3%95/"/>
      <url>2020/06/12/6.%20%E5%9F%BA%E6%9C%AC%E5%8F%A5%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="6-基本句法"><a href="#6-基本句法" class="headerlink" title="6. 基本句法"></a>6. 基本句法</h2><h3 id="1-基本结构"><a href="#1-基本结构" class="headerlink" title="1.基本结构"></a>1.基本结构</h3><ol><li>顺序语句</li><li>分支语句</li><li>循环语句</li></ol><h4 id="1-if-switch-语句"><a href="#1-if-switch-语句" class="headerlink" title="1. if, switch 语句"></a>1. if, switch 语句</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isLeapYear</span><span class="params">(<span class="keyword">unsigned</span> <span class="keyword">int</span> year)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> ((year % <span class="number">4</span> == <span class="number">0</span> &amp;&amp; year % <span class="number">100</span> !=<span class="number">0</span>) || (year % <span class="number">400</span> == <span class="number">0</span>))&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">enum</span> _<span class="title">COLOR</span>&#123;</span></span><br><span class="line">    RED,</span><br><span class="line">    GREEN,</span><br><span class="line">    BLUE,</span><br><span class="line">    UNKNOWN</span><br><span class="line">&#125;color;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; isLeapYear(<span class="number">2000</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; isLeapYear(<span class="number">2001</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; isLeapYear(<span class="number">2020</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    color color0;</span><br><span class="line">    color0 = BLUE;</span><br><span class="line">    <span class="keyword">if</span> (color0==RED)&#123;<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;red&quot;</span> &lt;&lt; <span class="built_in">endl</span>;&#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (color0==GREEN)&#123;<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;green&quot;</span> &lt;&lt; <span class="built_in">endl</span>;&#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (color0==BLUE)&#123;<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;blue&quot;</span> &lt;&lt; <span class="built_in">endl</span>;&#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;unknown&quot;</span> &lt;&lt; <span class="built_in">endl</span>;&#125;</span><br><span class="line"></span><br><span class="line">    color color1;</span><br><span class="line">    color1 = GREEN;</span><br><span class="line">    <span class="keyword">switch</span> (color1) &#123;</span><br><span class="line">        <span class="keyword">case</span> RED: <span class="comment">// case 后面接常数值</span></span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;red&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> GREEN:</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;green&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> BLUE:</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;blue&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;unknown&quot;</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>if 和switch 比较</strong>：</p><ul><li>使用场景<ol><li>switch 只支持常量值固定相等的分支判断</li><li>if还可用于区间判断</li><li>用switch能做的，用if都能做，反过来则不行</li></ol></li><li>性能比较<ol><li>分支较少是，差别不是很多；分支多是，switch性能较高</li><li>if开始处几个分支效率很高，之后递减</li><li>switch所有case的速度几乎一样</li></ol></li></ul><h3 id="2-自定义结构——枚举enum"><a href="#2-自定义结构——枚举enum" class="headerlink" title="2. 自定义结构——枚举enum"></a>2. 自定义结构——枚举enum</h3><ul><li>使用<code>#define</code> 和 <code>const</code>创建符号常量，使用<code>enum</code>不仅能够创建符号常量，还能定义新的数据类型。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">enum</span> <span class="title">wT</span>&#123;</span>Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday&#125;;</span><br><span class="line">    wT weekday;</span><br><span class="line">    weekday = Monday;</span><br><span class="line">    weekday = Wednesday;</span><br><span class="line">    <span class="comment">// weekday = 1 非法 不能直接给int值，只能赋值成wT定义好的类型值 不能做左值</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; weekday &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>使用细节：</strong></p><ol><li>枚举值不可以做左值</li><li>非枚举变量不可以赋值给枚举变量</li><li>枚举变量可以赋值给非枚举变量</li></ol><h3 id="3-自定义结构——结构体与联合体"><a href="#3-自定义结构——结构体与联合体" class="headerlink" title="3. 自定义结构——结构体与联合体"></a>3. 自定义结构——结构体与联合体</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">union</span> <span class="title">Score</span>&#123;</span></span><br><span class="line">        <span class="keyword">double</span> ds; <span class="comment">// 8</span></span><br><span class="line">        <span class="keyword">char</span> level; <span class="comment">// 1 联合体使用同一个空间</span></span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">Student</span>&#123;</span></span><br><span class="line">        <span class="keyword">char</span> name[<span class="number">6</span>]; <span class="comment">// 6 * 1 bytes</span></span><br><span class="line">        <span class="keyword">int</span> age;<span class="comment">// 4 bytes</span></span><br><span class="line">        Score s;<span class="comment">// 8 bytes</span></span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="keyword">sizeof</span>(Score) &lt;&lt; <span class="built_in">endl</span>; <span class="comment">// 8</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="keyword">sizeof</span>(Student) &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//24</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结构体数据对齐问题：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">s1</span>&#123;</span></span><br><span class="line">        <span class="keyword">char</span> x;<span class="comment">//1byte 不满 4个字节， 在32位cpu中，直接补成4字节</span></span><br><span class="line">        <span class="keyword">int</span> z;<span class="comment">// 4</span></span><br><span class="line">        <span class="keyword">short</span> y;<span class="comment">//2 不满 4个字节， 在32位cpu中，直接补成4字节 共12个字节</span></span><br><span class="line">    &#125;;</span><br><span class="line"> <span class="class"><span class="keyword">struct</span> <span class="title">s2</span>&#123;</span></span><br><span class="line">    <span class="keyword">char</span> x;<span class="comment">//1 </span></span><br><span class="line">    <span class="keyword">short</span> y;<span class="comment">//2 先一个字节 加 2个字节凑成4字节</span></span><br><span class="line">    <span class="keyword">int</span> z;<span class="comment">//4 共8个字节</span></span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">s3</span>&#123;</span></span><br><span class="line">    <span class="keyword">char</span> x;<span class="comment">//1</span></span><br><span class="line">    <span class="keyword">short</span> y;<span class="comment">//2</span></span><br><span class="line">    <span class="keyword">double</span> z;<span class="comment">//8 以最大的size考虑，上面1+2会补成8，共16bytes</span></span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="keyword">sizeof</span>(s1) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="keyword">sizeof</span>(s2) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="keyword">sizeof</span>(s3) &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><p>对于32位CPU：</p><ul><li><code>char</code>： 任何地址</li><li><code>short</code>：偶数地址</li><li><code>int</code>： 4的整数倍地址</li><li><code>double</code>： 8的整数倍地址</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Student stu;</span><br><span class="line"></span><br><span class="line"><span class="built_in">memcpy</span>(stu.name, <span class="string">&quot;lili&quot;</span>, <span class="number">6</span>);</span><br><span class="line">stu.age = <span class="number">16</span>;</span><br><span class="line">stu.s.ds = <span class="number">95.5</span>;</span><br><span class="line">stu.s.level = <span class="string">&#x27;A&#x27;</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; stu.name &lt;&lt; <span class="built_in">endl</span>; <span class="comment">// lili</span></span><br></pre></td></tr></table></figure><h3 id="4-循环语句"><a href="#4-循环语句" class="headerlink" title="4. 循环语句"></a>4. 循环语句</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>, index = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (index &lt;= <span class="number">100</span>)&#123;</span><br><span class="line">        sum += index;</span><br><span class="line">        index ++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; sum &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// for循环</span></span><br><span class="line">    <span class="comment">// index = 1;</span></span><br><span class="line">    sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span>  index = <span class="number">1</span>; index &lt;= <span class="number">100</span>; ++index)&#123;</span><br><span class="line">        sum += index;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; sum &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//do-while语句</span></span><br><span class="line">    sum = <span class="number">0</span>, index = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">do</span>&#123;</span><br><span class="line">        sum += index;</span><br><span class="line">        index += <span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">while</span> (index &lt;= <span class="number">100</span>);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; sum &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Q:输出所有形如aabb的四位完全平方数。</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="comment">// 实现 aabb的完全平方数</span></span><br><span class="line">    <span class="keyword">int</span> num;</span><br><span class="line">    <span class="keyword">double</span> m;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> a = <span class="number">0</span>; a &lt; <span class="number">10</span>; ++a) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> b = <span class="number">0</span>; b &lt; <span class="number">10</span>; ++b) &#123;</span><br><span class="line">            num = a * <span class="number">1100</span> + b * <span class="number">11</span>;</span><br><span class="line">            m = <span class="built_in">sqrt</span>(num);</span><br><span class="line">            <span class="keyword">if</span> ((m - <span class="keyword">int</span>(m)) &lt; <span class="number">0.000001</span>)&#123;</span><br><span class="line">                <span class="built_in">cout</span> &lt;&lt; num &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">                <span class="built_in">cout</span> &lt;&lt; m &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> high, low;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> index = <span class="number">0</span>;; ++index) &#123;</span><br><span class="line">        n = index * index;</span><br><span class="line">        <span class="keyword">if</span> (n &lt; <span class="number">1000</span>)&#123;</span><br><span class="line">            <span class="keyword">continue</span>; <span class="comment">// 继续下一次循环，这一次不再往下继续</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (n &gt; <span class="number">9999</span>)&#123;</span><br><span class="line">            <span class="keyword">break</span>;<span class="comment">//退出循环</span></span><br><span class="line">        &#125;</span><br><span class="line">        high = n / <span class="number">100</span>; <span class="comment">// 4567 / 100 = 45</span></span><br><span class="line">        low = n % <span class="number">100</span>; <span class="comment">// 4567 % 100 = 67</span></span><br><span class="line">        <span class="keyword">if</span> ((high / <span class="number">10</span> == high % <span class="number">10</span>) &amp;&amp; (low / <span class="number">10</span> == low % <span class="number">10</span>))&#123;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; n &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; index &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">___________________________</span><br><span class="line"><span class="number">7744</span></span><br><span class="line"><span class="number">88</span></span><br></pre></td></tr></table></figure><h3 id="5-函数"><a href="#5-函数" class="headerlink" title="5. 函数"></a>5. 函数</h3><p>一个C++程序是由若干个源程序文件构成，而一个源程序是由若干个函数构成，函数将一段逻辑封装起来，便于复用</p><p>从用户角度看，可以分为：</p><ul><li>库函数： 标准函数， 由c++系统提供</li><li>用户自定义函数：需要用户定义后使用</li></ul><p>从组成角度看：</p><ul><li>返回类型： 一个函数可以返回一个值</li><li>函数名称：函数名和参数列表一起构成了函数签名</li><li>参数： 参数列表包括函数参数的类型、顺序、数量。参数是可选的，函数可以不包含参数。</li></ul><h4 id="1-overload-函数重载"><a href="#1-overload-函数重载" class="headerlink" title="1. overload 函数重载"></a>1. overload 函数重载</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">test</span><span class="params">(<span class="keyword">int</span> a)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">test</span><span class="params">(<span class="keyword">double</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">int</span>(b);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">test</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">double</span> b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> a + b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> (*p)(<span class="keyword">int</span>);</span><br><span class="line">    p = test;<span class="comment">// 函数指针</span></span><br><span class="line">    <span class="keyword">int</span> result = (*p)(<span class="number">1</span>);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; result &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">//    int result = test(1);</span></span><br><span class="line"><span class="comment">//    cout &lt;&lt; result &lt;&lt; endl;</span></span><br><span class="line">    result = test(<span class="number">2.0</span>);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; result &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    result = test(<span class="number">1</span>, <span class="number">2.0</span>);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; result &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2-指数函数的指针与返回指针的函数"><a href="#2-指数函数的指针与返回指针的函数" class="headerlink" title="2. 指数函数的指针与返回指针的函数"></a>2. 指数函数的指针与返回指针的函数</h4><p>每一个函数都占用一段内存单元，它们有一个起始地址，指向函数入口地址的指针称为函数指针。</p><p>一般形式： 数据类型 （*指针变量名）(参数表)；</p><p><code>int (*p)(int)</code></p><p><strong>区别：</strong></p><ul><li><code>int (*p)(int);</code>是指针，指向一个函数入口地址</li><li><code>int* p(int);</code>是函数，返回值是一个指针</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MaxValue</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (x &gt; y)? x:y;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MinValue</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (x &lt; y)? x:y;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">add</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x + y;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ProcessNum</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y, <span class="keyword">int</span>(*p)(<span class="keyword">int</span> a, <span class="keyword">int</span> b))</span></span>&#123; <span class="comment">//回调函数</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; p(x, y) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> x = <span class="number">10</span>, y= <span class="number">20</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; ProcessNum(x, y, MaxValue) &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; ProcessNum(x, y, MinValue) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; ProcessNum(x, y, add) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-命名空间"><a href="#3-命名空间" class="headerlink" title="3. 命名空间"></a>3. 命名空间</h4><ul><li>命名空间，可以作为附加信息来区分不同库中名称的函数、类、变量等，命名空间即定义了上下文。本质上，命名空间就是定义了一个范围。</li><li>关键字：using 和namespace。 <code>std::name</code></li></ul><h4 id="4-函数体"><a href="#4-函数体" class="headerlink" title="4. 函数体"></a>4. 函数体</h4><p>函数主体包含一组定义函数执行任务的语句。</p><h4 id="5-内联函数-inline-function"><a href="#5-内联函数-inline-function" class="headerlink" title="5.内联函数 inline function"></a>5.内联函数 inline function</h4><ul><li>如果一个函数是内联的，那么在编译时，编译器会把该函数的代码副本放置在每个调用该函数的地方</li><li>引入内联函数的目的是为了解决程序中函数调用的效率问题</li><li>注意：内联函数内部不能有太复杂的逻辑，编译器有时会有自己的优化策略，所以内联不一定起作用。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">Fib</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> ( n == <span class="number">0</span> )&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> ( n == <span class="number">1</span> )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> Fib(n<span class="number">-1</span>) + Fib(n<span class="number">-2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 有可能没用</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="number">10</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; Fib(n) &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="6-数学归纳法"><a href="#6-数学归纳法" class="headerlink" title="6. 数学归纳法"></a>6. 数学归纳法</h3><p>递归的四个基本法则：</p><ol><li>基准情形：无须递归就能解出；</li><li>不断推进：每一次递归调用都必须是求解状况朝接近基准情形的方向推进</li><li>设计法则：假设所有递归调用都能运行</li><li>合成效益法则——compound interest rule:求解一个问题的同一个实例是，切勿在不同的递归调用中做重复性的工作。</li></ol><p><strong>递归recursion的缺陷：</strong></p><ol><li>递归是一种重要的编程思想：<ul><li>很多重要的算法都包含递归的思想</li><li>递归最大的缺陷：<ul><li>空间删需要开辟大量的栈空间1</li><li>时间上可能需要有大量重复运算</li></ul></li></ul></li><li>递归的优化<ul><li>尾递归： 所有递归形式的调用都出现在函数的末尾</li><li>使用循环替代</li><li>使用动态规划：时间换空间</li></ul></li></ol><p>分别使用递归，循环，尾递归、动态规划来实现斐波那契数。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;assert.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> g_a[<span class="number">1000</span>];<span class="comment">//全局数组，记录fib计算的值</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Fib</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> ( n == <span class="number">0</span> )&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> ( n == <span class="number">1</span> )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> Fib(n<span class="number">-1</span>) + Fib(n<span class="number">-2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Fibloop</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n &lt; <span class="number">2</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> n;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> n0=<span class="number">0</span>, n1=<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> tmp;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i&lt;=n; i++)&#123;</span><br><span class="line">        tmp = n0;</span><br><span class="line">        n0 = n1;</span><br><span class="line">        n1 = n0 + tmp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> n1;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 尾递归</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Fiblast</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">int</span> ret0, <span class="keyword">int</span> ret1)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(n==<span class="number">0</span>)</span><br><span class="line">    &#123;<span class="keyword">return</span> <span class="number">0</span>;&#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (n == <span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> ret1;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> Fiblast(n<span class="number">-1</span>, ret1, ret0 + ret1);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 动态规划</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Fibd</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">    g_a[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    g_a[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=n; i++)&#123;</span><br><span class="line">        <span class="keyword">if</span> (g_a[i]==<span class="number">0</span>)&#123;</span><br><span class="line">            g_a[i] = g_a[i<span class="number">-1</span>] + g_a[i<span class="number">-2</span>];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> g_a[n];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="number">10</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; Fib(n) &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; Fibloop(n) &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; Fiblast(n, <span class="number">0</span>, <span class="number">1</span>) &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; Fibd(n) &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    assert(Fib(<span class="number">10</span>)==<span class="number">55</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Cpp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Cpp </tag>
            
            <tag> 函数体 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Resnet 网络</title>
      <link href="2020/06/05/2.ResNet%20%E7%BD%91%E7%BB%9C/"/>
      <url>2020/06/05/2.ResNet%20%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h3 id="1-Resnet-架构"><a href="#1-Resnet-架构" class="headerlink" title="1. Resnet 架构"></a>1. Resnet 架构</h3><p>ResNet原文： <a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></p><h4 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h4><p>在<code>Abstract</code>中, 就说到更深的网络更难训练，并且会出现退化现象。而残差网络更容易优化，并且增加网络深度能提高准确率。</p><blockquote><p> We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.</p></blockquote><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210221234025.png" alt="image-20210221234022732" style="zoom:50%;" /></p><p>如上图所示，当模型深度增加时，模型的test error却变大了，这就是“退化”问题。</p><p>当更深的网络能够开始收敛时，暴露了一个退化问题：随着网络深度的增加，准确率达到饱和（这可能并不奇怪）然后迅速下降。意外的是，这种下降不是由过拟合引起的，并且在适当的深度模型上添加更多的层会导致更高的训练误差。</p><blockquote><p>When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error.</p></blockquote><p>对于退化问题，简单的想法是，如果我们直接把后面的层和浅层直接相连，从效果上来说不应该比浅层网络差。</p><blockquote><p>There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers on hand are unable to find solutions that are comparably good or better than the constructed solution (or unable to do so in feasible time).</p></blockquote><p>通过引入<strong>deep residual learning</strong>学习框架解决了退化问题。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210221235803.png" alt="image-20210221235801547" style="zoom: 50%;" /></p><script type="math/tex; mode=display">F(\mathbf{x}) = H(\mathbf{x}) - \mathbf{x}</script><p>在卷积神经网络中， $F(\mathbf{x})$输出维度可能和$\mathbf{x}$不一样。文中按下面两种方式处理：</p><ol><li><p>当维度一致是，二者直接相加，公式：</p><script type="math/tex; mode=display">\mathbf{y} = \mathcal{F}(\mathbf{x}, \{W_i\}) + \mathbf{x} \tag{1}</script></li><li><p>当维度不一致是， $\mathbf{x}$做一次矩阵变换， 公式：</p></li></ol><script type="math/tex; mode=display">\mathbf{y} = \mathcal{F}(\mathbf{x}, \{W_i\}) + W_s\mathbf{x} \tag{2}</script><p>下图中，左边是VGG-19,中间是仿VGG19堆叠的34层网络，记为plain-34，网络更深，但<a href="https://aigonna.com/2020/06/02/1.VGG%20%E7%BD%91%E7%BB%9C/">FLOPs</a>(有计算公式) 仅为VGG-19的18%，VGG-19两层全连接层计算量太大。最右边是针对中间加入了跨层连接即残差结构，注意实线就是直接恒等变换和后面的feature map直接相加，就是用公式1，虚线就是由于维度不匹配，用公式2。三个模型计算量分别为：<code>19.6 billion FLOPs</code>、<code>3.6 billion FLOPs</code>、 <code>3.6 billion FLOPs</code>。 <strong>残差结构既不增加计算复杂度（除了几乎可以忽略的元素相加），又不增加模型的参数量，同时这也为模型间的比较提供了方便</strong></p><p><strong>升维有两种方式：</strong>第一种是直接全补0，这样做优势是不会增加网络的参数；第二种是1 x 1卷积升维，后面实验部分会进行比较。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210222000428.png" alt="image-20210222000426907" style="zoom:50%;" /></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>Res18、Res34、Res50、Res101、Res152网络结构如下：</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210222112038.png" alt="image-20210222112028710" style="zoom:33%;" /></p><p>Res50、Res101、Res152采用的是被称为<strong>bottleneck</strong>的残差结构：</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210222112448.png" alt="image-20210222112446810" style="zoom: 50%;" /></p><p>bottleneck结构就是前面先用1 x 1卷积降维，后面再用1 x 1卷积升维以符合维度大小，这样做可以大大减少计算量。<strong>注意bottleneck中3 x 3的卷积层只有一个，而不是普通结构的两个。</strong></p><p>1x1 小卷积作用：</p><ol><li>升维或降维</li><li>通道融合 / 跨通道信息交互</li><li>保持feature map尺寸不变（不损失分辨率）的情况下增加网络的非线性特性（虽然1 x 1卷积是线性的，但ReLU是非线性的）</li></ol><p>下图比较：在ImageNet上训练。细曲线表示训练误差，粗曲线表示中心裁剪图像的验证误差。左：18层和34层的简单网络。右：18层和34层的ResNet。在本图中，残差网络与对应的简单网络相比没有额外的参数。ResNet网络更深，验证误差也能降低，而不跟简单网络一样出现深层网络的退化问题。</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210222112825.png" alt="image-20210222112820209" style="zoom:33%;" /></p><p><strong>训练技巧</strong>：</p><ul><li>图像水平翻转，减去均值，224x224随机裁剪</li><li>对于跳跃结构，当输入与输出的维度一样时，不需要进行任何处理，二者直接相加；当输入与输出维度不同时，输入要进行变换去匹配输出的维度：zero-padding或1x1卷积</li><li><p>设计网络的规则：对于输出特征图大小相同的层，有相同数量的Filters，即channel数相同。当特征图大小减半时（池化），Filters数量翻倍。</p></li><li><p>每个卷积后和激活前采用BN</p></li><li><code>batchsize =256，lr = 0.1</code>，当误差稳定时更新<code>lr = lr * 0.1</code>，SGD优化函数，<code>weight_decay = 0.0001，momentum = 0.9</code>。</li><li>未使用Dropout</li><li>网络末端以全局平均池化层结束，后接Softmax输出</li></ul><h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><p>简化的Resnet</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ResBlock, self).__init__()</span><br><span class="line">        <span class="comment"># 包含一个主干分支和串联分支</span></span><br><span class="line">        self.layer = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channel, out_channel,</span><br><span class="line">                      kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channel),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(out_channel, out_channel,</span><br><span class="line">                      kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channel)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> in_channel != out_channel <span class="keyword">or</span> stride &gt; <span class="number">1</span>:</span><br><span class="line">            self.shortcut = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channel, out_channel,</span><br><span class="line">                          kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>),</span><br><span class="line">                nn.BatchNorm2d(out_channel),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out1 = self.layer(x)</span><br><span class="line">        out2 = self.shortcut(x)</span><br><span class="line">        out = out1 + out2</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_layer</span>(<span class="params">self, block, out_channel, stride, num_block</span>):</span></span><br><span class="line">        layer_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_block):</span><br><span class="line">            <span class="keyword">if</span> i==<span class="number">0</span>:</span><br><span class="line">                in_stride = stride</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                in_stride = <span class="number">1</span></span><br><span class="line">            layer_list.append(block(self.in_channel, out_channel, in_stride))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将in_channel变为out_channel</span></span><br><span class="line">            self.in_channel = out_channel</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layer_list)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ResBlock</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__()</span><br><span class="line">        self.in_channel = <span class="number">32</span></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>,</span><br><span class="line">                      kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.layer1 = self.make_layer(ResBlock, out_channel=<span class="number">64</span>, stride=<span class="number">1</span>, num_block=<span class="number">2</span>)</span><br><span class="line">        self.layer2 = self.make_layer(ResBlock, out_channel=<span class="number">128</span>, stride=<span class="number">2</span>, num_block=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self.make_layer(ResBlock, out_channel=<span class="number">256</span>, stride=<span class="number">2</span>, num_block=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self.make_layer(ResBlock, out_channel=<span class="number">512</span>, stride=<span class="number">2</span>, num_block=<span class="number">2</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.layer1(out)</span><br><span class="line">        out = self.layer2(out)</span><br><span class="line">        out = self.layer3(out)</span><br><span class="line">        out = self.layer4(out)</span><br><span class="line">        out = F.avg_pool2d(out, <span class="number">4</span>)</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = self.fc(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet</span>():</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(ResBlock)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>[1] <a href="https://niecongchong.github.io/2019/06/11/ResNet%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">ResNet论文翻译——中文版</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/56961832">ResNet论文笔记及代码剖析</a></p><p>[3] <a href="https://www.cnblogs.com/aiblbns/p/11143978.html">ResNet论文总结</a></p><p>[4] <a href="https://www.jianshu.com/p/bb479421de64">ResNet论文和代码解读</a></p>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> ResNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5. 指针</title>
      <link href="2020/06/02/5.%20%E6%8C%87%E9%92%88/"/>
      <url>2020/06/02/5.%20%E6%8C%87%E9%92%88/</url>
      
        <content type="html"><![CDATA[<h2 id="5-指针"><a href="#5-指针" class="headerlink" title="5. 指针"></a>5. 指针</h2><h3 id="1-指针的引用"><a href="#1-指针的引用" class="headerlink" title="1. 指针的引用"></a>1. 指针的引用</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># <span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a = <span class="number">112</span>, b = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">float</span> c = <span class="number">3.14f</span>;</span><br><span class="line">    <span class="keyword">int</span>* d = &amp;a;</span><br><span class="line">    <span class="keyword">float</span>* e = &amp;c;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; d &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; e &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; (*d )&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; (*e) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">OUT:</span><br><span class="line"><span class="number">0xa1fe04</span></span><br><span class="line"><span class="number">0xa1fe00</span></span><br><span class="line"><span class="number">112</span></span><br><span class="line"><span class="number">3.14</span></span><br></pre></td></tr></table></figure><h3 id="2-左值和右值"><a href="#2-左值和右值" class="headerlink" title="2. 左值和右值"></a>2. 左值和右值</h3><p>一般说法，编译器为其单独分配了一块存储空间，可以取其地址的，左值可以放在赋值运算符左边；</p><p>右值是数据本身，不能取到其自身地址，右值只能赋值运算右边。</p><p>具体来说：</p><ul><li>左值最常见的情况如函数和数据成员的名字；</li><li>右值是没有标识符、不可以取地址的表达式，一般称之为“临时对象”</li><li><code>&amp;a</code>是允许的，而<code>&amp;(b+c)</code>不能通过编译，因此a是一个左值，(b+c)是一个右值</li></ul><h3 id="3-指针的数组、数组的指针"><a href="#3-指针的数组、数组的指针" class="headerlink" title="3. 指针的数组、数组的指针"></a>3. 指针的数组、数组的指针</h3><ul><li>指针的数组 array of pointers : <code>T* t[]</code></li><li>数组的指针 a pointer to an array : <code>T (*t) []</code> <code>[]</code>优先级比较高</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">4</span>;</span><br><span class="line">    <span class="keyword">int</span>* iP = &amp;i;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; (*iP) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">double</span> d = <span class="number">3.14</span>;</span><br><span class="line">    <span class="keyword">double</span> * dP = &amp;d;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; (*dP) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">char</span> c = <span class="string">&#x27;a&#x27;</span>;</span><br><span class="line">    <span class="keyword">char</span> * cP = &amp;c;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; (*cP) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">-OUT:</span><br><span class="line">    <span class="number">4</span></span><br><span class="line">    <span class="number">3.14</span></span><br><span class="line">    a</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// array of pointers 和 a pointer to an array</span></span><br><span class="line">    <span class="keyword">int</span> c[<span class="number">4</span>] = &#123;<span class="number">0x40000000</span>, <span class="number">0x4FFFFFFF</span>, <span class="number">0x00000000</span>, <span class="number">0x7FFFFFFF</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span>* a[<span class="number">4</span>]; <span class="comment">//array of pointers</span></span><br><span class="line">    <span class="keyword">int</span>(*b) [<span class="number">4</span>]; <span class="comment">// a pointer to an array</span></span><br><span class="line">    b = &amp;c; <span class="comment">//这里数组个数得匹配，</span></span><br><span class="line">    <span class="comment">// 将数组c中元素赋给数组a</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">unsigned</span> <span class="keyword">int</span> i = <span class="number">0</span>; i&lt;<span class="number">4</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        a[i] = &amp;(c[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; *(a[<span class="number">0</span>]) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; *(b) [<span class="number">3</span>] &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">-OUT:</span><br><span class="line"><span class="number">1073741824</span></span><br><span class="line"><span class="number">2147483647</span></span><br></pre></td></tr></table></figure><h3 id="4-const-pointer-与-pointer-to-const"><a href="#4-const-pointer-与-pointer-to-const" class="headerlink" title="4. const pointer 与 pointer to const"></a>4. const pointer 与 pointer to const</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span>  strHelloworld[] = &#123;<span class="string">&quot;helloworld&quot;</span>&#125;; </span><br><span class="line"><span class="keyword">char</span> <span class="keyword">const</span>  *pStr1 = <span class="string">&quot;helloworld&quot;</span>;</span><br><span class="line"><span class="comment">// 修饰char 说明内容是不可改变的</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span> *pStr1 = <span class="string">&quot;helloworld&quot;</span>; </span><br><span class="line"><span class="comment">// 修更常见的写法</span></span><br><span class="line"><span class="keyword">char</span>* <span class="keyword">const</span>  pStr2 = <span class="string">&quot;helloworld&quot;</span>;</span><br><span class="line"><span class="comment">// 修饰指针， 说明不变的是地址</span></span><br><span class="line"><span class="keyword">char</span> <span class="keyword">const</span> * <span class="keyword">const</span> pStr3 = <span class="string">&quot;helloworld&quot;</span>;</span><br><span class="line"><span class="comment">//两个const分别修饰char和指针，说明内容和地址都不可改变</span></span><br><span class="line">pStr1 = strHelloworld;</span><br><span class="line"><span class="comment">//pStr2 = strHelloworld; //pStr2不可改</span></span><br><span class="line"><span class="comment">//pStr3 = strHelloworld; //pStr3不可改</span></span><br></pre></td></tr></table></figure><ul><li>关于<code>const</code>修饰部分：<ol><li>看左侧最近的部分</li><li>如果左侧没有，则看右侧</li></ol></li></ul><h3 id="5-指针的指针"><a href="#5-指针的指针" class="headerlink" title="5. 指针的指针"></a>5. 指针的指针</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a = <span class="number">12</span>;</span><br><span class="line"><span class="keyword">int</span>* b = &amp;a;</span><br><span class="line"><span class="keyword">int</span>** c = &amp;b;</span><br></pre></td></tr></table></figure><ul><li><code>*</code>操作符具有从右向左的结合性， <code>**</code>相当于<code>*(*c)</code>,必须从里向外逐层求值，<code>*c</code>是c指向的位置，即b；<code>**c</code>相当于 <code>*b</code>，得到变量a的值。</li></ul><h3 id="6-NULL"><a href="#6-NULL" class="headerlink" title="6. NULL"></a>6. NULL</h3><p><code>int* a = NULL</code></p><ul><li>一个特殊的指针变量，表示不指向任何东西</li><li>它给了一种方法，来表示特定的指针目前未指向任何东西</li><li>使用注意事项：<ul><li>对于一个指针，如果已经知道江北初始化为什么样的地址，那么请赋给它这个地址值，否则把它设为<strong>NULL</strong></li><li>在对一个指针进行间接引用前，请先判断这个指针的值是否为<strong>NULL</strong></li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a = <span class="number">123</span>;</span><br><span class="line">    <span class="keyword">int</span> *b = &amp;a;</span><br><span class="line">    <span class="keyword">int</span> **c = &amp;b;</span><br><span class="line">    <span class="comment">// NULL 的使用</span></span><br><span class="line">    <span class="keyword">int</span> *pA = <span class="literal">NULL</span>;</span><br><span class="line">    pA = &amp;a;</span><br><span class="line">    <span class="keyword">if</span> (pA != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; (*pA) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    pA = <span class="literal">NULL</span>; <span class="comment">// 不用时一定又置为NULL</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br><span class="line">OUT：</span><br><span class="line"><span class="number">123</span></span><br></pre></td></tr></table></figure><h4 id="杜绝“-野-”指针"><a href="#杜绝“-野-”指针" class="headerlink" title="杜绝“ 野 ”指针"></a>杜绝“ 野 ”指针</h4><ul><li>指向“垃圾”内存的指针。if判断对其不起作用，因为不为NULL。</li></ul><p>一般有三种情况：</p><ol><li>指针变量没有初始化</li><li>已经释放不用的指针没有置NULL，如delete和free之后的指针</li><li>指针操作超越了变量的作用范围</li></ol><p><strong>Note：</strong> <strong>没有初始化的，不用的或者超出范围的指针请把值置为NULL。</strong></p><h3 id="7-指针相关操作符"><a href="#7-指针相关操作符" class="headerlink" title="7. 指针相关操作符"></a>7. 指针相关操作符</h3><p><code>&amp; 与 *</code> 的比较：</p><p>左值取地址，右值取地址里的内容。</p><p><strong>左值和右值？？</strong></p><h3 id="8-与-操作符"><a href="#8-与-操作符" class="headerlink" title="8. ++ 与  - - 操作符"></a>8. ++ 与  - - 操作符</h3><ul><li><code>++ i</code> : 先++， 再赋值给<code>i</code></li><li><code>i++</code>: 先赋值给i, 再++</li></ul><p><strong>++i和i++区别？？？</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a = <span class="number">1</span>; b = <span class="number">2</span>; c;</span><br><span class="line">c = a+++b; <span class="comment">// 相当于a++ +b</span></span><br><span class="line">d = a++++b; <span class="comment">// 相当于a++ ++b, error</span></span><br></pre></td></tr></table></figure><h3 id="9-内存分析"><a href="#9-内存分析" class="headerlink" title="9. 内存分析"></a>9. 内存分析</h3><h4 id="1-栈和队列"><a href="#1-栈和队列" class="headerlink" title="1.栈和队列"></a>1.栈和队列</h4><ol><li><p>栈区（stack）——由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。</p></li><li><p>堆区（heap）—— 一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收 。注意它与数据结构中的堆是两回事，分配方式倒是类似于链表。</p></li><li>全局区（静态区）（static）——，全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域， 未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。 - 程序结束后有系统释放</li><li>文字常量区 ——常量字符串就是放在这里的。 程序结束后由系统释放</li><li>程序代码区——存放函数体的二进制代码。</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">int</span> a = <span class="number">0</span>;                                                <span class="comment">//(GVAR)全局初始化区 </span></span><br><span class="line"><span class="keyword">int</span>* p1;                                                   <span class="comment">//(bss)全局未初始化区 </span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span>                                               <span class="comment">//(text)代码区</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">     <span class="keyword">int</span> b=<span class="number">1</span>;                                              <span class="comment">//(stack)栈区变量 </span></span><br><span class="line">     <span class="keyword">char</span> s[] = <span class="string">&quot;abc&quot;</span>;                                 <span class="comment">//(stack)栈区变量</span></span><br><span class="line">     <span class="keyword">int</span>*p2=<span class="literal">NULL</span>;                                     <span class="comment">//(stack)栈区变量</span></span><br><span class="line">     <span class="keyword">char</span> *p3 = <span class="string">&quot;123456&quot;</span>;                         <span class="comment">//123456\0在常量区, p3在(stack)栈区</span></span><br><span class="line">     <span class="keyword">static</span> <span class="keyword">int</span> c = <span class="number">0</span>;                                   <span class="comment">//(GVAR)全局(静态)初始化区 </span></span><br><span class="line">     p1 = <span class="keyword">new</span> <span class="keyword">int</span>(<span class="number">10</span>);                               <span class="comment">//(heap)堆区变量</span></span><br><span class="line">     p2 = <span class="keyword">new</span> <span class="keyword">int</span>(<span class="number">20</span>);                               <span class="comment">//(heap)堆区变量</span></span><br><span class="line">     <span class="keyword">char</span>* p4 = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">7</span>];                     <span class="comment">//(heap)堆区变量</span></span><br><span class="line">     strcpy_s(p4, <span class="number">7</span>, <span class="string">&quot;123456&quot;</span>);                  <span class="comment">//(text)代码区</span></span><br><span class="line"> </span><br><span class="line">     <span class="comment">//(text)代码区</span></span><br><span class="line">     <span class="keyword">if</span> (p1 != <span class="literal">NULL</span>)</span><br><span class="line">     &#123;</span><br><span class="line">         <span class="keyword">delete</span> p1;</span><br><span class="line">         p1 = <span class="literal">NULL</span>;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">if</span> (p2 != <span class="literal">NULL</span>)</span><br><span class="line">     &#123;</span><br><span class="line">         <span class="keyword">delete</span> p2;</span><br><span class="line">         p2 = <span class="literal">NULL</span>;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">if</span> (p4 != <span class="literal">NULL</span>)</span><br><span class="line">     &#123;</span><br><span class="line">         <span class="keyword">delete</span>[ ] p4;</span><br><span class="line">         p4 = <span class="literal">NULL</span>;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="comment">//(text)代码区</span></span><br><span class="line">     <span class="keyword">return</span> <span class="number">0</span>;                                            <span class="comment">//(text)代码区</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/miller999999/pic/raw/master/img/blog4/cppstorage.png" alt="cppstorage" style="zoom: 80%;" /></p><p><strong>堆heap——动态分配资源</strong>：</p><ol><li>从现代语言的编程角度看，使用堆，或者说使用内存分配是很自然的；</li><li>动态内存带来了不确定性：内存分配耗时要多久？失败了怎么办？在实时性比较高的场合，如嵌入式控制器和电信设备。</li><li>一般地，当我们在堆上分配内存时，很多语言使用new这样的关键字，有些语言则是隐式分配。在c++中new的对应词是delete，因为c++是可以让程序员完全结果内存的分配释放的。</li></ol><h4 id="2-资源管理方案——RAII"><a href="#2-资源管理方案——RAII" class="headerlink" title="2. 资源管理方案——RAII"></a>2. 资源管理方案——RAII</h4><p><strong>Resource Acquisition Is Initialization:</strong></p><ul><li>C++所特有的资源管理方式。主流语言中，C++是唯一一个依赖RAII做资源管理的</li><li>RAII依托栈和析构函数，来对所有资源——包括堆内存在内进行管理。</li><li>RAII有些比较成熟的智能指针代表：<code>std::auto_ptr</code>和<code>boost::shared_ptr</code></li></ul><h4 id="3-C-中栈和堆的对比"><a href="#3-C-中栈和堆的对比" class="headerlink" title="3. C++ 中栈和堆的对比"></a>3. C++ 中栈和堆的对比</h4><div class="table-container"><table><thead><tr><th></th><th>stack</th><th>heap</th></tr></thead><tbody><tr><td>作用域</td><td>函数体内，语句块{}作用域；</td><td>整个程序范围内，由new，malloc开始，delete， free结束</td></tr><tr><td>编译期间大小确定</td><td>变量大小范围确定</td><td>变量大小范围不确定，需要运行期确定</td></tr><tr><td>大小范围</td><td>win：1M，linux默认8M或10M，可以用ulimit -s查询</td><td>所有系统的堆空间大小是接近内存（虚拟内存）的总大小的（一部分被OS占用）</td></tr><tr><td>内存分配方式</td><td>地址由高到低减少</td><td>地址由低到高增加</td></tr><tr><td>内容是否可变</td><td>可变</td><td>可变</td></tr></tbody></table></div><h4 id="4-内存泄漏"><a href="#4-内存泄漏" class="headerlink" title="4. 内存泄漏"></a>4. 内存泄漏</h4><p><strong>定义</strong>：</p><p>​    程序中已动态分配的<strong>堆内存由于某种原因程序未释放或无法释放</strong>，造成系统内存的浪费，导致<strong>程序运行速度减慢甚至系统崩溃等严重后果</strong>。</p><p><strong>原因和排查方式</strong>：</p><ol><li>内存泄漏主要发生在堆内存分配方式中，即“配置内存后，所有指向该内存的指针都遗失了”。若缺乏语言的垃圾回收机制，这样的内存就无法归还系统。</li><li>因为内存泄漏属于程序运行中的问题，无法通过编译识别，所以只能在程序运行过程中来判别和诊断。</li></ol><h3 id="10-C-的智能指针"><a href="#10-C-的智能指针" class="headerlink" title="10. C++的智能指针"></a>10. C++的智能指针</h3><p>C++中推出了四种常用的智能指针：</p><ul><li><code>unique_ptr</code></li><li><code>shared_ptr</code></li><li><code>weak_ptr</code>: C++ 11中已经被遗弃</li><li><code>auto_ptr</code>：C++ 17中正式删除。</li></ul><p>从应用方面来分析这几种智能指针：</p><ol><li>应用场景<ul><li>对象所有权</li><li>生命周期</li></ul></li><li>性能分析</li></ol><h4 id="1-auto-ptr"><a href="#1-auto-ptr" class="headerlink" title="1. auto_ptr"></a>1. auto_ptr</h4><p>由new expression 获得对象，在auto_ptr 对象销毁是，其所管理的对象也会被自动delete掉。</p><p><strong>所有权转移</strong>：不小心把它传递给另外的智能指针，原来的指针就不在拥有这个对象了。<strong>转交给新对象，然后再将原对象指针置为nullptr。</strong></p><p>示例：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;memory&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    &#123;<span class="comment">// 这个大括号只是为了确定auto_ptr作用范围 int</span></span><br><span class="line">        <span class="function"><span class="built_in">auto_ptr</span>&lt;<span class="keyword">int</span>&gt; <span class="title">pl</span><span class="params">(<span class="keyword">new</span> <span class="keyword">int</span>(<span class="number">10</span>))</span></span>;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; *pl &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// auto_ptr C++ 17中移除 拥有严格对象所有权语义的智能指针</span></span><br><span class="line">        <span class="comment">// auto_ptr 原理： 在拷贝和赋值过程中，直接剥夺原对象对内存的控制权，转交给新对象</span></span><br><span class="line">        <span class="comment">// 然后再将原对象指针置为nullptr，这种做法也叫管理权转移</span></span><br><span class="line">        <span class="comment">// 缺点：当我们再次去访问原对象是，程序就会报错，所以auto_ptr实现原理就不好</span></span><br><span class="line">        <span class="built_in">auto_ptr</span>&lt;<span class="built_in">string</span>&gt; languages[<span class="number">5</span>] =&#123;</span><br><span class="line">                <span class="built_in">auto_ptr</span>&lt;<span class="built_in">string</span>&gt;(<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">&quot;C&quot;</span>)),</span><br><span class="line">                <span class="built_in">auto_ptr</span>&lt;<span class="built_in">string</span>&gt;(<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">&quot;Java&quot;</span>)),</span><br><span class="line">                <span class="built_in">auto_ptr</span>&lt;<span class="built_in">string</span>&gt;(<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">&quot;C++&quot;</span>)),</span><br><span class="line">                <span class="built_in">auto_ptr</span>&lt;<span class="built_in">string</span>&gt;(<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">&quot;Python&quot;</span>)),</span><br><span class="line">                <span class="built_in">auto_ptr</span>&lt;<span class="built_in">string</span>&gt;(<span class="keyword">new</span> <span class="built_in">string</span>(<span class="string">&quot;GO&quot;</span>))</span><br><span class="line">        &#125;;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;There are some computer languagues here: \n&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; <span class="number">2</span>; ++i)&#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; *languages[i] &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">auto_ptr</span>&lt;<span class="built_in">string</span>&gt; pC;</span><br><span class="line">    pC = languages[<span class="number">2</span>];<span class="comment">//languages[2] 失去所有权，将languages[2]转交给pC</span></span><br><span class="line">    <span class="comment">// 此时languages[2]不再引用该字符串从而变成空指针</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;There are some computer languagues here: \n&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; <span class="number">2</span>; ++i)&#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; *languages[i] &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;The winner is &quot;</span> &lt;&lt; *pC &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">——————————————————————————————</span><br><span class="line"><span class="number">10</span></span><br><span class="line">There are some computer languagues here:</span><br><span class="line">C</span><br><span class="line">Java</span><br><span class="line">There are some computer languagues here:</span><br><span class="line">C</span><br><span class="line">Java</span><br><span class="line">The winner is C++</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="2-unique-ptr"><a href="#2-unique-ptr" class="headerlink" title="2. unique_ptr"></a>2. unique_ptr</h4><p><code>unique_ptr</code>:是专属所有权，所以<code>unique_ptr</code>管理的内存，只能被一个对象持有，不支持复制和赋值。</p><p>移动语义：<code>unique_ptr</code>禁止了拷贝语义，但有时我们也需要能够转移所有权，用<code>std::move()</code>进行控制所有权的转移。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;    </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;memory&gt;    </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;    </span><br><span class="line">    <span class="keyword">auto</span> w = <span class="built_in">std</span>::make_unique&lt;<span class="keyword">int</span>&gt;(<span class="number">10</span>);    </span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; *(w.get()) &lt;&lt; <span class="built_in">endl</span>;    </span><br><span class="line"></span><br><span class="line">    <span class="comment">//auto w2 = w;//编译错误，无法讲w复制给w2    </span></span><br><span class="line">    <span class="comment">//因为复制从语义上来说，两个对象将共享同一块内存    </span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// unique_ptr 只支持移动语义</span></span><br><span class="line">    <span class="keyword">auto</span> w2 = <span class="built_in">std</span>::move(w);   <span class="comment">// w2获得内存所有权，w此时等于nullptr                                                </span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; ((w.get() != <span class="literal">nullptr</span>)? (*w.get()):<span class="number">-1</span>) &lt;&lt; <span class="built_in">endl</span>;                    </span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; ((w2.get() != <span class="literal">nullptr</span>)?(*w2.get()):<span class="number">-1</span>) &lt;&lt; <span class="built_in">endl</span>;                   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;                                                                 </span><br><span class="line">&#125;      </span><br><span class="line">——————————————————————————————————————</span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">-1</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><code>cout &lt;&lt; ((w.get() != nullptr)? (*w.get()):-1) &lt;&lt; endl</code> 三目运算符， <code>(w.get() != nullptr)?</code>成立吗？成立取：前面，否则取：后面。</p><h4 id="3-shared-ptr"><a href="#3-shared-ptr" class="headerlink" title="3. shared_ptr"></a>3. shared_ptr</h4><p><code>shared_ptr</code>通过一个引用计数共享一个对象。</p><p><code>shared_ptr</code>是为了解决<code>auto_ptr</code>在对象所有权上的局限性，在使用<strong>引用计数的机制</strong>上提供了可以共享所有权的智能指针，当然也需要额外的开销。</p><p><strong>当引用计数为0时，该对象没有被使用，可以进行析构</strong>。</p><h4 id="4-weak-ptr"><a href="#4-weak-ptr" class="headerlink" title="4. weak_ptr"></a>4. weak_ptr</h4><p><code>weak_ptr</code>被设计为与 <code>shared_ptr</code>共同工作，用一种观察这模式工作。</p><p>作用：协作 <code>shared_ptr</code>工作，可获得资源的观测权，像旁观者那样观测资源使用情况。</p><p>意味着 <code>weak_ptr</code>只对 <code>shared_ptr</code>进行引用，而不改变其引用计数，当被观察的<code>shared_ptr</code>失效后，相应的 <code>weak_ptr</code>也相应失效。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;    </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;memory&gt;    </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;    </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="comment">//shared_ptr</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// shared_ptr 代表的是共享所有权， 即多个shared_ptr 可以共享同一块内存</span></span><br><span class="line">        <span class="keyword">auto</span> wA = <span class="built_in">shared_ptr</span>&lt;<span class="keyword">int</span>&gt;(<span class="keyword">new</span> <span class="keyword">int</span>(<span class="number">20</span>));</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">auto</span> wA2 = wA;</span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; ((wA2.get() != <span class="literal">nullptr</span>)? (*wA2.get()):<span class="number">-1</span>) &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//20</span></span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; ((wA.get() != <span class="literal">nullptr</span>)? (*wA.get()):<span class="number">-1</span>) &lt;&lt; <span class="built_in">endl</span>;<span class="comment">//20</span></span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; wA2.use_count() &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//2 wA2.use_count() 引用计数</span></span><br><span class="line">            <span class="built_in">cout</span> &lt;&lt; wA.use_count() &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//2</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//cout &lt;&lt; wA2.use_count() &lt;&lt; endl; //</span></span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; wA.use_count() &lt;&lt; <span class="built_in">endl</span>; <span class="comment">// 1</span></span><br><span class="line">        <span class="comment">// shared_ptr内部是利用计数来实现内存的自动管理，每当赋值一个shared_ptr</span></span><br><span class="line">        <span class="comment">// 引用计数会+1.当shared_ptr离开作用域时，引用计数会-1</span></span><br><span class="line">        <span class="comment">// 当引用计数为0的时候，则delete内存</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// move 语法</span></span><br><span class="line">    <span class="keyword">auto</span> wAA = <span class="built_in">std</span>::make_shared&lt;<span class="keyword">int</span>&gt;(<span class="number">30</span>);</span><br><span class="line">    <span class="keyword">auto</span> wAA2 = <span class="built_in">std</span>::move(wAA); <span class="comment">// 此时wAA等于nullptr, wAA2.use_count()等于1</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; ((wAA.get() != <span class="literal">nullptr</span>)? (*wAA.get()):<span class="number">-1</span>) &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//-1</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; ((wAA2.get() != <span class="literal">nullptr</span>)? (*wAA2.get()):<span class="number">-1</span>) &lt;&lt; <span class="built_in">endl</span>;<span class="comment">//30</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; wAA.use_count() &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//0 wA2.use_count() 引用计数</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; wAA2.use_count() &lt;&lt; <span class="built_in">endl</span>; <span class="comment">//1</span></span><br><span class="line">    <span class="comment">// 将wAA 对象move给wAA2，意味着wAA放弃了对内存的所有权和管理，此时wAA对象等于nullptr</span></span><br><span class="line">    <span class="comment">// 而wAA2获得了对象所有权，但因为此时wAA已不再持有对象，因此wAA2引用计数减1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">----------------------------------------</span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">-1</span></span><br><span class="line"><span class="number">30</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="11-C-的引用"><a href="#11-C-的引用" class="headerlink" title="11. C++的引用"></a>11. C++的引用</h3><h4 id="1-引用"><a href="#1-引用" class="headerlink" title="1.引用"></a>1.引用</h4><p>引用： 是一种特殊的指针，不允许修改的指针。</p><ul><li><p>使用指针的坑：</p><ul><li>空指针</li><li>野指针</li><li>不知不觉改变了指针的值，却继续引用</li></ul></li><li><p>引用：</p><ul><li>不存在空引用</li><li>必须初始化</li><li>一个引用永远指向它初始化的对象</li></ul></li></ul><h4 id="2-基本使用"><a href="#2-基本使用" class="headerlink" title="2. 基本使用"></a>2. 基本使用</h4><p>引用：可以认为是指定变量的别名，使用是可以认为是变量本身。</p><p><code>int x = 1;</code></p><p><code>int &amp;rx = x;</code>这两句中&amp;是引用，注意与指针的区别</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;assert.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span> &amp;a, <span class="keyword">int</span> &amp;b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> temp;</span><br><span class="line">    temp = a;</span><br><span class="line">    a = b;</span><br><span class="line">    b = temp;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap2</span><span class="params">(<span class="keyword">int</span> *a, <span class="keyword">int</span> *b)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> temp;</span><br><span class="line">    temp = *a;</span><br><span class="line">    *a = *b;</span><br><span class="line">    *b = temp;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> x = <span class="number">1</span>, x2 = <span class="number">3</span>;</span><br><span class="line">    <span class="keyword">int</span> &amp;rx = x;</span><br><span class="line">    rx = <span class="number">2</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; x &lt;&lt; <span class="built_in">endl</span>; <span class="comment">// 2</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; rx &lt;&lt; <span class="built_in">endl</span>;<span class="comment">//2</span></span><br><span class="line">    rx = x2;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; x &lt;&lt; <span class="built_in">endl</span>;<span class="comment">//3</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; rx &lt;&lt; <span class="built_in">endl</span>;<span class="comment">//3</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> a = <span class="number">3</span>, b = <span class="number">4</span>;</span><br><span class="line">    swap(a, b);</span><br><span class="line">    assert(a==<span class="number">4</span>&amp;&amp;b==<span class="number">3</span>);</span><br><span class="line">    swap2(&amp;a, &amp;b);</span><br><span class="line">    assert(a==<span class="number">3</span>&amp;&amp;b==<span class="number">4</span>);</span><br><span class="line">    swap(rx, x);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; x &lt;&lt; <span class="built_in">endl</span>;<span class="comment">//3</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; rx &lt;&lt; <span class="built_in">endl</span>;<span class="comment">//3</span></span><br><span class="line">    rx = x2 + <span class="number">1</span>;</span><br><span class="line">    swap(rx, x);</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; x &lt;&lt; <span class="built_in">endl</span>;<span class="comment">//4</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; rx &lt;&lt; <span class="built_in">endl</span>;<span class="comment">//4</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-C-的引用的作用"><a href="#3-C-的引用的作用" class="headerlink" title="3. C++的引用的作用"></a>3. C++的引用的作用</h4><ul><li>有了指针为什么还要引用？——为了支持运算符重载；</li><li>有了引用为什么还需要指针？——为了兼容c语言</li></ul>]]></content>
      
      
      <categories>
          
          <category> Cpp </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> 指针 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VGG 网络</title>
      <link href="2020/06/02/1.VGG%20%E7%BD%91%E7%BB%9C/"/>
      <url>2020/06/02/1.VGG%20%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h3 id="1-VGG结构："><a href="#1-VGG结构：" class="headerlink" title="1. VGG结构："></a>1. VGG结构：</h3><p>VGG 原文在：<a href="https://arxiv.org/pdf/1409.1556.pdf">VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION</a></p><p>在<code>ABSTRACT</code>  中：</p><blockquote><p> Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small ( 3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. </p></blockquote><p>就说了VGG最大特点是用小卷积滤波器(3x3)，16-19层, <strong>Small filters, Deeper networks</strong></p><p>VGG16 结构示意图(对应论文Config D)：</p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210221231111.png?=raw" width=50% height=50%>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">VGG16结构示意图</div> </center><p>论文中VGG不同结构如下(C和D区别在有没有1x1卷积)：</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210221200543.png" alt="image-20210221200541135" style="zoom:40%;" /></p><h4 id="conv-3x3-优势"><a href="#conv-3x3-优势" class="headerlink" title="conv 3x3 优势"></a>conv 3x3 优势</h4><p>在VGG中，采用3个3x3的小卷积能够代替一个7x7的大卷积，要快速计算感受野，用这个 <a href="https://fomoro.com/research/article/receptive-field-calculator#3,1,1,SAME;3,1,1,SAME;3,1,1,SAME"><a href="https://fomoro.com/">Fomoro AI</a></a></p><center>    <img style="border-radius: 0.3125em;    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"     src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210219155841.png?=raw" width=400 height=220>     <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">3层layer感受野</div> </center><p>先计算特征图。带<code>dilation</code>的卷积操作，特征图size为：</p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210219154253.png" alt="image-20210219154251782" style="zoom: 33%;" /></p><p>像cs231n中，(不带dilation)计算特征图公式：</p><script type="math/tex; mode=display">W_{out} = \frac{W - K + 2P }{S} + 1</script><p>其中, $W$是输入尺寸， $K$是kernel_size, $P$是padding大小， $S$是步长</p><p>那么，经过第一层卷积后， (K=3, S=1, padding=1), 输入224x224，输出为224x224,对应vgg 图中第一层卷积。</p><p>感受野计算code，详细见 <a href="https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807">A guide to receptive field arithmetic for Convolutional Neural Networks</a></p><script type="math/tex; mode=display">l_k = l_{k-1}+ [(f_k -1) * \prod_{i=1}^{k-1}s_i]</script><p>其中， </p><div class="table-container"><table><thead><tr><th style="text-align:left">No.</th><th style="text-align:center">Layers</th><th style="text-align:center">Kernel Size</th><th style="text-align:center">Stride</th></tr></thead><tbody><tr><td style="text-align:left">1</td><td style="text-align:center">Conv1</td><td style="text-align:center">3*3</td><td style="text-align:center">1</td></tr><tr><td style="text-align:left">2</td><td style="text-align:center">Pool1</td><td style="text-align:center">2*2</td><td style="text-align:center">2</td></tr><tr><td style="text-align:left">3</td><td style="text-align:center">Conv2</td><td style="text-align:center">3*3</td><td style="text-align:center">1</td></tr></tbody></table></div><p>对于初始感受野$l_0=1$，各层感受野分别为:</p><script type="math/tex; mode=display">\begin{aligned}&l_0 = 1\\&l_1 = 1 + (3-1)=3\\&l_2 = 3 + (2-1)*1=4\\&l_3 = 4+(3-1)*1*2=8\end{aligned}</script><p>那么像图3层layer感受野中，Layer1中感受野为1，Layer2中感受野为(按VGG16 中卷积层 $f_k=3, s_i=1$ )：3,  Layer3为5， Layer4为7.也是7x7的感受野。</p><p>更重要的是<strong>参数量</strong>大大降低，</p><p>一个7x7卷积核的参数量为49， 3个3x3的卷积核计算量为3x3x3.</p><p><strong>如何理解卷积层的参数量和计算量</strong>？</p><ul><li>参数量：参于计算参数的个数，占用内存空间(考虑bias)</li></ul><script type="math/tex; mode=display">(C_{in} * (K * K) + 1)* C_{out} \tag{1}</script><ul><li>FLOPS: 每秒浮点运算次数，理解为计算速度。是衡量<strong>硬件性能</strong>的指标</li><li>FLOPs: 浮点运算数，理解为计算量。可以用来衡量算法、模型的复杂度, H、W是输出feature map的尺寸（宽和高）</li></ul><script type="math/tex; mode=display">(C_{in} *2* K * K) * H_{out} * W_{out} * C_{out} \tag{2}</script><ul><li>MAC:乘加次数，用来衡量计算量<script type="math/tex; mode=display">C_{in} * K * K * H_{out} * W_{out} * C_{out} \tag{3}</script></li></ul><p>计算实例, 如下图，来自于 <a href="http://cs231n.stanford.edu/slides/2020/lecture_9.pdf">cs231n lecture9</a></p><p><img src="https://gitee.com/miller999999/bpic/raw/master/img/blog/20210221220740.png" alt="image-20210221220738811" style="zoom: 50%;" /></p><p>原文对3x3的解释：</p><blockquote><p>First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer $3 × 3$convolution stack has C channels, the stack is parametrised by $3 (3^2C^2) = 27C^2$ weights; at the same time, a single $7 × 7$ conv. layer would require $7^2C^2 = 49C^2 $parameters, i.e. 81%more. This can be seen as imposing a regularisation on the $7 × 7 $conv. filters, forcing them to have a decomposition through the $3 × 3$ filters (with non-linearity injected in between)</p></blockquote><p>文章认为 LRN(local Response Normalization) 并没有提升模型在 ILSVRC 数据集上的表现，反而增加了内存消耗和计算时间。</p><p>模型 C 和 D 的层数一样，但 C 层使用了 1×1 的卷积核，用于对输入的线性转换，增加非线性决策函数，而不影响卷积层的接受视野。后面的评估阶段也有证明，使用增加的 1×1 卷积核不如添加 3×3 的卷积核。</p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>[1] <a href="http://deanhan.com/2018/07/26/vgg16/">VGG16学习笔记</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/42233779">VGG 论文阅读记录</a></p><p>[3] [<a href="https://www.cnblogs.com/wangguchangqing/p/10338560.html">卷积神经网络之VGG</a></p><p>[4] <a href="http://noahsnail.com/2017/08/17/2017-08-17-VGG%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E6%96%87%E7%89%88/">VGG论文翻译——中文版</a></p><p>[5] <a href="https://www.plob.org/article/22077.html">感受野计算公式</a></p><p>[6]  <a href="https://www.cnblogs.com/shine-lee/p/12069176.html">感受野</a></p>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> VGG </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
